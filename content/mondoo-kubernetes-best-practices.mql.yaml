# Copyright (c) Mondoo, Inc.
# SPDX-License-Identifier: BUSL-1.1
policies:
  - uid: mondoo-kubernetes-best-practices
    name: Mondoo Kubernetes Best Practices
    version: 1.1.1
    license: BUSL-1.1
    tags:
      mondoo.com/category: best-practices
      mondoo.com/platform: kubernetes
    require:
      - provider: k8s
    authors:
      - name: Mondoo, Inc
        email: hello@mondoo.com
    docs:
      desc: |-
        The Mondoo Kubernetes Best Practices policy validates that Kubernetes workloads follow operational best practices that help maintain cluster reliability, resource efficiency, and operational consistency. Missing resource requests, improper namespace usage, or inadequate labeling can lead to resource contention, scheduling failures, and difficulty managing workloads at scale.

        This policy checks workload configurations including resource requests and limits, namespace usage, host aliases, port configurations, and labeling standards across Deployments, DaemonSets, StatefulSets, Jobs, CronJobs, and Pods.

        ## Remote scan

        Remotely scan a Kubernetes cluster without installing an agent.

        ### Prerequisites

        Remote scans of Kubernetes clusters require a `KUBECONFIG` with access to the cluster you want to scan.

        ### Scan a Kubernetes cluster

        Open a terminal and configure an environment variable with the path to your `KUBECONFIG`:

        ```bash
        export KUBECONFIG=/path/to/kubeconfig
        ```

        Run a scan of the Kubernetes cluster:

        ```bash
        cnspec scan k8s
        ```

        ## Join the community!

        Our goal is to build policies that are simple to deploy, accurate, and actionable.

        If you have any suggestions for how to improve this policy, or if you need support, [join the community](https://github.com/orgs/mondoohq/discussions) in GitHub Discussions.
    groups:
      - title: CronJobs
        filters: asset.platform == "k8s-cronjob"
        checks:
          - uid: mondoo-kubernetes-best-practices-cronjob-default-namespace
          - uid: mondoo-kubernetes-best-practices-cronjob-hostalias
          - uid: mondoo-kubernetes-best-practices-cronjob-ports-hostport
          - uid: mondoo-kubernetes-best-practices-cronjob-requestcpu
          - uid: mondoo-kubernetes-best-practices-cronjob-requestmemory
      - title: StatefulSets
        filters: asset.platform == "k8s-statefulset"
        checks:
          - uid: mondoo-kubernetes-best-practices-statefulset-default-namespace
          - uid: mondoo-kubernetes-best-practices-statefulset-hostalias
          - uid: mondoo-kubernetes-best-practices-statefulset-livenessprobe
          - uid: mondoo-kubernetes-best-practices-statefulset-ports-hostport
          - uid: mondoo-kubernetes-best-practices-statefulset-readinessProbe
          - uid: mondoo-kubernetes-best-practices-statefulset-requestcpu
          - uid: mondoo-kubernetes-best-practices-statefulset-requestmemory
      - title: Deployments
        filters: asset.platform == "k8s-deployment"
        checks:
          - uid: mondoo-kubernetes-best-practices-deployment-default-namespace
          - uid: mondoo-kubernetes-best-practices-deployment-hostalias
          - uid: mondoo-kubernetes-best-practices-deployment-livenessprobe
          - uid: mondoo-kubernetes-best-practices-deployment-ports-hostport
          - uid: mondoo-kubernetes-best-practices-deployment-readinessProbe
          - uid: mondoo-kubernetes-best-practices-deployment-requestcpu
          - uid: mondoo-kubernetes-best-practices-deployment-requestmemory
      - title: Jobs
        filters: asset.platform == "k8s-job"
        checks:
          - uid: mondoo-kubernetes-best-practices-job-default-namespace
          - uid: mondoo-kubernetes-best-practices-job-hostalias
          - uid: mondoo-kubernetes-best-practices-job-ports-hostport
          - uid: mondoo-kubernetes-best-practices-job-requestcpu
          - uid: mondoo-kubernetes-best-practices-job-requestmemory
      - title: Replicasets
        filters: asset.platform == "k8s-replicaset"
        checks:
          - uid: mondoo-kubernetes-best-practices-replicaset-default-namespace
          - uid: mondoo-kubernetes-best-practices-replicaset-hostalias
          - uid: mondoo-kubernetes-best-practices-replicaset-livenessprobe
          - uid: mondoo-kubernetes-best-practices-replicaset-ports-hostport
          - uid: mondoo-kubernetes-best-practices-replicaset-readinessProbe
          - uid: mondoo-kubernetes-best-practices-replicaset-requestcpu
          - uid: mondoo-kubernetes-best-practices-replicaset-requestmemory
      - title: Daemonsets
        filters: asset.platform == "k8s-daemonset"
        checks:
          - uid: mondoo-kubernetes-best-practices-daemonset-default-namespace
          - uid: mondoo-kubernetes-best-practices-daemonset-hostalias
          - uid: mondoo-kubernetes-best-practices-daemonset-livenessprobe
          - uid: mondoo-kubernetes-best-practices-daemonset-ports-hostport
          - uid: mondoo-kubernetes-best-practices-daemonset-readinessProbe
          - uid: mondoo-kubernetes-best-practices-daemonset-requestcpu
          - uid: mondoo-kubernetes-best-practices-daemonset-requestmemory
      - title: Pods
        filters: asset.platform == "k8s-pod"
        checks:
          - uid: mondoo-kubernetes-best-practices-pod-default-namespace
          - uid: mondoo-kubernetes-best-practices-pod-hostalias
          - uid: mondoo-kubernetes-best-practices-pod-livenessprobe
          - uid: mondoo-kubernetes-best-practices-pod-no-owner
          - uid: mondoo-kubernetes-best-practices-pod-ports-hostport
          - uid: mondoo-kubernetes-best-practices-pod-readinessProbe
          - uid: mondoo-kubernetes-best-practices-pod-requestcpu
          - uid: mondoo-kubernetes-best-practices-pod-requestmemory
      - title: Ingress Configuration
        filters: asset.platform == "k8s-ingress"
        checks:
          - uid: mondoo-kubernetes-best-practices-ingress-cert-expiration
    scoring_system: highest impact
queries:
  - uid: mondoo-kubernetes-best-practices-pod-no-owner
    title: Pods should have an owner
    impact: 50
    mql: |
      k8s.pod {
        manifest['metadata']['ownerReferences'] != empty && manifest['metadata']['ownerReferences'].length > 0
      }
    docs:
      desc: |
        This check verifies that Pods have the `ownerReferences` metadata field populated, which indicates they were created by a controller (such as a Deployment, ReplicaSet, DaemonSet, StatefulSet, or Job) rather than directly.

        **Why this matters**

        The `ownerReferences` metadata field establishes the relationship between a Pod and its managing controller. Pods without this metadata are typically "orphaned" or manually created, which creates operational risks:

          - **No automatic recovery**: Without a controller tracking the Pod, Kubernetes will not recreate it if it crashes or the node fails.
          - **No garbage collection**: Orphaned Pods are not automatically cleaned up when their parent resources are deleted.
          - **Manual lifecycle management**: Updates, scaling, and deletion must be handled manually for each Pod.
          - **Cluster maintenance risks**: During node drains or upgrades, Pods without owner references may be lost permanently.

        Ensure Pods are created through controllers like Deployments, which set the `ownerReferences` field automatically and provide lifecycle management.
      audit: |
        Check for Pods without an owner reference. Any line of output starting with '0' will indicate a Pod that has no owner:

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | [(.metadata.ownerReferences | length), .metadata.namespace, .metadata.name] | @tsv'
        ```
      remediation: |
        For each Pod without an owner, ensure the Pod is owned by an appropriate Kubernetes object (eg Deployment, Job, DaemonSet, etc.) that will manage relaunching the Pod in the event of a failure.
  - uid: mondoo-kubernetes-best-practices-pod-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.pod {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-cronjob-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.cronjob {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-statefulset-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.statefulset {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-deployment-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.deployment {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-job-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.job {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-replicaset-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.replicaset {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-daemonset-requestcpu
    title: Container should request CPU
    impact: 20
    mql: |
      k8s.daemonset {
        initContainers {
          resources['requests']['cpu'] != empty
        }
        containers {
          resources['requests']['cpu'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define CPU resource requests, which tell the Kubernetes scheduler how much CPU capacity the container needs.

        **Why this matters**

        Without CPU requests, Kubernetes cannot make informed scheduling decisions, leading to operational and stability risks:

          - **Resource contention**: Pods may be scheduled on nodes without sufficient CPU capacity, causing performance degradation for all workloads on that node.
          - **Unpredictable throttling**: During high load, containers without requests receive lower priority and may be heavily throttled.
          - **Quality of Service degradation**: Pods without requests are classified as BestEffort QoS, making them first to be evicted under resource pressure.
          - **Autoscaling failures**: Horizontal Pod Autoscaler and cluster autoscaler rely on resource requests to function correctly.
          - **Capacity planning challenges**: Without requests, it is impossible to accurately plan cluster capacity or predict resource utilization.

        Defining CPU requests ensures predictable scheduling, fair resource allocation, and stable cluster operations.
      audit: |
        Check for the existence of CPU `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
      remediation: |
        Define the required resources for CPU `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  cpu: "250m" # <-- set CPU requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-pod-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.pod {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-cronjob-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.cronjob {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-statefulset-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.statefulset {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-deployment-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.deployment {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-job-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.job {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-replicaset-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.replicaset {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-daemonset-requestmemory
    title: Container should request memory
    impact: 20
    mql: |
      k8s.daemonset {
        initContainers {
          resources['requests']['memory'] != empty
        }
        containers {
          resources['requests']['memory'] != empty
        }
      }
    docs:
      desc: |
        This check ensures that containers define memory resource requests, which tell the Kubernetes scheduler how much memory the container needs.

        **Why this matters**

        Without memory requests, Kubernetes cannot properly schedule Pods or manage resources, leading to serious stability risks:

          - **Out-of-memory kills**: Containers may be scheduled on nodes with insufficient memory, leading to OOMKilled errors and application crashes.
          - **Node instability**: Memory-hungry containers without requests can exhaust node memory, affecting all workloads on that node.
          - **Eviction priority**: Pods without memory requests are classified as BestEffort QoS and are evicted first when nodes experience memory pressure.
          - **Autoscaling issues**: Vertical Pod Autoscaler and cluster autoscaler cannot function correctly without memory requests defined.
          - **Unpredictable performance**: Applications may experience severe performance degradation when competing for limited memory resources.

        Defining memory requests ensures containers are scheduled on nodes with adequate resources and receive guaranteed memory allocation.
      audit: |
        Check for the existence of memory `requests` resources.

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
      remediation: |
        Define the required resources for memory `requests` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1
              resources:
                requests:
                  memory: "1Gi" # <-- set memory requests
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-best-practices-pod-livenessprobe
    title: Container should configure a livenessProbe
    impact: 20
    mql: |
      if (k8s.pod.manifest['metadata']['ownerReferences'].none(_['kind'] == 'Job')) {
        k8s.pod {
          containers {
            probeSpecified = livenessProbe['httpGet'] != empty || livenessProbe['tcpSocket'] != empty || livenessProbe['exec'] != empty

            probeSpecified == true
          }
        }
      }
    docs:
      desc: |
        This check ensures that containers define a livenessProbe, which allows Kubernetes to detect when a container has become unresponsive and needs to be restarted.

        **Why this matters**

        Without liveness probes, Kubernetes cannot detect application failures that do not result in container crashes, leading to degraded service availability:

          - **Silent failures**: Applications may enter deadlock states, infinite loops, or become unresponsive while the container process continues running.
          - **Zombie containers**: Failed containers remain in Running state, consuming resources and receiving traffic they cannot handle.
          - **Extended outages**: Without automatic restarts, failed containers require manual intervention to recover, increasing mean time to recovery.
          - **Load balancer issues**: Traffic continues to be routed to unhealthy containers, causing user-facing errors and degraded experience.
          - **Cascading failures**: A single unresponsive container can cause timeouts and failures in dependent services.

        Configuring liveness probes enables Kubernetes to automatically restart unhealthy containers, improving application resilience and reducing operational burden.
      audit: |
        Check for the existence of `livenessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `livenessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-statefulset-livenessprobe
    title: Container should configure a livenessProbe
    impact: 20
    mql: |
      k8s.statefulset {
        containers {
          probeSpecified = livenessProbe['httpGet'] != empty || livenessProbe['tcpSocket'] != empty || livenessProbe['exec'] != empty

          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a livenessProbe, which allows Kubernetes to detect when a container has become unresponsive and needs to be restarted.

        **Why this matters**

        Without liveness probes, Kubernetes cannot detect application failures that do not result in container crashes, leading to degraded service availability:

          - **Silent failures**: Applications may enter deadlock states, infinite loops, or become unresponsive while the container process continues running.
          - **Zombie containers**: Failed containers remain in Running state, consuming resources and receiving traffic they cannot handle.
          - **Extended outages**: Without automatic restarts, failed containers require manual intervention to recover, increasing mean time to recovery.
          - **Load balancer issues**: Traffic continues to be routed to unhealthy containers, causing user-facing errors and degraded experience.
          - **Cascading failures**: A single unresponsive container can cause timeouts and failures in dependent services.

        Configuring liveness probes enables Kubernetes to automatically restart unhealthy containers, improving application resilience and reducing operational burden.
      audit: |
        Check for the existence of `livenessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `livenessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-deployment-livenessprobe
    title: Container should configure a livenessProbe
    impact: 20
    mql: |
      k8s.deployment {
        containers {
          probeSpecified = livenessProbe['httpGet'] != empty || livenessProbe['tcpSocket'] != empty || livenessProbe['exec'] != empty
          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a livenessProbe, which allows Kubernetes to detect when a container has become unresponsive and needs to be restarted.

        **Why this matters**

        Without liveness probes, Kubernetes cannot detect application failures that do not result in container crashes, leading to degraded service availability:

          - **Silent failures**: Applications may enter deadlock states, infinite loops, or become unresponsive while the container process continues running.
          - **Zombie containers**: Failed containers remain in Running state, consuming resources and receiving traffic they cannot handle.
          - **Extended outages**: Without automatic restarts, failed containers require manual intervention to recover, increasing mean time to recovery.
          - **Load balancer issues**: Traffic continues to be routed to unhealthy containers, causing user-facing errors and degraded experience.
          - **Cascading failures**: A single unresponsive container can cause timeouts and failures in dependent services.

        Configuring liveness probes enables Kubernetes to automatically restart unhealthy containers, improving application resilience and reducing operational burden.
      audit: |
        Check for the existence of `livenessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `livenessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-replicaset-livenessprobe
    title: Container should configure a livenessProbe
    impact: 20
    mql: |
      k8s.replicaset {
        containers {
          probeSpecified = livenessProbe['httpGet'] != empty || livenessProbe['tcpSocket'] != empty || livenessProbe['exec'] != empty

          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a livenessProbe, which allows Kubernetes to detect when a container has become unresponsive and needs to be restarted.

        **Why this matters**

        Without liveness probes, Kubernetes cannot detect application failures that do not result in container crashes, leading to degraded service availability:

          - **Silent failures**: Applications may enter deadlock states, infinite loops, or become unresponsive while the container process continues running.
          - **Zombie containers**: Failed containers remain in Running state, consuming resources and receiving traffic they cannot handle.
          - **Extended outages**: Without automatic restarts, failed containers require manual intervention to recover, increasing mean time to recovery.
          - **Load balancer issues**: Traffic continues to be routed to unhealthy containers, causing user-facing errors and degraded experience.
          - **Cascading failures**: A single unresponsive container can cause timeouts and failures in dependent services.

        Configuring liveness probes enables Kubernetes to automatically restart unhealthy containers, improving application resilience and reducing operational burden.
      audit: |
        Check for the existence of `livenessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `livenessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-daemonset-livenessprobe
    title: Container should configure a livenessProbe
    impact: 20
    mql: |
      k8s.daemonset {
        containers {
          probeSpecified = livenessProbe['httpGet'] != empty || livenessProbe['tcpSocket'] != empty || livenessProbe['exec'] != empty

          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a livenessProbe, which allows Kubernetes to detect when a container has become unresponsive and needs to be restarted.

        **Why this matters**

        Without liveness probes, Kubernetes cannot detect application failures that do not result in container crashes, leading to degraded service availability:

          - **Silent failures**: Applications may enter deadlock states, infinite loops, or become unresponsive while the container process continues running.
          - **Zombie containers**: Failed containers remain in Running state, consuming resources and receiving traffic they cannot handle.
          - **Extended outages**: Without automatic restarts, failed containers require manual intervention to recover, increasing mean time to recovery.
          - **Load balancer issues**: Traffic continues to be routed to unhealthy containers, causing user-facing errors and degraded experience.
          - **Cascading failures**: A single unresponsive container can cause timeouts and failures in dependent services.

        Configuring liveness probes enables Kubernetes to automatically restart unhealthy containers, improving application resilience and reducing operational burden.
      audit: |
        Check for the existence of `livenessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `livenessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              livenessProbe: # <--- Set a livenessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-pod-readinessProbe
    title: Container should configure a readinessProbe
    impact: 20
    mql: |
      if (k8s.pod.manifest['metadata']['ownerReferences'].none(_['kind'] == 'Job')) {
        k8s.pod {
          containers {
            probeSpecified = readinessProbe['httpGet'] != empty || readinessProbe['tcpSocket'] != empty || readinessProbe['exec'] != empty

            probeSpecified == true
          }
        }
      }
    docs:
      desc: |
        This check ensures that containers define a readinessProbe, which allows Kubernetes to determine when a container is ready to accept traffic.

        **Why this matters**

        Without readiness probes, Kubernetes cannot distinguish between containers that are starting up and those that are ready to serve requests:

          - **Premature traffic routing**: New Pods receive traffic before the application is fully initialized, causing errors for users.
          - **Deployment failures masked**: Rolling updates may complete even when new Pods are not functioning correctly, leading to degraded service.
          - **Startup race conditions**: Applications that require time to load configurations, warm caches, or establish connections may fail initial requests.
          - **Service discovery issues**: Endpoints are added to Services before the application can handle requests, causing intermittent failures.
          - **Health check confusion**: Without readiness probes, load balancers cannot accurately determine which Pods should receive traffic.

        Configuring readiness probes ensures traffic is only routed to containers that are fully prepared to handle requests, improving reliability during deployments and scaling events.
      audit: |
        Check for the existence of `readinessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `readinessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-statefulset-readinessProbe
    title: Container should configure a readinessProbe
    impact: 20
    mql: |
      k8s.statefulset {
        containers {
          probeSpecified = readinessProbe['httpGet'] != empty || readinessProbe['tcpSocket'] != empty || readinessProbe['exec'] != empty

          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a readinessProbe, which allows Kubernetes to determine when a container is ready to accept traffic.

        **Why this matters**

        Without readiness probes, Kubernetes cannot distinguish between containers that are starting up and those that are ready to serve requests:

          - **Premature traffic routing**: New Pods receive traffic before the application is fully initialized, causing errors for users.
          - **Deployment failures masked**: Rolling updates may complete even when new Pods are not functioning correctly, leading to degraded service.
          - **Startup race conditions**: Applications that require time to load configurations, warm caches, or establish connections may fail initial requests.
          - **Service discovery issues**: Endpoints are added to Services before the application can handle requests, causing intermittent failures.
          - **Health check confusion**: Without readiness probes, load balancers cannot accurately determine which Pods should receive traffic.

        Configuring readiness probes ensures traffic is only routed to containers that are fully prepared to handle requests, improving reliability during deployments and scaling events.
      audit: |
        Check for the existence of `readinessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `readinessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-deployment-readinessProbe
    title: Container should configure a readinessProbe
    impact: 20
    mql: |
      k8s.deployment {
        containers {
          probeSpecified = readinessProbe['httpGet'] != empty || readinessProbe['tcpSocket'] != empty || readinessProbe['exec'] != empty
          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a readinessProbe, which allows Kubernetes to determine when a container is ready to accept traffic.

        **Why this matters**

        Without readiness probes, Kubernetes cannot distinguish between containers that are starting up and those that are ready to serve requests:

          - **Premature traffic routing**: New Pods receive traffic before the application is fully initialized, causing errors for users.
          - **Deployment failures masked**: Rolling updates may complete even when new Pods are not functioning correctly, leading to degraded service.
          - **Startup race conditions**: Applications that require time to load configurations, warm caches, or establish connections may fail initial requests.
          - **Service discovery issues**: Endpoints are added to Services before the application can handle requests, causing intermittent failures.
          - **Health check confusion**: Without readiness probes, load balancers cannot accurately determine which Pods should receive traffic.

        Configuring readiness probes ensures traffic is only routed to containers that are fully prepared to handle requests, improving reliability during deployments and scaling events.
      audit: |
        Check for the existence of `readinessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `readinessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-replicaset-readinessProbe
    title: Container should configure a readinessProbe
    impact: 20
    mql: |
      k8s.replicaset {
        containers {
          probeSpecified = readinessProbe['httpGet'] != empty || readinessProbe['tcpSocket'] != empty || readinessProbe['exec'] != empty

          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a readinessProbe, which allows Kubernetes to determine when a container is ready to accept traffic.

        **Why this matters**

        Without readiness probes, Kubernetes cannot distinguish between containers that are starting up and those that are ready to serve requests:

          - **Premature traffic routing**: New Pods receive traffic before the application is fully initialized, causing errors for users.
          - **Deployment failures masked**: Rolling updates may complete even when new Pods are not functioning correctly, leading to degraded service.
          - **Startup race conditions**: Applications that require time to load configurations, warm caches, or establish connections may fail initial requests.
          - **Service discovery issues**: Endpoints are added to Services before the application can handle requests, causing intermittent failures.
          - **Health check confusion**: Without readiness probes, load balancers cannot accurately determine which Pods should receive traffic.

        Configuring readiness probes ensures traffic is only routed to containers that are fully prepared to handle requests, improving reliability during deployments and scaling events.
      audit: |
        Check for the existence of `readinessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `readinessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-daemonset-readinessProbe
    title: Container should configure a readinessProbe
    impact: 20
    mql: |
      k8s.daemonset {
        containers {
          probeSpecified = readinessProbe['httpGet'] != empty || readinessProbe['tcpSocket'] != empty || readinessProbe['exec'] != empty

          probeSpecified == true
        }
      }
    docs:
      desc: |
        This check ensures that containers define a readinessProbe, which allows Kubernetes to determine when a container is ready to accept traffic.

        **Why this matters**

        Without readiness probes, Kubernetes cannot distinguish between containers that are starting up and those that are ready to serve requests:

          - **Premature traffic routing**: New Pods receive traffic before the application is fully initialized, causing errors for users.
          - **Deployment failures masked**: Rolling updates may complete even when new Pods are not functioning correctly, leading to degraded service.
          - **Startup race conditions**: Applications that require time to load configurations, warm caches, or establish connections may fail initial requests.
          - **Service discovery issues**: Endpoints are added to Services before the application can handle requests, causing intermittent failures.
          - **Health check confusion**: Without readiness probes, load balancers cannot accurately determine which Pods should receive traffic.

        Configuring readiness probes ensures traffic is only routed to containers that are fully prepared to handle requests, improving reliability during deployments and scaling events.
      audit: |
        Check for the existence of `readinessProbe`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
      remediation: |
        Define a `readinessProbe` in the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              readinessProbe: # <--- Set a readinessProbe like this
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 5
                periodSeconds: 5
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
        title: Pod Lifecycle - Container probes
  - uid: mondoo-kubernetes-best-practices-pod-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.pod {
        podSpec['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-cronjob-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.cronjob {
        manifest['spec']['jobTemplate']['spec']['template']['spec']['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-statefulset-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.statefulset {
        manifest['spec']['template']['spec']['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-deployment-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.deployment {
        manifest['spec']['template']['spec']['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Deployment
        spec:
          ...
          spec:
            hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
            containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-job-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.job {
        manifest['spec']['template']['spec']['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-replicaset-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.replicaset {
        manifest['spec']['template']['spec']['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-daemonset-hostalias
    title: Pod should not define hostAliases
    impact: 60
    mql: |
      k8s.daemonset {
        manifest['spec']['template']['spec']['hostAliases'] == null
      }
    docs:
      desc: |
        This check ensures that Pods do not use hostAliases to modify the container's /etc/hosts file with custom DNS entries.

        **Why this matters**

        Using hostAliases to manage DNS entries locally within Pods bypasses centralized DNS management and creates operational and security risks:

          - **Configuration drift**: Local DNS overrides can become inconsistent across Pods, leading to unpredictable behavior and difficult-to-debug connectivity issues.
          - **Security bypass**: Attackers who compromise a Pod definition could redirect traffic to malicious endpoints by adding fraudulent hostAliases entries.
          - **Maintenance burden**: Changes to DNS mappings require redeploying workloads rather than updating centralized DNS records.
          - **Service discovery conflicts**: hostAliases can override Kubernetes DNS service discovery, causing Pods to connect to wrong endpoints.
          - **Audit challenges**: Local DNS modifications are harder to track and audit compared to centralized DNS configurations.

        Use Kubernetes DNS, CoreDNS configurations, or external DNS services instead of hostAliases for managing DNS entries in a consistent and auditable manner.
      audit: |
        Check for the existence of `hostAliases` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostAliases: # <--- Don't set DNS entries using hostAliases
            - ip: "127.0.0.1"
              hostnames:
                - "foo.local"
                - "bar.local"
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: Remove the `hostAliases` setting from the Pod spec. Instead, use a proper DNS service or Kubernetes DNS for managing DNS entries.
    refs:
      - url: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
        title: Adding entries to Pod /etc/hosts with HostAliases
  - uid: mondoo-kubernetes-best-practices-pod-default-namespace
    title: Workloads should not run in default namespace
    impact: 20
    mql: |
      k8s.pod {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no Pods:

        ```yaml
        kubectl get pods -n default
        ```
      remediation: |
        For any Pods running in the default Namespace, update/redeploy the Pods (or the parent Deployment, CronJob, etc) to a non-default Namespace:

        ```yaml
        apiVersion:v1
        kind: Pod
        metadata:
          name: examplePod
          namespace: pod-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-daemonset-default-namespace
    title: DaemonSets should not run in the default namespace
    impact: 20
    mql: |
      k8s.daemonset {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no DaemonSets:

        ```yaml
        kubectl get daemonsets -n default
        ```
      remediation: |
        For any Daemonsets running in the default Namespace, update/redeploy the DaemonSets to a non-default Namespace:

        ```yaml
        apiVersion:apps/v1
        kind: DaemonSet
        metadata:
          name: exampleDaemonSet
          namespace: daemonset-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-replicaset-default-namespace
    title: ReplicaSets should not run in the default namespace
    impact: 20
    mql: |
      k8s.replicaset {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no ReplicaSets:

        ```yaml
        kubectl get replicasets -n default
        ```
      remediation: |
        For any ReplicaSets running in the default Namespace, update/redeploy the ReplicaSets (or the parent Deployment) to a non-default Namespace:

        ```yaml
        apiVersion:apps/v1
        kind: ReplicaSet
        metadata:
          name: exampleReplicaSet
          namespace: replicaset-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-job-default-namespace
    title: Jobs should not run in the default namespace
    impact: 20
    mql: |
      k8s.job {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no Jobs:

        ```yaml
        kubectl get jobs -n default
        ```
      remediation: |
        For any Jobs running in the default Namespace, update/redeploy the Jobs (or the parent CronJobs) to a non-default Namespace:

        ```yaml
        apiVersion:batch/v1
        kind: Job
        metadata:
          name: exampleJob
          namespace: job-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-deployment-default-namespace
    title: Deployments should not run in the default namespace
    impact: 20
    mql: |
      k8s.deployment {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no Deployments:

        ```yaml
        kubectl get deployments -n default
        ```
      remediation: |
        For any Deployments running in the default Namespace, update/redeploy the Deployments to a non-default Namespace:

        ```yaml
        apiVersion:apps/v1
        kind: Deployment
        metadata:
          name: exampleDeployment
          namespace: deployment-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-statefulset-default-namespace
    title: StatefulSets should not run in the default namespace
    impact: 20
    mql: |
      k8s.statefulset {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no StatefulSets:

        ```yaml
        kubectl get statefulsets -n default
        ```
      remediation: |
        For any StatefulSets running in the default Namespace, update/redeploy the StatefulSets to a non-default Namespace:

        ```yaml
        apiVersion:apps/v1
        kind: StatefulSet
        metadata:
          name: exampleStatefulset
          namespace: statefulset-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-cronjob-default-namespace
    title: CronJobs should not run in the default namespace
    impact: 20
    mql: |
      k8s.cronjob {
        namespace != "default"
      }
    docs:
      desc: |
        This check verifies that workloads are deployed to a namespace other than `default`, ensuring proper organization and security isolation within the cluster.

        **Why this matters**

        The default namespace lacks the security controls and resource isolation that dedicated namespaces provide:

          - **No access controls**: Resources in default cannot be easily restricted with RBAC policies, allowing any user with cluster access to view or modify workloads.
          - **No resource isolation**: Without namespace-specific Resource Quotas and Limit Ranges, workloads compete for cluster resources without constraints.
          - **Network exposure**: Network Policies cannot effectively isolate traffic when workloads share the default namespace with other applications.
          - **Operational confusion**: Mixed workloads in default make it difficult to track ownership, apply consistent policies, or audit resources.
          - **Accidental access**: Developers may inadvertently access or modify production workloads if everything runs in default.

        Deploying workloads to dedicated namespaces enables fine-grained RBAC, resource quotas, network segmentation, and clearer organizational boundaries.
      audit: |
        Check to ensure no workloads are running in the default Namespace. this command should return no CronJobs:

        ```yaml
        kubectl get cronjob -n default
        ```
      remediation: |
        For any CronJobs running in the default Namespace, update/redeploy the CronJobs to a non-default Namespace:

        ```yaml
        apiVersion:batch/v1
        kind: CronJob
        metadata:
          name: exampleCronJob
          namespace: cronjob-namespace # <--- Define a namespace for workloads
        ```
    refs:
      - url: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-organizing-with-namespaces
        title: "Kubernetes best practices: Organizing with Namespaces"
      - url: https://kubernetes.io/docs/concepts/services-networking/network-policies/
        title: Kubernetes Network Policies
      - url: https://kubernetes.io/docs/concepts/policy/limit-range/
        title: Kubernetes Limit Ranges
  - uid: mondoo-kubernetes-best-practices-pod-ports-hostport
    title: Pods should not bind to a host port
    impact: 40
    mql: |
      k8s.pod.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no Pods are binding any of their containers to a host port:

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | select( (.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any Pods that bind to a host port, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not bind to a host port:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - ports:
              - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                name: http
                protocol: TCP
              - containerPort: 443
                name: https
                protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-daemonset-ports-hostport
    title: DaemonSets should not bind to a host port
    impact: 40
    mql: |
      k8s.daemonset.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no DaemonSets are binding any of their containers to a host port:

        ```bash
        kubectl get daemonsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any DaemonSets that bind to a host port, update the DaemonSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-replicaset-ports-hostport
    title: ReplicaSets should not bind to a host port
    impact: 40
    mql: |
      k8s.replicaset.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no ReplicaSets are binding any of their containers to a host port:

        ```bash
        kubectl get replicasets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any ReplicaSets that bind to a host port, update the ReplicaSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-job-ports-hostport
    title: Jobs should not bind to a host port
    impact: 40
    mql: |
      k8s.job.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no Jobs are binding any of their containers to a host port:

        ```bash
        kubectl get jobs -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any ReplicaSets that bind to a host port, update the Jobs to ensure they do not bind to a host port:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-deployment-ports-hostport
    title: Deployments should not bind to a host port
    impact: 40
    mql: |
      k8s.deployment.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no Deployments are binding any of their containers to a host port:

        ```bash
        kubectl get deployments -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any Deployments that bind to a host port, update the Deployments to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-statefulset-ports-hostport
    title: StatefulSets should not bind to a host port
    impact: 40
    mql: |
      k8s.statefulset.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no StatefulSets are binding any of their containers to a host port:

        ```bash
        kubectl get statefulsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any StatefulSets that bind to a host port, update the StatefulSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-cronjob-ports-hostport
    title: CronJobs should not bind to a host port
    impact: 40
    mql: |
      k8s.cronjob.podSpec {
        _['containers'] {
          _['name']
          _['ports'] == null || _['ports'].all(_['hostPort'] == null)
        }
      }
    docs:
      desc: |
        This check verifies that containers do not bind to host ports, which directly exposes the container on the node's network interface.

        **Why this matters**

        Using host ports creates security and operational risks that undermine Kubernetes' networking model:

          - **Network exposure**: Host ports bypass Kubernetes Services and expose containers directly to the network, potentially to external traffic.
          - **Scheduling constraints**: Only one Pod can use a specific host port per node, severely limiting scheduling flexibility and horizontal scaling.
          - **Port conflicts**: Multiple applications competing for the same host ports can cause deployment failures and unpredictable behavior.
          - **Security boundary bypass**: Host ports allow direct network access to containers, circumventing network policies and service mesh controls.
          - **Audit complexity**: Traffic to host ports is harder to monitor, log, and trace through standard Kubernetes observability tools.

        Use Kubernetes Services (ClusterIP, NodePort, or LoadBalancer) instead of host ports to properly manage network access and maintain security boundaries.
      audit: |
        Check to ensure no CronJobs are binding any of their containers to a host port:

        ```bash
        kubectl get cronjobs -A -o json | jq -r '.items[] | select( (.spec.jobTemplate.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any CronJobs that bind to a host port, update the CronJobs to ensure they do not bind to a host port:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - ports:
                      - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                        name: http
                        protocol: TCP
                      - containerPort: 443
                        name: https
                        protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: "Kubernetes Configuration Best Practices: hostPort"
  - uid: mondoo-kubernetes-best-practices-ingress-cert-expiration
    title: Ingress certificates less than 15 days from expiration
    impact: 40
    mql: |
      k8s.ingress.tls.all(
        certificates.all(
          expiresIn.days > 15
        )
      )
    docs:
      desc: |
        This check verifies that TLS certificates used by Ingress resources have more than 15 days remaining before expiration.

        **Why this matters**

        Expired or soon-to-expire TLS certificates cause immediate service disruptions and security risks:

          - **Service outages**: When certificates expire, browsers and clients refuse connections, causing complete service unavailability for HTTPS traffic.
          - **Security warnings**: Users encounter browser security warnings that damage trust and may lead them to bypass important security protections.
          - **Compliance violations**: Many security standards require valid TLS certificates; expired certificates can trigger audit failures.
          - **Emergency remediation**: Certificate renewals done under time pressure are error-prone and may cause additional outages during replacement.
          - **Man-in-the-middle exposure**: Users accustomed to ignoring certificate warnings become vulnerable to interception attacks.

        Implement automated certificate management (such as cert-manager) and monitor certificate expiration dates to ensure timely renewal well before expiration.
      audit: |
        Check to ensure no Ingress Secrets contain TLS certificates near expiration:

        Display all Ingress resources with TLS Secret data:

        ```bash
        kubectl get ingress -A -o json | jq -r '.items[] | select(.spec.tls != empty) | .metadata.namespace + "/" + .metadata.name'
        ```

        For each Ingress resource, check the certificate expiration dates in the Secrets (under `.spec.tls[].secretName`). Ensure that the expiration dates don't expire soon:

        ```bash
        kubectl get secret --namespace NAMESPACE_OF_INGRESS NAME_OF_SECRET -o json | jq -r '.data["tls.crt"]' | base64 -d | openssl x509 -noout -text | grep "Not After"
        ```
      remediation: |
        For all Secrets with expired or soon-to-expire certificates, update the Secret data with refreshed certificates.
