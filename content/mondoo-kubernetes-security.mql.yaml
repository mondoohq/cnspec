# Copyright (c) Mondoo, Inc.
# SPDX-License-Identifier: BUSL-1.1
policies:
  - uid: mondoo-kubernetes-security
    name: Mondoo Kubernetes Cluster and Workload Security
    version: 1.2.1
    license: BUSL-1.1
    tags:
      mondoo.com/category: security
      mondoo.com/platform: kubernetes
    authors:
      - name: Mondoo, Inc
        email: hello@mondoo.com
    docs:
      desc: |-
        # Overview

        The Mondoo Kubernetes Cluster and Workload Security policy provides guidance for establishing secure Kubernetes cluster configurations and workload deployments.

        If you have questions, comments, or have identified ways to improve this policy, please write us at hello@mondoo.com, or reach out in [GitHub Discussions](https://github.com/orgs/mondoohq/discussions).

        ## Remote scan

        Remote scans use cnspec providers to retrieve on-demand scan results without having to install any agents.

        ### Prerequisites

        Remote scans of Kubernetes clusters requires a `KUBECONFIG` with access to the cluster you want to scan.

        ### Scan a Kubernetes cluster

        Open a terminal and configure an environment variable with the path to your `KUBECONFIG`:

        ```bash
        export KUBECONFIG=/path/to/kubeconfig
        ```

        Run a scan of the Kubernetes cluster:

        ```bash
        cnspec scan k8s
        ```

        ## Join the community!

        Our goal is to build policies that are simple to deploy, accurate, and actionable.

        If you have any suggestions for how to improve this policy, or if you need support, [join the community](https://github.com/orgs/mondoohq/discussions) in GitHub Discussions.
    groups:
      - title: Kubernetes API Server
        filters: |
          asset.family.contains('linux')
          processes.where( executable == /kube-apiserver/ ).list != []
        checks:
          - uid: mondoo-kubernetes-security-api-server-no-anonymous-auth
          - uid: mondoo-kubernetes-security-https-api-server
          - uid: mondoo-kubernetes-security-secure-admin-conf
          - uid: mondoo-kubernetes-security-secure-controller-manager_conf
          - uid: mondoo-kubernetes-security-secure-etcd-data-dir
          - uid: mondoo-kubernetes-security-secure-kube-apiserver-yml
          - uid: mondoo-kubernetes-security-secure-pki-directory
          - uid: mondoo-kubernetes-security-secure-scheduler_conf
      - title: Kubernetes kubelet
        filters: |
          asset.family.contains('linux')
          processes.where( executable == /kubelet/ ).list != []
        checks:
          - uid: mondoo-kubernetes-security-kubelet-anonymous-authentication
          - uid: mondoo-kubernetes-security-kubelet-authorization-mode
          - uid: mondoo-kubernetes-security-kubelet-event-record-qps
          - uid: mondoo-kubernetes-security-kubelet-iptables-util-chains
          - uid: mondoo-kubernetes-security-kubelet-protect-kernel-defaults
          - uid: mondoo-kubernetes-security-kubelet-read-only-port
          - uid: mondoo-kubernetes-security-kubelet-rotate-certificates
          - uid: mondoo-kubernetes-security-kubelet-strong-ciphers
          - uid: mondoo-kubernetes-security-kubelet-tls-certificate
          - uid: mondoo-kubernetes-security-secure-kubelet-cert-authorities
          - uid: mondoo-kubernetes-security-secure-kubelet-config
      - title: Kubernetes CronJobs Security
        filters: asset.platform == "k8s-cronjob"
        checks:
          - uid: mondoo-kubernetes-security-cronjob-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-cronjob-capability-net-raw
          - uid: mondoo-kubernetes-security-cronjob-capability-sys-admin
          - uid: mondoo-kubernetes-security-cronjob-containerd-socket
          - uid: mondoo-kubernetes-security-cronjob-crio-socket
          - uid: mondoo-kubernetes-security-cronjob-docker-socket
          - uid: mondoo-kubernetes-security-cronjob-hostipc
          - uid: mondoo-kubernetes-security-cronjob-hostnetwork
          - uid: mondoo-kubernetes-security-cronjob-hostpath-readonly
          - uid: mondoo-kubernetes-security-cronjob-hostpid
          - uid: mondoo-kubernetes-security-cronjob-imagepull
          - uid: mondoo-kubernetes-security-cronjob-limitcpu
          - uid: mondoo-kubernetes-security-cronjob-limitmemory
          - uid: mondoo-kubernetes-security-cronjob-ports-hostport
          - uid: mondoo-kubernetes-security-cronjob-privilegedcontainer
          - uid: mondoo-kubernetes-security-cronjob-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-cronjob-runasnonroot
          - uid: mondoo-kubernetes-security-cronjob-serviceaccount
      - title: Kubernetes StatefulSets Security
        filters: asset.platform == "k8s-statefulset"
        checks:
          - uid: mondoo-kubernetes-security-statefulset-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-statefulset-capability-net-raw
          - uid: mondoo-kubernetes-security-statefulset-capability-sys-admin
          - uid: mondoo-kubernetes-security-statefulset-containerd-socket
          - uid: mondoo-kubernetes-security-statefulset-crio-socket
          - uid: mondoo-kubernetes-security-statefulset-docker-socket
          - uid: mondoo-kubernetes-security-statefulset-hostipc
          - uid: mondoo-kubernetes-security-statefulset-hostnetwork
          - uid: mondoo-kubernetes-security-statefulset-hostpath-readonly
          - uid: mondoo-kubernetes-security-statefulset-hostpid
          - uid: mondoo-kubernetes-security-statefulset-imagepull
          - uid: mondoo-kubernetes-security-statefulset-limitcpu
          - uid: mondoo-kubernetes-security-statefulset-limitmemory
          - uid: mondoo-kubernetes-security-statefulset-ports-hostport
          - uid: mondoo-kubernetes-security-statefulset-privilegedcontainer
          - uid: mondoo-kubernetes-security-statefulset-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-statefulset-runasnonroot
          - uid: mondoo-kubernetes-security-statefulset-serviceaccount
      - title: Kubernetes Deployments Security
        filters: asset.platform == "k8s-deployment"
        checks:
          - uid: mondoo-kubernetes-security-deployment-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-deployment-capability-net-raw
          - uid: mondoo-kubernetes-security-deployment-capability-sys-admin
          - uid: mondoo-kubernetes-security-deployment-containerd-socket
          - uid: mondoo-kubernetes-security-deployment-crio-socket
          - uid: mondoo-kubernetes-security-deployment-docker-socket
          - uid: mondoo-kubernetes-security-deployment-hostipc
          - uid: mondoo-kubernetes-security-deployment-hostnetwork
          - uid: mondoo-kubernetes-security-deployment-hostpath-readonly
          - uid: mondoo-kubernetes-security-deployment-hostpid
          - uid: mondoo-kubernetes-security-deployment-imagepull
          - uid: mondoo-kubernetes-security-deployment-k8s-dashboard
          - uid: mondoo-kubernetes-security-deployment-limitcpu
          - uid: mondoo-kubernetes-security-deployment-limitmemory
          - uid: mondoo-kubernetes-security-deployment-ports-hostport
          - uid: mondoo-kubernetes-security-deployment-privilegedcontainer
          - uid: mondoo-kubernetes-security-deployment-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-deployment-runasnonroot
          - uid: mondoo-kubernetes-security-deployment-serviceaccount
          - uid: mondoo-kubernetes-security-deployment-tiller
      - title: Kubernetes Jobs Security
        filters: asset.platform == "k8s-job"
        checks:
          - uid: mondoo-kubernetes-security-job-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-job-capability-net-raw
          - uid: mondoo-kubernetes-security-job-capability-sys-admin
          - uid: mondoo-kubernetes-security-job-containerd-socket
          - uid: mondoo-kubernetes-security-job-crio-socket
          - uid: mondoo-kubernetes-security-job-docker-socket
          - uid: mondoo-kubernetes-security-job-hostipc
          - uid: mondoo-kubernetes-security-job-hostnetwork
          - uid: mondoo-kubernetes-security-job-hostpath-readonly
          - uid: mondoo-kubernetes-security-job-hostpid
          - uid: mondoo-kubernetes-security-job-imagepull
          - uid: mondoo-kubernetes-security-job-limitcpu
          - uid: mondoo-kubernetes-security-job-limitmemory
          - uid: mondoo-kubernetes-security-job-ports-hostport
          - uid: mondoo-kubernetes-security-job-privilegedcontainer
          - uid: mondoo-kubernetes-security-job-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-job-runasnonroot
          - uid: mondoo-kubernetes-security-job-serviceaccount
      - title: Kubernetes ReplicaSets Security
        filters: asset.platform == "k8s-replicaset"
        checks:
          - uid: mondoo-kubernetes-security-replicaset-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-replicaset-capability-net-raw
          - uid: mondoo-kubernetes-security-replicaset-capability-sys-admin
          - uid: mondoo-kubernetes-security-replicaset-containerd-socket
          - uid: mondoo-kubernetes-security-replicaset-crio-socket
          - uid: mondoo-kubernetes-security-replicaset-docker-socket
          - uid: mondoo-kubernetes-security-replicaset-hostipc
          - uid: mondoo-kubernetes-security-replicaset-hostnetwork
          - uid: mondoo-kubernetes-security-replicaset-hostpath-readonly
          - uid: mondoo-kubernetes-security-replicaset-hostpid
          - uid: mondoo-kubernetes-security-replicaset-imagepull
          - uid: mondoo-kubernetes-security-replicaset-limitcpu
          - uid: mondoo-kubernetes-security-replicaset-limitmemory
          - uid: mondoo-kubernetes-security-replicaset-ports-hostport
          - uid: mondoo-kubernetes-security-replicaset-privilegedcontainer
          - uid: mondoo-kubernetes-security-replicaset-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-replicaset-runasnonroot
          - uid: mondoo-kubernetes-security-replicaset-serviceaccount
      - title: Kubernetes DaemonSets Security
        filters: asset.platform == "k8s-daemonset"
        checks:
          - uid: mondoo-kubernetes-security-daemonset-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-daemonset-capability-net-raw
          - uid: mondoo-kubernetes-security-daemonset-capability-sys-admin
          - uid: mondoo-kubernetes-security-daemonset-containerd-socket
          - uid: mondoo-kubernetes-security-daemonset-crio-socket
          - uid: mondoo-kubernetes-security-daemonset-docker-socket
          - uid: mondoo-kubernetes-security-daemonset-hostipc
          - uid: mondoo-kubernetes-security-daemonset-hostnetwork
          - uid: mondoo-kubernetes-security-daemonset-hostpath-readonly
          - uid: mondoo-kubernetes-security-daemonset-hostpid
          - uid: mondoo-kubernetes-security-daemonset-imagepull
          - uid: mondoo-kubernetes-security-daemonset-limitcpu
          - uid: mondoo-kubernetes-security-daemonset-limitmemory
          - uid: mondoo-kubernetes-security-daemonset-ports-hostport
          - uid: mondoo-kubernetes-security-daemonset-privilegedcontainer
          - uid: mondoo-kubernetes-security-daemonset-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-daemonset-runasnonroot
          - uid: mondoo-kubernetes-security-daemonset-serviceaccount
      - title: Kubernetes Pods Security
        filters: asset.platform == "k8s-pod"
        checks:
          - uid: mondoo-kubernetes-security-pod-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-pod-capability-net-raw
          - uid: mondoo-kubernetes-security-pod-capability-sys-admin
          - uid: mondoo-kubernetes-security-pod-containerd-socket
          - uid: mondoo-kubernetes-security-pod-crio-socket
          - uid: mondoo-kubernetes-security-pod-docker-socket
          - uid: mondoo-kubernetes-security-pod-hostipc
          - uid: mondoo-kubernetes-security-pod-hostnetwork
          - uid: mondoo-kubernetes-security-pod-hostpath-readonly
          - uid: mondoo-kubernetes-security-pod-hostpid
          - uid: mondoo-kubernetes-security-pod-imagepull
          - uid: mondoo-kubernetes-security-pod-k8s-dashboard
          - uid: mondoo-kubernetes-security-pod-limitcpu
          - uid: mondoo-kubernetes-security-pod-limitmemory
          - uid: mondoo-kubernetes-security-pod-ports-hostport
          - uid: mondoo-kubernetes-security-pod-privilegedcontainer
          - uid: mondoo-kubernetes-security-pod-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-pod-runasnonroot
          - uid: mondoo-kubernetes-security-pod-serviceaccount
          - uid: mondoo-kubernetes-security-pod-tiller
    scoring_system: highest impact
props:
  - uid: mondooKubernetesSecurityAllowedCiphers
    title: Define the hardened SSL/ TLS ciphers
    mql: |
      return ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256",
        "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA",
        "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
        "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA",
        "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
        "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
        "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
        "TLS_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256",
        "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"]
queries:
  - uid: mondoo-kubernetes-security-kubelet-anonymous-authentication
    title: Disable anonymous authentication for kubelet
    impact: 100
    mql: |
      kubelet.configuration['authentication']['anonymous']['enabled'] == false
    docs:
      desc: |
        This check ensures that anonymous authentication is disabled for the kubelet. Disabling anonymous authentication prevents unauthenticated users from accessing the kubelet's HTTPS endpoint, which could otherwise expose sensitive cluster information.

        **Why this matters**

        The kubelet is a core Kubernetes component responsible for managing pods on each node. If anonymous authentication is enabled:

        - Unauthenticated users may gain access to the kubelet API, potentially exposing sensitive data or cluster state.
        - Attackers could exploit this access to gather information or attempt further attacks on the cluster.
        - Allowing anonymous requests undermines access control and violates security best practices.

        By ensuring anonymous authentication is disabled, organizations reduce the risk of unauthorized access and strengthen the overall security posture of their Kubernetes clusters.
      audit: |
        If running the kubelet with the CLI parameter '--anonymous-auth', or running with 'authentication.anonymous.enabled' defined in the kubelet configuration file, ensure that the value is set to 'false'.
      remediation: |
        Set the '--anonymous-auth' CLI parameter and/or the 'authentication.anonymous.enabled' field in the kubelet configuration file to 'false'.
    refs:
      - url: https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication
        title: Kubelet authentication
  - uid: mondoo-kubernetes-security-kubelet-event-record-qps
    title: Configure kubelet to capture all event creation
    impact: 30
    mql: |
      kubelet.configuration['eventRecordQPS'] == 0
    docs:
      desc: |
        This check ensures that the kubelet is configured to capture all event creation by setting the event record QPS (queries per second) to 0. This configuration guarantees that all events are logged, which is important for auditing and troubleshooting purposes.

        **Why this matters**

        Capturing all event creation in the kubelet is essential for maintaining a complete audit trail and ensuring visibility into cluster activity. If event recording is limited or disabled:

        - Important events may not be logged, making it difficult to investigate incidents or troubleshoot issues.
        - Security-relevant activities could go undetected, increasing the risk of unnoticed attacks or misconfigurations.
        - Compliance with auditing and monitoring requirements may be compromised.

        By ensuring the kubelet is configured to capture all event creation, organizations can improve observability, support incident response, and maintain compliance with security best practices.
      audit: |
        If running the kubelet with the CLI parameter '--event-qps', or running with 'eventRecordQPS' defined in the kubelet configuration file, ensure that the value is set to '0'.
      remediation: |
        Set the '--event-qps' CLI parameter and/or the 'eventRecordQPS' field in the kubelet configuration file to '0'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-iptables-util-chains
    title: Configure kubelet to ensure IPTables rules are set on host
    impact: 30
    mql: |
      kubelet.configuration['makeIPTablesUtilChains'] == true
    docs:
      desc: |
        This check ensures that the kubelet is set up to create IPTables utility rules for various Kubernetes components. This configuration is important for maintaining correct network traffic routing and enforcing security policies at the node level.

        **Why this matters**

        IPTables utility chains are used by Kubernetes to manage network rules for pods and services. If the kubelet is not configured to create these utility chains:

        - Network traffic may not be properly routed, leading to connectivity issues between pods and services.
        - Security policies that rely on IPTables rules may not be enforced, increasing the risk of unauthorized access.
        - Troubleshooting and auditing network flows can become more difficult due to inconsistent or missing IPTables rules.

        By ensuring the kubelet is configured to create IPTables utility chains, organizations can maintain reliable network operations and enforce security best practices within their Kubernetes clusters.
      audit: |
        If running the kubelet with the CLI parameter '--make-iptables-util-chains', or running with 'makeIPTablesUtilChains' defined in the kubelet configuration file, ensure that the value is set to 'true'.
      remediation: |
        Set the '--make-iptables-util-chains' CLI parameter and/or the 'makeIPTablesUtilChains' field in the kubelet configuration file to 'true'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-protect-kernel-defaults
    title: Configure kubelet to protect kernel defaults
    impact: 60
    mql: |
      kubelet.configuration["protectKernelDefaults"] == "true"
    docs:
      desc: |
        This check ensures that the kubelet is configured to protect kernel defaults by setting `protectKernelDefaults` to `true`. This configuration prevents the kubelet from modifying kernel tunables at startup and enforces the use of secure, recommended kernel settings.

        **Why this matters**

        The kubelet manages pods and containers on each node and interacts closely with the underlying operating system. If `protectKernelDefaults` is not enabled:

        - The kubelet may attempt to change kernel parameters, potentially introducing insecure or inconsistent system states.
        - Security best practices may be bypassed if the kubelet overrides hardened kernel settings.
        - Unintended modifications to kernel tunables can lead to unpredictable behavior or vulnerabilities.

        By ensuring that `protectKernelDefaults` is set to `true`, organizations can maintain consistent, secure kernel configurations and reduce the risk of misconfiguration or privilege escalation.
      audit: |
        If running the kubelet with the CLI parameter '--protect-kernel-defaults', or running with 'protectKernelDefaults' defined in the kubelet configuration file, ensure that the value is set to 'true'.
      remediation: |
        Set the '--protect-kernel-defaults' CLI parameter and/or the 'protectKernelDefaults' field in the kubelet configuration file to 'true'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-read-only-port
    title: Do not allow unauthenticated read-only port on kubelet
    impact: 60
    mql: |
      kubelet.configuration['readOnlyPort'] == 0 || kubelet.configuration['readOnlyPort'] == null
    docs:
      desc: |
        This check ensures that the kubelet is not configured to serve unauthenticated read-only access. Disabling the read-only port prevents unauthenticated users from accessing sensitive kubelet information.

        **Why this matters**

        The kubelet read-only port, if enabled, allows unauthenticated access to the kubelet's HTTP endpoint. If this port is open:

        - Unauthenticated users may retrieve sensitive information about pods and containers running on the node.
        - Attackers could exploit this access to gather data or attempt further attacks on the cluster.
        - Allowing unauthenticated requests undermines access control and violates security best practices.

        By ensuring the kubelet read-only port is disabled, organizations reduce the risk of unauthorized access and strengthen the overall security posture of their Kubernetes clusters.
      audit: |
        If running the kubelet with the CLI parameter '--read-only-port', or running with 'readOnlyPort' defined in the kubelet configuration file, ensure that the value is either '0' or simply not set ('0' is the default).
      remediation: |
        Set the '--read-only-port' CLI parameter or the 'readOnlyPort' field in the kubelet configuration file to '0'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-authorization-mode
    title: Ensure the kubelet is not configured with the AlwaysAllow authorization mode
    impact: 100
    mql: |
      kubelet.configuration['authorization']['mode'] != "AlwaysAllow"
    docs:
      desc: |
        This check ensures that the kubelet is not configured with the AlwaysAllow authorization mode. Disabling AlwaysAllow enforces proper access control and prevents unauthorized requests from being automatically permitted.

        **Why this matters**

        The AlwaysAllow authorization mode allows all requests to the kubelet API, bypassing access controls and security policies. If this mode is enabled:

        - Any user or process can interact with the kubelet API without restriction, increasing the risk of privilege escalation or cluster compromise.
        - Sensitive operations and data may be exposed to unauthorized users.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the kubelet is not configured with AlwaysAllow, organizations can enforce strong access controls and maintain a secure Kubernetes environment.
      audit: |
        If running the kubelet with the CLI parameter '--authorization-mode', or running with 'authorization.mode' defined in the kubelet configuration file, ensure that the value is not set to 'AlwaysAllow'.
      remediation: |
        If the kubelet is configured with the CLI parameter '--authorization-mode', set it to something that isn't 'AlwaysAllow' (eg 'Webhook').

        If the kubelet is configured via the kubelet config file with the 'authorization.mode' parameter, set it to something that isn't 'AlwaysAllow' (eg. 'Webhook').
    refs:
      - url: https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization
        title: Kubelet authorization
  - uid: mondoo-kubernetes-security-kubelet-strong-ciphers
    title: Configure kubelet to use only strong cryptography
    impact: 100
    props:
      - uid: mondooKubernetesSecurityAllowedCiphers
        title: Define the hardened SSL/ TLS ciphers
        mql: |
          return ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256",
            "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA",
            "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
            "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA",
            "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
            "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
            "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
            "TLS_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256",
            "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"]
    mql: |
      kubelet.configuration['tlsCipherSuites'] != empty
      if (kubelet.configuration['tlsCipherSuites'] != empty) {
        kubelet.configuration['tlsCipherSuites'].map( _.trim ).containsOnly(props.mondooKubernetesSecurityAllowedCiphers)
      }
    docs:
      desc: |
        This check ensures that the kubelet is configured to use only strong cryptography by restricting the allowed TLS cipher suites to a hardened list. This configuration helps prevent the use of weak or outdated ciphers that could expose sensitive data or allow attackers to compromise encrypted communications.

        **Why this matters**

        The kubelet is a critical component in Kubernetes clusters, responsible for managing pods and communicating with the API server. If the kubelet allows weak or deprecated TLS ciphers:

        - Encrypted traffic between the kubelet and other components may be vulnerable to interception or decryption.
        - Attackers could exploit known weaknesses in legacy ciphers to gain unauthorized access or disrupt cluster operations.
        - Compliance with security standards and best practices may be violated.

        By ensuring the kubelet is restricted to strong, modern TLS ciphers, organizations can protect sensitive data in transit and maintain a robust security posture for their Kubernetes environments.
      audit: |
        If running the kubelet with the CLI parameter '--tls-cipher-suites', or running with 'tlsCipherSuites' defined in the kubelet configuration file, ensure that the list of allowed ciphers is not empty and that all included ciphers are included in the following list:

        "TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
        "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
        "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA",
        "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
        "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_RSA_WITH_3DES_EDE_CBC_SHA",
        "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256", "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"
      remediation: |
        Define the list of allowed TLS ciphers to include only items from the strong list of ciphers.

        If the kubelet is configured with the CLI parameter '--tls-cipher-suites', update the list (or define the parameter) to only include strong ciphers.

        If the kubelet is configured via the kubelet config file with the 'tlsCipherSuites' parameter, update the list (or create an entry for 'tlsCipherSuites') to only include string ciphers.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-tls-certificate
    title: Run kubelet with a user-provided certificate/key
    impact: 100
    mql: |
      kubelet.configuration["tlsCertFile"] != empty
      kubelet.configuration["tlsPrivateKeyFile"] != empty
    docs:
      desc: |
        This check ensures that the kubelet is not running with self-signed certificates generated by the kubelet itself. Instead, it requires the kubelet to use a user-provided certificate and key for secure communication.

        **Why this matters**

        Using self-signed certificates generated by the kubelet can weaken the security of the Kubernetes cluster. If the kubelet is allowed to generate its own certificates:

        - The certificates may not be trusted by other components or external systems, leading to trust issues.
        - Self-signed certificates are more susceptible to man-in-the-middle attacks if not managed properly.
        - Compliance with organizational or regulatory security standards may be compromised.

        By ensuring that the kubelet uses a user-provided certificate and key, organizations can enforce stronger security controls, maintain trust across cluster components, and align with best practices for certificate management.
      audit: |
        The kubelet CLI parameters override values in the kubelet configuration file.

        Check the kubelet CLI parameters to see whether '--tls-cert-file' and '--tls-private-key' are set to a non-empty path/string.

        Check the kubelet configuration file to see whether 'tlsCertFile' and 'tlsPrivateKeyFile' are set to a non-empty path/string.
      remediation: |
        Configure the kubelet to use a user-provided certificate/key pair for serving up HTTPS.

        After acquiring the TLS certificate/key pair, update the kubelet configuration file

        Or if using the deprecated kubelet CLI parameters, update the '--tls-cert-file' and '--tls-private-key-file' parameters to use the new certificate/key.
  - uid: mondoo-kubernetes-security-kubelet-rotate-certificates
    title: Run kubelet with automatic certificate rotation
    impact: 80
    mql: |
      kubelet.configuration["rotateCertificates"] != "false"
    docs:
      desc: |
        This check ensures that the kubelet is running with automatic certificate rotation enabled. This configuration allows the kubelet to automatically renew its certificates with the API server as they approach expiration, maintaining uninterrupted secure communication.

        **Why this matters**

        Automatic certificate rotation is essential for maintaining secure and reliable communication between the kubelet and the Kubernetes API server. If certificate rotation is not enabled:

        - Expired certificates can disrupt communication, causing nodes to become unavailable or workloads to fail.
        - Manual certificate management increases operational overhead and the risk of misconfiguration.
        - Compliance with security best practices and organizational policies may be compromised.

        By enabling automatic certificate rotation, organizations ensure that kubelet certificates are always valid, reducing the risk of outages and maintaining a strong security posture.
      audit: |
        Check the kubelet CLI parameters to ensure '--rotate-certificates' is not set to false, and that the kubelet config file has not set 'rotateCertificates' to false.
      remediation: |
        Depending on where the configuration behavior is defined (CLI parameters override config file values), update the kubelet CLI parameters to set '--rotate-certificates' to true, and/or update the kubelet configuration to set 'rotateCertificates' to true.
    refs:
      - url: https://kubernetes.io/docs/tasks/tls/certificate-rotation/
        title: Configure Certificate Rotation for the Kubelet
  - uid: mondoo-kubernetes-security-secure-kubelet-config
    title: Ownership and permissions of kubelet configuration should be restricted
    impact: 80
    mql: |
      if (kubelet.configFile != empty) {
        if (kubelet.configFile.exists) {
          kubelet.configFile {
            user.name == "root"
            group.name == "root"
          }
          kubelet.configFile.permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |
        This check ensures that the kubelet configuration file is owned by the root user and group, and that its permissions are set to restrict access to only the root user. This configuration helps prevent unauthorized users from accessing or modifying sensitive kubelet settings.

        **Why this matters**

        The kubelet configuration file contains critical settings that control the behavior and security of the Kubernetes node. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive configuration details or credentials.
        - Malicious actors could modify the file to weaken node security or disrupt cluster operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring proper ownership and restrictive permissions on the kubelet configuration file, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      audit: |
        View the kubelet configuration file details:

        ```
        $ ls -l /etc/kubernetes/kubelet.conf
        -rw-r--r-- 1 root root 1155 Sep 21 15:03 /etc/kubernetes/kubelet.conf
        ```
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Update the ownership and permissions:

            ```bash
            chown root:root /etc/kubernetes/kubelet.conf
            chmod 600 /etc/kubernetes/kubelet.conf
            ```
        - id: ansible
          desc: |
            **Using Ansible**

            ```yaml
            ---
            - name: Secure kubelet configuration file
              hosts: all
              become: true

              tasks:
                - name: Ensure kubelet configuration file has correct ownership and permissions
                  ansible.builtin.file:
                    path: /etc/kubernetes/kubelet.conf
                    owner: root
                    group: root
                    mode: '0600'
            ```
        - id: bash
          desc: |
            **Using a Bash Script**

            ```bash
            #!/bin/bash
            set -e

            echo "Securing kubelet configuration file..."
            chown root:root /etc/kubernetes/kubelet.conf
            chmod 600 /etc/kubernetes/kubelet.conf
            ```
  - uid: mondoo-kubernetes-security-secure-kubelet-cert-authorities
    title: Specify a kubelet certificate authorities file and ensure proper ownership and permissions
    impact: 100
    mql: |
      kubelet.configuration['authentication']['x509']['clientCAFile'] != empty
      if (kubelet.configuration['authentication']['x509']['clientCAFile'] != empty) {
        cafile = kubelet.configuration["authentication"]["x509"]["clientCAFile"]
        file(cafile) {
          user.name == "root"
          group.name == "root"
        }
        file(cafile).permissions {
          user_readable == true
          user_executable == false
          group_readable == false
          group_writeable == false
          group_executable == false
          other_readable == false
          other_writeable == false
          other_executable == false
        }
      }
    docs:
      desc: |
        This check ensures that the kubelet's certificate authorities configuration file is owned by the root user and group, and that its permissions are set to restrict access to only the root user. This configuration helps prevent unauthorized users from accessing or modifying the certificate authorities file, which is critical for secure kubelet authentication.

        **Why this matters**

        The certificate authorities file is used to validate client certificates for kubelet authentication. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive certificate data, increasing the risk of credential compromise.
        - Malicious actors could modify the file to weaken authentication or disrupt cluster operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring proper ownership and restrictive permissions on the kubelet's certificate authorities file, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      audit: |
        View the ownership and permissions:

        ```
        $ ls -l /etc/srv/kubernetes/pki/ca-certificates.crt
        -rw------- 1 root root 1159 Sep 13 04:14 /etc/srv/kubernetes/pki/ca-certificates.crt
        ```
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Set the appropriate ownership and permissions:

            ```bash
            chown root:root /etc/srv/kubernetes/pki/ca-certificates.crt
            chmod 600 /etc/srv/kubernetes/pki/ca-certificates.crt
            ```
        - id: ansible
          desc: |
            **Using Ansible**

            ```yaml
            ---
            - name: Secure kubelet certificate authorities file
              hosts: all
              become: true

              tasks:
                - name: Ensure kubelet certificate authorities file has correct ownership and permissions
                  ansible.builtin.file:
                    path: /etc/srv/kubernetes/pki/ca-certificates.crt
                    owner: root
                    group: root
                    mode: '0600'
            ```
        - id: bash
          desc: |
            **Using a Bash Script**

            ```bash
            #!/bin/bash
            set -e

            echo "Securing kubelet certificate authorities file..."
            chown root:root /etc/srv/kubernetes/pki/ca-certificates.crt
            chmod 600 /etc/srv/kubernetes/pki/ca-certificates.crt
            ```
  - uid: mondoo-kubernetes-security-secure-kube-apiserver-yml
    title: Set secure file permissions on the API server pod specification file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/manifests/kube-apiserver.yaml").exists) {
        file("/etc/kubernetes/manifests/kube-apiserver.yaml") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        This check ensures that the API server pod specification file has permissions set to `600` and is owned by `root:root`. This configuration restricts access to the file, ensuring that only the root user can read or modify it.

        **Why this matters**

        The API server pod specification file is critical for the operation and security of the Kubernetes control plane. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive configuration details or credentials.
        - Malicious actors could modify the file to weaken cluster security or disrupt operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the API server pod specification file is owned by `root:root` and has restrictive permissions, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Run this command on the Control Plane node:

            ```bash
            chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml
            chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml
            ```
        - id: ansible
          desc: |
            **Using Ansible**

            ```yaml
            ---
            - name: Secure API server pod specification file
              hosts: all
              become: true

              tasks:
                - name: Ensure API server pod specification file has correct ownership and permissions
                  ansible.builtin.file:
                    path: /etc/kubernetes/manifests/kube-apiserver.yaml
                    owner: root
                    group: root
                    mode: '0600'
            ```
        - id: bash
          desc: |
            **Using a Bash Script**

            ```bash
            #!/bin/bash
            set -e

            echo "Securing API server pod specification file..."
            chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml
            chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml
            ```
  - uid: mondoo-kubernetes-security-secure-etcd-data-dir
    title: Set secure directory permissions on the etcd data directory
    impact: 60
    mql: |
      if (file("/var/lib/etcd").exists) {
        file("/var/lib/etcd") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == true
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "etcd"
          group.name == "etcd"
        }
      } else {
        dir = processes.where( executable == /etcd/ ).list[0].flags["data-dir"]
        file(dir) {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == true
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "etcd"
          group.name == "etcd"
        }
      }
    docs:
      desc: |
        This check ensures that the etcd data directory has permissions set to `700` and is owned by the `etcd` user and group. This configuration restricts access to sensitive etcd data, such as Kubernetes Secrets, to only the etcd service account.

        **Why this matters**

        The etcd data directory contains critical cluster data, including secrets and configuration information. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive data stored in etcd, increasing the risk of credential compromise or data leakage.
        - Malicious actors could modify or delete etcd data, potentially disrupting cluster operations or weakening security controls.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the etcd data directory is owned by the `etcd` user and group and has restrictive permissions, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

            ```bash
            ps -ef | grep etcd
            ```

            Run the below command:

            ```bash
            chmod 700 /var/lib/etcd
            chown etcd:etcd /var/lib/etcd
            ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/secret/
        title: Kubernetes Secrets
  - uid: mondoo-kubernetes-security-secure-admin-conf
    title: Set secure file permissions on the admin.conf file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/admin.conf").exists) {
        file("/etc/kubernetes/admin.conf") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        This check ensures that the `admin.conf` file is owned by `root:root` and has permissions set to `600`. This configuration restricts access to the file, ensuring that only the root user can read or modify it.

        **Why this matters**

        The `admin.conf` file contains credentials and configuration for cluster administration. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive configuration details or credentials.
        - Malicious actors could modify the file to weaken cluster security or disrupt operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the `admin.conf` file is owned by `root:root` and has restrictive permissions, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Run this command on the Control Plane node:

            ```bash
            chmod 600 /etc/kubernetes/admin.conf
            chown root:root /etc/kubernetes/admin.conf
            ```
        - id: ansible
          desc: |
            **Using Ansible**

            ```yaml
            ---
            - name: Secure admin.conf file
              hosts: all
              become: true

              tasks:
                - name: Ensure admin.conf file has correct ownership and permissions
                  ansible.builtin.file:
                    path: /etc/kubernetes/admin.conf
                    owner: root
                    group: root
                    mode: '0600'
            ```
        - id: bash
          desc: |
            **Using a Bash Script**

            ```bash
            #!/bin/bash
            set -e

            echo "Securing admin.conf file..."
            chown root:root /etc/kubernetes/admin.conf
            chmod 600 /etc/kubernetes/admin.conf
            ```
    refs:
      - url: https://kubernetes.io/docs/setup/
        title: Kubernetes Setup
  - uid: mondoo-kubernetes-security-secure-scheduler_conf
    title: Set secure file permissions on the scheduler.conf file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/scheduler.conf").exists) {
        file("/etc/kubernetes/scheduler.conf") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        This check ensures that the `scheduler.conf` file is owned by `root:root` and has permissions set to `600`. This configuration restricts access to the file, ensuring that only the root user can read or modify it.

        **Why this matters**

        The `scheduler.conf` file contains credentials and configuration for the Kubernetes scheduler. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive configuration details or credentials.
        - Malicious actors could modify the file to weaken cluster security or disrupt operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the `scheduler.conf` file is owned by `root:root` and has restrictive permissions, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Ensure that the `scheduler.conf` file has permissions set to `600` and is owned by `root:root` by running the following command on the Control Plane node:

            ```bash
            chmod 600 /etc/kubernetes/scheduler.conf
            chown root:root /etc/kubernetes/scheduler.conf
            ```
        - id: ansible
          desc: |
            **Using Ansible**

            ```yaml
            ---
            - name: Secure scheduler.conf file
              hosts: all
              become: true

              tasks:
                - name: Ensure scheduler.conf file has correct ownership and permissions
                  ansible.builtin.file:
                    path: /etc/kubernetes/scheduler.conf
                    owner: root
                    group: root
                    mode: '0600'
            ```
        - id: bash
          desc: |
            **Using a Bash Script**

            ```bash
            #!/bin/bash
            set -e

            echo "Securing scheduler.conf file..."
            chown root:root /etc/kubernetes/scheduler.conf
            chmod 600 /etc/kubernetes/scheduler.conf
            ```
  - uid: mondoo-kubernetes-security-secure-controller-manager_conf
    title: Set secure file permissions on the controller-manager.conf file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/controller-manager.conf").exists) {
        file("/etc/kubernetes/controller-manager.conf") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        This check ensures that the `controller-manager.conf` file is owned by `root:root` and has permissions set to `600`. This configuration restricts access to the file, ensuring that only the root user can read or modify it.

        **Why this matters**

        The `controller-manager.conf` file contains credentials and configuration for the Kubernetes controller manager. If ownership or permissions are misconfigured:

        - Unauthorized users may gain access to sensitive configuration details or credentials.
        - Malicious actors could modify the file to weaken cluster security or disrupt operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the `controller-manager.conf` file is owned by `root:root` and has restrictive permissions, organizations can reduce the risk of unauthorized access and maintain a secure Kubernetes environment.
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Ensure that the `controller-manager.conf` file has permissions set to `600` and is owned by `root:root` by running the following command on the Control Plane node:

            ```bash
            chmod 600 /etc/kubernetes/controller-manager.conf
            chown root:root /etc/kubernetes/controller-manager.conf
            ```
        - id: ansible
          desc: |
            **Using Ansible**

            ```yaml
            ---
            - name: Secure controller-manager.conf file
              hosts: all
              become: true

              tasks:
                - name: Ensure controller-manager.conf file has correct ownership and permissions
                  ansible.builtin.file:
                    path: /etc/kubernetes/controller-manager.conf
                    owner: root
                    group: root
                    mode: '0600'
            ```
        - id: bash
          desc: |
            **Using a Bash Script**

            ```bash
            #!/bin/bash
            set -e

            echo "Securing controller-manager.conf file..."
            chown root:root /etc/kubernetes/controller-manager.conf
            chmod 600 /etc/kubernetes/controller-manager.conf
            ```
  - uid: mondoo-kubernetes-security-secure-pki-directory
    title: Ensure that the Kubernetes PKI/SSL directory is owned by root:root
    impact: 65
    mql: |
      if (processes.where(executable == /kube-apiserver/).list[0].flags["etcd-certfile"] != empty) {
        clientCAFile = processes.where(executable == /kube-apiserver/).list[0].flags["etcd-certfile"]
        ssldir = file(clientCAFile).dirname
        file(ssldir) {
          user.name == "root"
          group.name == "root"
        }
      } else {
        file("/etc/kubernetes/pki") {
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        This check ensures that the Kubernetes PKI/SSL directory is owned by `root:root`. This configuration restricts access to sensitive certificate and key material, ensuring that only the root user and group can modify or access these files.

        **Why this matters**

        The PKI/SSL directory contains critical certificates and keys used to secure communication within the Kubernetes cluster. If ownership is misconfigured:

        - Unauthorized users may gain access to or modify certificates, compromising cluster security.
        - Attackers could replace trusted certificates, enabling man-in-the-middle attacks or unauthorized access.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the PKI/SSL directory is owned by `root:root`, organizations can maintain strong access controls and protect the integrity of cluster encryption and authentication.
      remediation:
        - id: cli
          desc: |
            **Using CLI**

            Ensure that the PKI/SSL directory is owned by `root:root` by running the following command on the Control Plane node:

            ```bash
            chown -R root:root /etc/kubernetes/pki/
            ```

            or if your PKI/SSL directory is located elsewhere, adjust the path accordingly such as:

            ```bash
            chown -R root:root /etc/kubernetes/ssl/
            ```
    refs:
      - url: https://kubernetes.io/docs/setup/best-practices/certificates/
        title: PKI certificates and requirements
  - uid: mondoo-kubernetes-security-https-api-server
    title: Ensure the kube-apiserver is not listening on an insecure HTTP port
    impact: 70
    mql: |
      processes.where(executable == /kube-apiserver/).list {
        flags["insecure-port"] == 0
      }
    docs:
      desc: |
        This check ensures that the kube-apiserver is not listening on an insecure HTTP port. Disabling the insecure port enforces encrypted communication and prevents sensitive data from being transmitted in plaintext.

        **Why this matters**

        The Kubernetes API server is the central management point for the cluster. If it listens on an insecure HTTP port:

        - Unencrypted traffic can be intercepted, exposing sensitive cluster data and credentials.
        - Attackers may exploit the insecure port to gain unauthorized access or disrupt cluster operations.
        - Compliance with security best practices and regulatory requirements may be violated.

        By ensuring the kube-apiserver does not listen on an insecure HTTP port, organizations can protect sensitive data in transit and maintain a strong security posture for their Kubernetes clusters.
      remediation: |-
        Find the kube-apiserver process and check the `insecure-port` argument. If the argument is set to `0`, then the kube-apiserver is not listening on an insecure HTTP port:

        ```bash
        ps aux | grep kube-apiserver
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security
        title: Controlling Access to the Kubernetes API - Transport security
  - uid: mondoo-kubernetes-security-api-server-no-anonymous-auth
    title: Ensure the kube-apiserver does not allow anonymous authentication
    impact: 100
    mql: |
      processes.where(executable == /kube-apiserver/).list {
        flags["anonymous-auth"] == "false"
      }
    docs:
      desc: |
        This check ensures that the kube-apiserver does not allow anonymous authentication. Disabling anonymous authentication enforces proper access control and prevents unauthenticated users from accessing the Kubernetes API server.

        **Why this matters**

        The Kubernetes API server is the central management point for the cluster. If anonymous authentication is enabled:

        - Unauthenticated users may gain access to sensitive cluster resources and information.
        - Attackers could exploit this access to perform unauthorized actions or escalate privileges.
        - Compliance with security standards and best practices may be compromised.

        By ensuring that anonymous authentication is disabled, organizations can maintain strong access controls and protect the security and integrity of their Kubernetes clusters.
      remediation: |-
        Find the kube-apiserver process and check the `--anonymous-auth` argument. If the argument is set to `false`, then the kube-apiserver does not allow anonymous authentication:

        ```bash
        ps aux | grep kube-apiserver
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
        title: Anonymous requests
  - uid: mondoo-kubernetes-security-pod-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: |
      k8s.pod {
        podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
      }
    docs:
      desc: |
        This check ensures that containers do not mount the Docker socket (`/var/run/docker.sock`). Mounting the Docker socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the Docker socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the Docker socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-cronjob-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: |
      k8s.cronjob {
        podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
      }
    docs:
      desc: |
        This check ensures that containers do not mount the Docker socket (`/var/run/docker.sock`). Mounting the Docker socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the Docker socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the Docker socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-statefulset-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.statefulset.podSpec['volumes'] == null || k8s.statefulset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the Docker socket (`/var/run/docker.sock`). Mounting the Docker socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the Docker socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the Docker socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-deployment-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.deployment.podSpec['volumes'] == null || k8s.deployment.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the Docker socket (`/var/run/docker.sock`). Mounting the Docker socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the Docker socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the Docker socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-job-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.job.podSpec['volumes'] == null || k8s.job.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the Docker socket (`/var/run/docker.sock`). Mounting the Docker socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the Docker socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the Docker socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-replicaset-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.replicaset.podSpec['volumes'] == null || k8s.replicaset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the Docker socket (`/var/run/docker.sock`). Mounting the Docker socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the Docker socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the Docker socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-daemonset-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.daemonset.podSpec['volumes'] == null || k8s.daemonset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-pod-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.pod.podSpec['volumes'] == null || k8s.pod.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-cronjob-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.cronjob.podSpec['volumes'] == null || k8s.cronjob.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-statefulset-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.statefulset.podSpec['volumes'] == null || k8s.statefulset.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-deployment-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.deployment.podSpec['volumes'] == null || k8s.deployment.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers in Deployments do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-job-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.job.podSpec['volumes'] == null || k8s.job.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-replicaset-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.replicaset.podSpec['volumes'] == null || k8s.replicaset.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the containerd socket (`/run/containerd/containerd.sock`). Mounting the containerd socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the containerd socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the containerd socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-daemonset-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.daemonset.podSpec['volumes'] == null || k8s.daemonset.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-pod-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.pod.podSpec['volumes'] == null || k8s.pod.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-cronjob-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.cronjob.podSpec['volumes'] == null || k8s.cronjob.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-statefulset-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.statefulset.podSpec['volumes'] == null || k8s.statefulset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-deployment-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.deployment.podSpec['volumes'] == null || k8s.deployment.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-job-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.job.podSpec['volumes'] == null || k8s.job.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-replicaset-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.replicaset.podSpec['volumes'] == null || k8s.replicaset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-daemonset-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.daemonset.podSpec['volumes'] == null || k8s.daemonset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        This check ensures that containers do not mount the CRI-O socket (`/var/run/crio/crio.sock`). Mounting the CRI-O socket into a container provides direct access to the container runtime, bypassing Kubernetes security controls and authentication.

        **Why this matters**

        Allowing containers to access the CRI-O socket can lead to significant security risks:

        - Containers can create or control other containers on the host, potentially escalating privileges.
        - Attackers may gain access to the host file system or create containers that are not visible to the Kubernetes API.
        - Sensitive information and credentials may be exposed, and compliance with security best practices may be violated.

        By ensuring that containers do not mount the CRI-O socket, organizations can prevent privilege escalation, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-pod-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.pod.ephemeralContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.pod.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.pod.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.cronjob.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.cronjob.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.statefulset.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.statefulset.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.deployment.containers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.deployment.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.job.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.job.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.replicaset.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.replicaset.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.daemonset.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.daemonset.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        This check ensures that privilege escalation is not allowed in containers by setting `allowPrivilegeEscalation` to `false` in the security context. This configuration prevents processes within containers from gaining more privileges than their parent process, reducing the risk of privilege escalation attacks.

        **Why this matters**

        Allowing privilege escalation in containers can enable attackers or compromised processes to gain elevated privileges, potentially leading to container escapes or unauthorized access to the host system. If privilege escalation is permitted:

        - Containers may be able to bypass security boundaries and access sensitive resources.
        - Attackers could exploit vulnerabilities to gain root or administrative access on the host.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that privilege escalation is disallowed, organizations can strengthen container isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.pod.ephemeralContainers.all( securityContext['privileged'] != true )
      k8s.pod.initContainers.all( securityContext['privileged'] != true )
      k8s.pod.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.cronjob.initContainers.all( securityContext['privileged'] != true )
      k8s.cronjob.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.statefulset.initContainers.all( securityContext['privileged'] != true )
      k8s.statefulset.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.deployment.containers.all( securityContext['privileged'] != true )
      k8s.deployment.initContainers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.job.initContainers.all( securityContext['privileged'] != true )
      k8s.job.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.replicaset.initContainers.all( securityContext['privileged'] != true )
      k8s.replicaset.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.daemonset.initContainers.all( securityContext['privileged'] != true )
      k8s.daemonset.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        This check ensures that containers are not running as privileged containers. Privileged containers have the same capabilities as the host, including unrestricted access to all devices and the host's network, which can undermine Kubernetes security controls.

        **Why this matters**

        Running containers as privileged grants them elevated permissions that can be exploited to compromise the host or other workloads. If containers are allowed to run as privileged:

        - Attackers may gain access to sensitive host resources or escalate privileges outside the container.
        - The isolation between workloads is weakened, increasing the risk of container escapes.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers do not run as privileged, organizations can maintain strong workload isolation and reduce the risk of privilege escalation or host compromise.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.pod.ephemeralContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.pod.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.pod.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.cronjob.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.cronjob.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.statefulset.containers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.statefulset.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.deployment.containers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.deployment.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.job.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.job.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.replicaset.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.replicaset.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.daemonset.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.daemonset.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        This check ensures that containers are configured to use an immutable (read-only) root filesystem. This configuration prevents processes within the container from modifying the filesystem at runtime, reducing the risk of unauthorized changes or persistence by attackers.

        **Why this matters**

        Allowing containers to write to the root filesystem can introduce significant security risks:

        - Attackers or compromised processes may modify binaries or configuration files, enabling privilege escalation or persistence.
        - Malicious changes to the filesystem can go undetected, making incident response and recovery more difficult.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that containers use a read-only root filesystem, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-runasnonroot
    title: Container should not run as root
    impact: 100
    filters: k8s.pod.annotations['policies.k8s.mondoo.com/mondoo-kubernetes-security-pod-runasnonroot'] != 'ignore'
    mql: |
      k8s.pod.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.pod.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.pod.podSpec.securityContext.runAsNonRoot == true
      k8s.pod.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.pod.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.pod.podSpec.securityContext.runAsNonRoot == true
      k8s.pod.ephemeralContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.pod.ephemeralContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.pod.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-runasnonroot
    title: Container should not run as root
    impact: 100
    filters: k8s.cronjob.annotations['policies.k8s.mondoo.com/mondoo-kubernetes-security-cronjob-runasnonroot'] != 'ignore'
    mql: |
      k8s.cronjob.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.cronjob.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.cronjob.podSpec.securityContext.runAsNonRoot == true
      k8s.cronjob.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.cronjob.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.cronjob.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.statefulset.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.statefulset.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.statefulset.podSpec.securityContext.runAsNonRoot == true
      k8s.statefulset.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.statefulset.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.statefulset.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.deployment.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.deployment.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.deployment.podSpec.securityContext.runAsNonRoot == true
      k8s.deployment.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.deployment.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.deployment.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-runasnonroot
    title: Container should not run as root
    impact: 100
    filters: k8s.job.annotations['policies.k8s.mondoo.com/mondoo-kubernetes-security-job-runasnonroot'] != 'ignore'
    mql: |
      k8s.job.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.job.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.job.podSpec.securityContext.runAsNonRoot == true
      k8s.job.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.job.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.job.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.replicaset.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.replicaset.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.replicaset.podSpec.securityContext.runAsNonRoot == true
      k8s.replicaset.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.replicaset.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.replicaset.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.daemonset.containers.all(securityContext['runAsNonRoot'] == true)
        || k8s.daemonset.containers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.daemonset.podSpec.securityContext.runAsNonRoot == true
      k8s.daemonset.initContainers.all(securityContext['runAsNonRoot'] == true)
        || k8s.daemonset.initContainers.all(securityContext['runAsNonRoot'] == empty)
        && k8s.daemonset.podSpec.securityContext.runAsNonRoot == true
    docs:
      desc: |
        This check ensures that containers do not run as the root user by requiring `runAsNonRoot: true` in the security context. This configuration enforces that containers run with a non-root user, reducing the risk of privilege escalation and unauthorized access.

        **Why this matters**

        Running containers as the root user grants them the same privileges as root on the host system. If containers run as root:

        - Attackers or compromised processes may gain elevated privileges, increasing the risk of container escapes or host compromise.
        - Security boundaries between workloads are weakened, making it easier to access sensitive resources.
        - Compliance with security best practices and organizational policies may be violated.

        By ensuring that containers run as non-root, organizations can strengthen workload isolation, reduce the attack surface, and maintain a secure Kubernetes environment.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.pod.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-cronjob-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.cronjob.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-statefulset-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.statefulset.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods in StatefulSets are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-deployment-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.deployment.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods in Deployments are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-job-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.job.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods in Jobs are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-replicaset-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: |
      k8s.replicaset.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods in ReplicaSets are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-daemonset-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: |
      k8s.daemonset.podSpec['hostNetwork'] != true
    docs:
      desc: |
        This check ensures that pods in DaemonSets are not configured to use the `hostNetwork` namespace. Setting `hostNetwork: true` allows containers to share the host's network namespace, including access to loopback devices and network interfaces.

        **Why this matters**

        Allowing pods to use the host network can introduce significant security risks:

        - Containers may intercept or interfere with network traffic intended for other pods or the host.
        - Attackers could exploit this access to escalate privileges or bypass network policies.
        - The isolation between workloads is weakened, increasing the risk of lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host network, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-pod-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.pod.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-cronjob-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.cronjob.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-statefulset-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.statefulset.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods in StatefulSets are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-deployment-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.deployment.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods in Jobs are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-job-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.job.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods in Jobs are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-replicaset-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.replicaset.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods in DaemonSets are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-daemonset-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: |
      k8s.daemonset.podSpec['hostPID'] != true
    docs:
      desc: |
        This check ensures that pods in DaemonSets are not configured to use the `hostPID` namespace. Setting `hostPID: true` allows containers to share the host's process ID namespace, which can be used to escalate privileges outside a container.

        **Why this matters**

        Allowing pods to use the host PID namespace can introduce significant security risks:

        - Containers may view or interact with processes running on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host PID namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-pod-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.pod.podSpec['hostIPC'] != true
    docs:
      desc: |
        This check ensures that pods are not configured to use the `hostIPC` namespace. Setting `hostIPC: true` allows containers to share the host's IPC namespace, which can be used to break container isolation.

        **Why this matters**

        Allowing pods to use the host IPC namespace can introduce significant security risks:

        - Containers may access or interfere with IPC resources on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host IPC namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-cronjob-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.cronjob.podSpec['hostIPC'] != true
    docs:
      desc: |
        This check ensures that pods in CronJobs are not configured to use the `hostIPC` namespace. Setting `hostIPC: true` allows containers to share the host's IPC namespace, which can be used to break container isolation.

        **Why this matters**

        Allowing pods to use the host IPC namespace can introduce significant security risks:

        - Containers may access or interfere with IPC resources on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host IPC namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-statefulset-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.statefulset.podSpec['hostIPC'] != true
    docs:
      desc: |
        This check ensures that pods in StatefulSets are not configured to use the `hostIPC` namespace. Setting `hostIPC: true` allows containers to share the host's IPC namespace, which can be used to break container isolation.

        **Why this matters**

        Allowing pods to use the host IPC namespace can introduce significant security risks:

        - Containers may access or interfere with IPC resources on the host, increasing the risk of privilege escalation or container escapes.
        - Attackers could exploit this access to interfere with host processes or gain unauthorized access to sensitive information.
        - The isolation between workloads is weakened, making it easier for threats to move laterally within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not use the host IPC namespace, organizations can maintain strong process isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-deployment-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: k8s.deployment.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-job-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.job.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-replicaset-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.replicaset.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-daemonset-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.daemonset.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-pod-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.pod.podSpec['serviceAccount'] == null || k8s.pod.podSpec['serviceAccount'] == k8s.pod.podSpec['serviceAccountName']
      k8s.pod.podSpec['serviceAccountName'] != '' || k8s.pod.podSpec['automountServiceAccountToken'] == false
      k8s.pod.podSpec['serviceAccountName'] != 'default' || k8s.pod.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-cronjob-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.cronjob.podSpec['serviceAccount'] == null || k8s.cronjob.podSpec['serviceAccount'] == k8s.cronjob.podSpec['serviceAccountName']
      k8s.cronjob.podSpec['serviceAccountName'] != '' || k8s.cronjob.podSpec['automountServiceAccountToken'] == false
      k8s.cronjob.podSpec['serviceAccountName'] != 'default' || k8s.cronjob.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-statefulset-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.statefulset.podSpec['serviceAccount'] == null || k8s.statefulset.podSpec['serviceAccount'] == k8s.statefulset.podSpec['serviceAccountName']
      k8s.statefulset.podSpec['serviceAccountName'] != '' || k8s.statefulset.podSpec['automountServiceAccountToken'] == false
      k8s.statefulset.podSpec['serviceAccountName'] != 'default' || k8s.statefulset.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-deployment-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.deployment.podSpec['serviceAccount'] == null || k8s.deployment.podSpec['serviceAccount'] == k8s.deployment.podSpec['serviceAccountName']
      k8s.deployment.podSpec['serviceAccountName'] != '' || k8s.deployment.podSpec['automountServiceAccountToken'] == false
      k8s.deployment.podSpec['serviceAccountName'] != 'default' || k8s.deployment.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-job-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.job.podSpec['serviceAccount'] == null || k8s.job.podSpec['serviceAccount'] == k8s.job.podSpec['serviceAccountName']
      k8s.job.podSpec['serviceAccountName'] != '' || k8s.job.podSpec['automountServiceAccountToken'] == false
      k8s.job.podSpec['serviceAccountName'] != 'default' || k8s.job.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-replicaset-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.replicaset.podSpec['serviceAccount'] == null || k8s.replicaset.podSpec['serviceAccount'] == k8s.replicaset.podSpec['serviceAccountName']
      k8s.replicaset.podSpec['serviceAccountName'] != '' || k8s.replicaset.podSpec['automountServiceAccountToken'] == false
      k8s.replicaset.podSpec['serviceAccountName'] != 'default' || k8s.replicaset.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-daemonset-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.daemonset.podSpec['serviceAccount'] == null || k8s.daemonset.podSpec['serviceAccount'] == k8s.daemonset.podSpec['serviceAccountName']
      k8s.daemonset.podSpec['serviceAccountName'] != '' || k8s.daemonset.podSpec['automountServiceAccountToken'] == false
      k8s.daemonset.podSpec['serviceAccountName'] != 'default' || k8s.daemonset.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's `spec.serviceAccountName` to the name of the ServiceAccount created for the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          serviceAccountName: some-account <-- Set the name of the ServiceAccount created for the Pod
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod. For example:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example-app
        spec:
          automountServiceAccountToken: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
  - uid: mondoo-kubernetes-security-pod-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.pod.ephemeralContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.pod.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.pod.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-cronjob-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.cronjob.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.cronjob.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-statefulset-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.statefulset.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.statefulset.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-deployment-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.deployment.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.deployment.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        This check ensures that Deployments are configured to always pull a specific container image using a tag or digest, and that the `imagePullPolicy` is set to `Always`. This configuration guarantees that each time a pod is started, the same container image is used, ensuring consistency and predictability across workloads.

        **Why this matters**

        Consistent image pulling is important for several reasons:

        - Ensures that all pods run the intended version of the container, reducing the risk of unexpected behavior or incompatibility.
        - Prevents the use of rolling tags like `latest` or `master`, which can change over time and introduce untested or insecure code into production.
        - Supports traceability and reproducibility, which are essential for debugging, auditing, and compliance.
        - Reduces the risk of supply chain attacks by ensuring only approved images are used.

        By enforcing strict image pull policies and avoiding mutable tags, organizations can improve reliability, security, and maintainability of their Kubernetes workloads.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-job-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.job.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.job.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        This check ensures that Jobs are configured to always pull a specific container image using a tag or digest, and that the `imagePullPolicy` is set to `Always`. This configuration guarantees that each time a pod is started, the same container image is used, ensuring consistency and predictability across workloads.

        **Why this matters**

        Consistent image pulling is important for several reasons:

        - Ensures that all pods run the intended version of the container, reducing the risk of unexpected behavior or incompatibility.
        - Prevents the use of rolling tags like `latest` or `master`, which can change over time and introduce untested or insecure code into production.
        - Supports traceability and reproducibility, which are essential for debugging, auditing, and compliance.
        - Reduces the risk of supply chain attacks by ensuring only approved images are used.

        By enforcing strict image pull policies and avoiding mutable tags, organizations can improve reliability, security, and maintainability of their Kubernetes workloads.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-replicaset-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.replicaset.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.replicaset.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        This check ensures that ReplicaSets are configured to always pull a specific container image using a tag or digest, and that the `imagePullPolicy` is set to `Always`. This configuration guarantees that each time a pod is started, the same container image is used, ensuring consistency and predictability across deployments.

        **Why this matters**

        Consistent image pulling is important for several reasons:

        - Ensures that all pods run the intended version of the container, reducing the risk of unexpected behavior or incompatibility.
        - Prevents the use of rolling tags like `latest` or `master`, which can change over time and introduce untested or insecure code into production.
        - Supports traceability and reproducibility, which are essential for debugging, auditing, and compliance.
        - Reduces the risk of supply chain attacks by ensuring only approved images are used.

        By enforcing strict image pull policies and avoiding mutable tags, organizations can improve reliability, security, and maintainability of their Kubernetes workloads.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-daemonset-imagepull
    title: Container image pull should be consistent
    impact: 60
    props:
      - uid: mondooKubernetesSecurityExcludedByFixedImages
        title: Exclude containers from the check when using fixed images using hash values.
        mql: |
          # Add a list of container images in the format <image-name>@<digest>, such as: return ['image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2', 'image@sha256:12a23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5123']
          return ['']
    mql: |
      k8s.daemonset.containers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
      k8s.daemonset.initContainers
        .where(imageName.in(props.mondooKubernetesSecurityExcludedByFixedImages) != true)
        .all( imagePullPolicy == 'Always' && imageName != /:latest/ && imageName.contains(':') == true )
    docs:
      desc: |
        This check ensures that DaemonSets are configured to always pull a specific container image using a tag or digest, and that the `imagePullPolicy` is set to `Always`. This configuration guarantees that each time a pod is started, the same container image is used, ensuring consistency and predictability across deployments.

        **Why this matters**

        Consistent image pulling is important for several reasons:

        - Ensures that all pods run the intended version of the container, reducing the risk of unexpected behavior or incompatibility.
        - Prevents the use of rolling tags like `latest` or `master`, which can change over time and introduce untested or insecure code into production.
        - Supports traceability and reproducibility, which are essential for debugging, auditing, and compliance.
        - Reduces the risk of supply chain attacks by ensuring only approved images are used.

        By enforcing strict image pull policies and avoiding mutable tags, organizations can improve reliability, security, and maintainability of their Kubernetes workloads.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-pod-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.pod.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.pod.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that pods are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-cronjob-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.cronjob.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.cronjob.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that CronJobs are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that CronJobs define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-statefulset-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.statefulset.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.statefulset.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that StatefulSets are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that StatefulSets define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-deployment-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.deployment.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.deployment.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that Deployments are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Deployments define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-job-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.job.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.job.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that Jobs are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Jobs define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-replicaset-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.replicaset.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.replicaset.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that ReplicaSets are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that ReplicaSets define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-daemonset-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.daemonset.initContainers.all( resources['limits']['cpu'] != empty )
      k8s.daemonset.containers.all( resources['limits']['cpu'] != empty )
    docs:
      desc: |
        This check ensures that DaemonSets are configured with CPU limits for all containers. Setting CPU limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting CPU limits for containers can introduce significant operational and security risks:

        - Containers without CPU limits may consume all available CPU on the node, causing other workloads to be throttled or the node to become unresponsive.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that DaemonSets define CPU limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-pod-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.pod.initContainers.all( resources['limits']['memory'] != empty )
      k8s.pod.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        This check ensures that pods are configured with memory limits for all containers. Setting memory limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting memory limits for containers can introduce significant operational and security risks:

        - Containers without memory limits may consume all available memory on the node, causing other workloads to be evicted or the node to crash.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods define memory limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-cronjob-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.cronjob.initContainers.all( resources['limits']['memory'] != empty )
      k8s.cronjob.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-statefulset-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.statefulset.initContainers.all( resources['limits']['memory'] != empty )
      k8s.statefulset.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        This check ensures that StatefulSets are configured with memory limits for all containers. Setting memory limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting memory limits for containers can introduce significant operational and security risks:

        - Containers without memory limits may consume all available memory on the node, causing other workloads to be evicted or the node to crash.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that StatefulSets define memory limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-deployment-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.deployment.initContainers.all( resources['limits']['memory'] != empty )
      k8s.deployment.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        This check ensures that Deployments are configured with memory limits for all containers. Setting memory limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting memory limits for containers can introduce significant operational and security risks:

        - Containers without memory limits may consume all available memory on the node, causing other workloads to be evicted or the node to crash.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Deployments define memory limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-job-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.job.initContainers.all( resources['limits']['memory'] != empty )
      k8s.job.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        This check ensures that Jobs are configured with memory limits for all containers. Setting memory limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting memory limits for containers can introduce significant operational and security risks:

        - Containers without memory limits may consume all available memory on the node, causing other workloads to be evicted or the node to crash.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Jobs define memory limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-replicaset-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.replicaset.initContainers.all( resources['limits']['memory'] != empty )
      k8s.replicaset.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        This check ensures that ReplicaSets are configured with memory limits for all containers. Setting memory limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting memory limits for containers can introduce significant operational and security risks:

        - Containers without memory limits may consume all available memory on the node, causing other workloads to be evicted or the node to crash.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that ReplicaSets define memory limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-daemonset-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.daemonset.initContainers.all( resources['limits']['memory'] != empty )
      k8s.daemonset.containers.all( resources['limits']['memory'] != empty )
    docs:
      desc: |
        This check ensures that DaemonSets are configured with memory limits for all containers. Setting memory limits prevents containers from consuming excessive host resources, which could lead to node instability or denial of service.

        **Why this matters**

        Not setting memory limits for containers can introduce significant operational and security risks:

        - Containers without memory limits may consume all available memory on the node, causing other workloads to be evicted or the node to crash.
        - Attackers or malfunctioning applications could exploit the lack of limits to perform denial-of-service attacks.
        - Predictable resource usage is essential for reliable scheduling and cluster stability.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that DaemonSets define memory limits for all containers, organizations can maintain cluster stability, prevent resource exhaustion, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-pod-capability-net-raw
    title: Pods should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.pod.podSpec['ephemeralContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['ephemeralContainers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['initContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['initContainers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that pods are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in pods to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Pods have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a Pod that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these Pods with:

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | select( .spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Pods that explicitly add the NET_RAW or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: examplePod
          namespace: example-namespace
        spec:
          containers:
            - securityContext:
                capabilities:
                  add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any Pods that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - securityContext:
                capabilities:
                  drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-daemonset-capability-net-raw
    title: DaemonSets should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.daemonset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.daemonset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that DaemonSets are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in DaemonSets to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that DaemonSets do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no DaemonSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a DaemonSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```bash
        kubectl get daemonsets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any DaemonSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any DaemonSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-replicaset-capability-net-raw
    title: ReplicaSets should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.replicaset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.replicaset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that ReplicaSets are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in ReplicaSets to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that ReplicaSets do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no ReplicaSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a ReplicaSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```bash
        kubectl get replicasets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any ReplicaSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any ReplicaSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-job-capability-net-raw
    title: Jobs should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that Jobs are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in Jobs to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Jobs do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Jobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a Job that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```bash
        kubectl get jobs -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Jobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any Jobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-deployment-capability-net-raw
    title: Deployments should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'].none( _['add'].contains("NET_RAW") ))
      k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'].none( _['add'].contains("ALL") ))
    docs:
      desc: |
        This check ensures that Deployments are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in Deployments to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Deployments do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Deployments have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a Deployment that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```bash
        kubectl get deployments -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Deployments that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any Deployments that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-statefulset-capability-net-raw
    title: StatefulSets should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.statefulset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.statefulset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that StatefulSets are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in StatefulSets to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that StatefulSets do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no StatefulSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a StatefulSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```bash
        kubectl get statefulsets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any StatefulSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any StatefulSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-cronjob-capability-net-raw
    title: CronJobs should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.cronjob.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.cronjob.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that CronJobs are not running with the NET_RAW capability. The NET_RAW capability allows a process to write raw packets to the network interface, which can be used to craft malicious ARP or DNS responses and bypass network security controls.

        **Why this matters**

        Allowing containers in CronJobs to run with the NET_RAW capability can introduce significant security risks:

        - Containers may be able to craft or intercept network packets, enabling network attacks such as spoofing or man-in-the-middle.
        - Attackers could exploit this capability to escalate privileges or disrupt network communications within the cluster.
        - The isolation between workloads is weakened, increasing the risk of lateral movement or compromise of other resources.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that CronJobs do not run with the NET_RAW capability, organizations can reduce the attack surface, maintain strong network isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no CronJobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```bash
        kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```


        Additionally, a CronJob that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```bash
        kubectl get cronjobs -A -o json | jq -r '.items[] | select( .spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any CronJobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - securityContext:
                        capabilities:
                          add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any CronJobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - securityContext:
                        capabilities:
                          drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-pod-capability-sys-admin
    title: Pods should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.pod.podSpec['initContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
      k8s.pod.podSpec['ephemeralContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
      k8s.pod.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that pods are not running with the SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls and can grant containers powerful privileges, even if they are not running as root.

        **Why this matters**

        Allowing containers in pods to run with the SYS_ADMIN capability can introduce significant security risks:

        - Containers may perform privileged operations that could compromise the host or other workloads.
        - Attackers could exploit this capability to escalate privileges or bypass security controls.
        - The isolation between workloads is weakened, increasing the risk of container escapes or lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not run with the SYS_ADMIN capability, organizations can reduce the attack surface, maintain workload isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Pods have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Pods that explicitly add the SYS_ADMIN or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: examplePod
          namespace: example-namespace
        spec:
          containers:
            - securityContext:
                capabilities:
                  add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-daemonset-capability-sys-admin
    title: DaemonSets should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.daemonset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that DaemonSets are not running with the SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls and can grant containers powerful privileges, even if they are not running as root.

        **Why this matters**

        Allowing containers in DaemonSets to run with the SYS_ADMIN capability can introduce significant security risks:

        - Containers may perform privileged operations that could compromise the host or other workloads.
        - Attackers could exploit this capability to escalate privileges or bypass security controls.
        - The isolation between workloads is weakened, increasing the risk of container escapes or lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that DaemonSets do not run with the SYS_ADMIN capability, organizations can reduce the attack surface, maintain workload isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no DaemonSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any DaemonSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-replicaset-capability-sys-admin
    title: ReplicaSets should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.replicaset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        ReplicaSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no ReplicaSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any ReplicaSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-job-capability-sys-admin
    title: Jobs should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.job.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that Jobs are not running with the SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls and can grant containers powerful privileges, even if they are not running as root.

        **Why this matters**

        Allowing containers in Jobs to run with the SYS_ADMIN capability can introduce significant security risks:

        - Containers may perform privileged operations that could compromise the host or other workloads.
        - Attackers could exploit this capability to escalate privileges or bypass security controls.
        - The isolation between workloads is weakened, increasing the risk of container escapes or lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Jobs do not run with the SYS_ADMIN capability, organizations can reduce the attack surface, maintain workload isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Jobs have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Jobs that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-deployment-capability-sys-admin
    title: Deployments should not run with SYS_ADMIN capability
    impact: 80
    mql: k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that Deployments are not running with the SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls and can grant containers powerful privileges, even if they are not running as root.

        **Why this matters**

        Allowing containers in Deployments to run with the SYS_ADMIN capability can introduce significant security risks:

        - Containers may perform privileged operations that could compromise the host or other workloads.
        - Attackers could exploit this capability to escalate privileges or bypass security controls.
        - The isolation between workloads is weakened, increasing the risk of container escapes or lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Deployments do not run with the SYS_ADMIN capability, organizations can reduce the attack surface, maintain workload isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Deployments have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Deployments that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-statefulset-capability-sys-admin
    title: StatefulSets should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.statefulset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that StatefulSets are not running with the SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls and can grant containers powerful privileges, even if they are not running as root.

        **Why this matters**

        Allowing containers in StatefulSets to run with the SYS_ADMIN capability can introduce significant security risks:

        - Containers may perform privileged operations that could compromise the host or other workloads.
        - Attackers could exploit this capability to escalate privileges or bypass security controls.
        - The isolation between workloads is weakened, increasing the risk of container escapes or lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that StatefulSets do not run with the SYS_ADMIN capability, organizations can reduce the attack surface, maintain workload isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no StatefulSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any StatefulSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-cronjob-capability-sys-admin
    title: CronJobs should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.cronjob.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        This check ensures that CronJobs are not running with the SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls and can grant containers powerful privileges, even if they are not running as root.

        **Why this matters**

        Allowing containers in CronJobs to run with the SYS_ADMIN capability can introduce significant security risks:

        - Containers may perform privileged operations that could compromise the host or other workloads.
        - Attackers could exploit this capability to escalate privileges or bypass security controls.
        - The isolation between workloads is weakened, increasing the risk of container escapes or lateral movement within the cluster.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that CronJobs do not run with the SYS_ADMIN capability, organizations can reduce the attack surface, maintain workload isolation, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no CronJobs have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```bash
        kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != empty and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any CronJobs that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - securityContext:
                        capabilities:
                          add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-pod-ports-hostport
    title: Pods should not bind to a host port
    impact: 80
    mql: |
      k8s.pod.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
      k8s.pod.podSpec['initContainers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that pods do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in pods to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Pods are binding any of their containers to a host port:

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | select( (.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any Pods that bind to a host port, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not bind to a host port:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - ports:
              - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                name: http
                protocol: TCP
              - containerPort: 443
                name: https
                protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-daemonset-ports-hostport
    title: DaemonSets should not bind to a host port
    impact: 80
    mql: |
      k8s.daemonset.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that DaemonSets do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in DaemonSets to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that DaemonSets do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no DaemonSets are binding any of their containers to a host port:

        ```bash
        kubectl get daemonsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any DaemonSets that bind to a host port, update the DaemonSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-replicaset-ports-hostport
    title: ReplicaSets should not bind to a host port
    impact: 80
    mql: |
      k8s.replicaset.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that ReplicaSets do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in ReplicaSets to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that ReplicaSets do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no ReplicaSets are binding any of their containers to a host port:

        ```bash
        kubectl get replicasets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any ReplicaSets that bind to a host port, update the ReplicaSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-job-ports-hostport
    title: Jobs should not bind to a host port
    impact: 80
    mql: |
      k8s.job.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that Jobs do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in Jobs to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Jobs do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Jobs are binding any of their containers to a host port:

        ```bash
        kubectl get jobs -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any ReplicaSets that bind to a host port, update the Jobs to ensure they do not bind to a host port:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-deployment-ports-hostport
    title: Deployments should not bind to a host port
    impact: 80
    mql: |
      k8s.deployment.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that Deployments do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in Deployments to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that Deployments do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no Deployments are binding any of their containers to a host port:

        ```bash
        kubectl get deployments -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any Deployments that bind to a host port, update the Deployments to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-statefulset-ports-hostport
    title: StatefulSets should not bind to a host port
    impact: 80
    mql: |
      k8s.statefulset.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that StatefulSets do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in StatefulSets to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that StatefulSets do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no StatefulSets are binding any of their containers to a host port:

        ```bash
        kubectl get statefulsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any StatefulSets that bind to a host port, update the StatefulSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-cronjob-ports-hostport
    title: CronJobs should not bind to a host port
    impact: 80
    mql: |
      k8s.cronjob.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        This check ensures that CronJobs do not bind any of their containers to a host port. Binding to host ports allows containers to bypass Kubernetes network policies and access control systems, potentially exposing the container directly to the host network.

        **Why this matters**

        Allowing containers in CronJobs to bind to host ports can introduce significant security risks:

        - Containers may bypass network segmentation and access controls, increasing the risk of lateral movement or unauthorized access.
        - Host ports expose container services directly on the host, which may not be intended and can be exploited by attackers.
        - Misconfigured host port bindings can lead to port conflicts, service disruptions, or accidental exposure of sensitive services.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that CronJobs do not bind to host ports, organizations can maintain strong network isolation, reduce the attack surface, and improve the overall security posture of their Kubernetes environment.
      audit: |
        Check to ensure no CronJobs are binding any of their containers to a host port:

        ```bash
        kubectl get cronjobs -A -o json | jq -r '.items[] | select( (.spec.jobTemplate.spec.template.spec.containers[].ports | . != empty and any(.[].hostPort; . != empty)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq
        ```
      remediation: |
        For any CronJobs that bind to a host port, update the CronJobs to ensure they do not bind to a host port:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - ports:
                      - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                        name: http
                        protocol: TCP
                      - containerPort: 443
                        name: https
                        protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-pod-hostpath-readonly
    title: Pods should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.pod.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
      k8s.pod.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['initContainers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
      k8s.pod.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['ephemeralContainers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that pods do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in pods to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a Pod are mounting hostPath volumes as read-write:

        ```bash
        kubectl get pods -A -o json | jq -r '.items[] | [.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Pod containers that mount a hostPath volume as read-write, update them (or the Deployment/StatefulSet/etc that created the Pod):

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - volumeMounts:
              - mountPath: /host
                name: hostpath-volume
                readOnly: true # <-- ensure readOnly is set to true
          volumes:
            - hostPath:
                path: /etc
              name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-daemonset-hostpath-readonly
    title: DaemonSets should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.daemonset.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that DaemonSets do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in DaemonSets to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a DaemonSet are mounting hostPath volumes as read-write:

        ```bash
        kubectl get daemonsets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any DaemonSet containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-replicaset-hostpath-readonly
    title: ReplicaSets should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.replicaset.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that ReplicaSets do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in ReplicaSets to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a ReplicaSet are mounting hostPath volumes as read-write:

        ```bash
        kubectl get replicasets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any ReplicaSet containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-job-hostpath-readonly
    title: Jobs should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.job.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that Jobs do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in Jobs to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a Job are mounting hostPath volumes as read-write:

        ```bash
        kubectl get jobs -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Job containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-deployment-hostpath-readonly
    title: Deployments should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.deployment.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that Deployments do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in Deployments to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a Deployment are mounting hostPath volumes as read-write:

        ```bash
        kubectl get deployments -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any Deployment containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-statefulset-hostpath-readonly
    title: StatefulSets should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.statefulset.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that StatefulSets do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in StatefulSets to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a StatefulSet are mounting hostPath volumes as read-write:

        ```bash
        kubectl get statefulsets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any StatefulSet containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-cronjob-hostpath-readonly
    title: CronJobs should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.cronjob.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != empty).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != empty ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        This check ensures that CronJobs do not mount hostPath volumes as read-write. Mounting hostPath volumes as read-write allows containers to modify the underlying host filesystem, which can break container isolation and introduce significant security risks.

        **Why this matters**

        Allowing containers in CronJobs to write to hostPath volumes can lead to:

        - Containers mutating or damaging the host system, potentially leading to container escapes.
        - Attackers gaining the ability to persist changes or escalate privileges on the host.
        - Increased risk of lateral movement within the cluster or compromise of other workloads.
        - Violations of security best practices and organizational policies.

        By ensuring that hostPath volumes are mounted as read-only, organizations can reduce the attack surface, maintain workload isolation, and protect the integrity of the Kubernetes environment.
      audit: |
        Check to ensure no containers in a CronJob are mounting hostPath volumes as read-write:

        ```bash
        kubectl get cronjobs -A -o json | jq -r '.items[] | [.spec.jobTemplate.spec.template.spec.volumes[] | select(.hostPath != empty) | .name] as $myVar | select(.spec.jobTemplate.spec.template.spec.containers[].volumeMounts | (. != empty and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq
        ```
      remediation: |
        For any CronJob containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-deployment-tiller
    title: Deployments should not run Tiller (Helm v2)
    impact: 40
    mql: |
      k8s.deployment.podSpec["containers"].none( _["image"].contains("tiller") )
    docs:
      desc: |
        This check ensures that deployments are not running Tiller (the in-cluster component for the Helm v2 package manager). Tiller communicates directly with the Kubernetes API and has broad RBAC permissions, which can expose the cluster to significant security risks if not properly controlled.

        **Why this matters**

        Running Tiller can introduce several security risks:

        - Tiller has cluster-wide access and can perform administrative actions, increasing the risk of privilege escalation.
        - If Tiller is exposed or misconfigured, attackers could exploit it to gain unauthorized access or control over the cluster.
        - Historical incidents have shown that exposed Tiller instances can lead to cluster compromise and abuse.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that deployments do not run Tiller, organizations can reduce the attack surface, protect sensitive cluster resources, and maintain a secure Kubernetes environment.
      audit: |
        Verify there are no deployments running Tiller:

        ```bash
        kubectl get deployments -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"
        ```
      remediation: |
        Delete any deployments that are running Tiller.
  - uid: mondoo-kubernetes-security-pod-tiller
    title: Pods should not run Tiller (Helm v2)
    impact: 40
    mql: |
      k8s.pod.podSpec["containers"].none( _["image"].contains("tiller") )
      k8s.pod.podSpec["initContainers"].none( _["image"].contains("tiller") )
      k8s.pod.podSpec["ephemeralContainers"].none( _["image"].contains("tiller") )
    docs:
      desc: |
        This check ensures that pods are not running Tiller (the in-cluster component for the Helm v2 package manager). Tiller communicates directly with the Kubernetes API and has broad RBAC permissions, which can expose the cluster to significant security risks if not properly controlled.

        **Why this matters**

        Running Tiller can introduce several security risks:

        - Tiller has cluster-wide access and can perform administrative actions, increasing the risk of privilege escalation.
        - If Tiller is exposed or misconfigured, attackers could exploit it to gain unauthorized access or control over the cluster.
        - Historical incidents have shown that exposed Tiller instances can lead to cluster compromise and abuse.
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not run Tiller, organizations can reduce the attack surface, protect sensitive cluster resources, and maintain a secure Kubernetes environment.
      audit: |
        Verify there are no pods running Tiller:

        ```bash
        kubectl get pods -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"
        ```
      remediation: |
        Delete any pods that are running Tiller.
  - uid: mondoo-kubernetes-security-deployment-k8s-dashboard
    title: Pods should not run Kubernetes dashboard
    impact: 40
    mql: |
      k8s.deployment.podSpec["containers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.deployment.labels["app"] == null || k8s.deployment.labels["app"] != "kubernetes-dashboard"
      k8s.deployment.labels["k8s-app"] == null || k8s.deployment.labels["k8s-app"] != "kubernetes-dashboard"
    docs:
      desc: |
        This check ensures that deployments are not running the Kubernetes dashboard. The Kubernetes dashboard provides a web-based UI for managing cluster resources, which can expose sensitive information and administrative capabilities if not properly secured.

        **Why this matters**

        Running the Kubernetes dashboard can introduce significant security risks:

        - The dashboard may expose sensitive cluster resources, such as workloads, configmaps, and secrets, to unauthorized users.
        - If the dashboard is publicly accessible or misconfigured, attackers could exploit it to extract credentials or escalate privileges.
        - Historical incidents, such as the 2019 Tesla breach, have demonstrated that exposed dashboards can lead to cluster compromise and abuse (e.g., unauthorized deployments or cryptocurrency mining).
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that deployments do not run the Kubernetes dashboard, organizations can reduce the attack surface, protect sensitive cluster data, and maintain a secure Kubernetes environment.
      audit: |
        Verify there are no deployments running Kubernetes dashboard:

        ```bash
        kubectl get deployments -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"
        ```
      remediation: |
        Delete any deployments that are running Kubernetes dashboard.
  - uid: mondoo-kubernetes-security-pod-k8s-dashboard
    title: Pods should not run Kubernetes dashboard
    impact: 40
    mql: |
      k8s.pod.podSpec["containers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.pod.podSpec["initContainers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.pod.podSpec["ephemeralContainers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.pod.labels["app"] == null || k8s.pod.labels["app"] != "kubernetes-dashboard"
      k8s.pod.labels["k8s-app"] == null || k8s.pod.labels["k8s-app"] != "kubernetes-dashboard"
    docs:
      desc: |
        This check ensures that pods are not running the Kubernetes dashboard. The Kubernetes dashboard provides a web-based UI for managing cluster resources, which can expose sensitive information and administrative capabilities if not properly secured.

        **Why this matters**

        Running the Kubernetes dashboard can introduce significant security risks:

        - The dashboard may expose sensitive cluster resources, such as workloads, configmaps, and secrets, to unauthorized users.
        - If the dashboard is publicly accessible or misconfigured, attackers could exploit it to extract credentials or escalate privileges.
        - Historical incidents, such as the 2019 Tesla breach, have demonstrated that exposed dashboards can lead to cluster compromise and abuse (e.g., unauthorized deployments or cryptocurrency mining).
        - Compliance with security best practices and organizational policies may be compromised.

        By ensuring that pods do not run the Kubernetes dashboard, organizations can reduce the attack surface, protect sensitive cluster data, and maintain a secure Kubernetes environment.
      audit: |
        Verify there are no pods running Kubernetes dashboard:

        ```bash
        kubectl get pods -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"
        ```
      remediation: |
        Delete any pods that are running Kubernetes dashboard.
