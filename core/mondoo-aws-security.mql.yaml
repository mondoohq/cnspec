# Copyright (c) Mondoo, Inc.
# SPDX-License-Identifier: BUSL-1.1

policies:
  - uid: mondoo-aws-security
    name: Mondoo AWS Security
    version: 4.0.0
    license: BUSL-1.1
    tags:
      mondoo.com/category: security
      mondoo.com/platform: aws,cloud
    authors:
      - name: Mondoo, Inc
        email: hello@mondoo.com
    docs:
      desc: |
        The Mondoo AWS Security policy provides guidance for establishing minimum recommended security and operational best practices for Amazon Web Services (AWS). The checks in this policy bundle are based on AWS's Operational Best Practices recommendations as part of the [AWS Config conformance packs](https://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html).

        ## Remote scan

        Remote scans use cnspec providers to retrieve on-demand scan results without having to install any agents.

        For a complete list of providers, run:

        ```bash
        cnspec scan --help
        ```

        ### Prerequisites

        Remote scanning of AWS accounts with cnspec relies on the access key ID and secret access key configured for the AWS CLI. To learn how to configure these keys in the AWS CLI, read [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).

        ### Scan an AWS account

        This command scans all enabled regions in an AWS account:

        ```bash
        cnspec scan aws
        ```

        ### Scan a single AWS region

        To specify a single region to scan with cnspec, use the `--region` flag with the AWS region code:

        ```bash
        cnspec scan aws --region us-west-2
        ```

        For a complete list of AWS region codes, read [Regions and Zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html).

        ### Scan an AWS account using a specific profile

        If multiple AWS profiles are configured for the AWS CLI, you can target a specific profile by setting the `AWS_PROFILE` environment variable or the `--profile` command line flag.

        ```bash
        export AWS_PROFILE=my-profile
        cnspec scan aws
        ```

        ```bash
        cnspec scan aws --profile my-profile
        ```

        ## Join the community!

        Our goal is to build policies that are simple to deploy, accurate, and actionable.

        If you have any suggestions for how to improve this policy or need support, [join the community](https://github.com/orgs/mondoohq/discussions) in GitHub Discussions.
    groups:
      - title: AWS IAM
        checks:
          - uid: mondoo-aws-security-access-keys-rotated
          - uid: mondoo-aws-security-mfa-enabled-for-iam-console-access
          - uid: mondoo-aws-security-root-account-mfa-enabled
          - uid: mondoo-aws-security-iam-password-policy
          - uid: mondoo-aws-security-iam-root-access-key-check
          - uid: mondoo-aws-security-iam-users-only-one-access-key
          - uid: mondoo-aws-security-iam-group-has-users-check
          - uid: mondoo-aws-security-iam-user-no-inline-policies-check
      - title: AWS Lambda Function
        checks:
          - uid: mondoo-aws-security-lambda-concurrency-check
      - title: AWS S3 Bucket
        checks:
          - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited
          - uid: mondoo-aws-security-s3-buckets-account-level-block-public-access
      - title: AWS Security Group
        checks:
          - uid: mondoo-aws-security-secgroup-restricted-ssh
          - uid: mondoo-aws-security-secgroup-restricted-vnc
          - uid: mondoo-aws-security-secgroup-restricted-rdp
          - uid: mondoo-aws-security-secgroup-restrict-traffic
      - title: AWS VPC
        checks:
          - uid: mondoo-aws-security-vpc-default-security-group-closed
          - uid: mondoo-aws-security-vpc-flow-logs-enabled
      - title: AWS DynamoDB Table
        checks:
          - uid: mondoo-aws-security-dynamodb-table-encrypted-kms
      - title: AWS RDS DB Instance
        checks:
          - uid: mondoo-aws-security-rds-instance-public-access-check
      - title: AWS Redshift Cluster
        checks:
          - uid: mondoo-aws-security-redshift-cluster-public-access-check
      - title: AWS EC2
        checks:
          - uid: mondoo-aws-security-ec2-ebs-encryption-by-default
          - uid: mondoo-aws-security-ec2-imdsv2-check
          - uid: mondoo-aws-security-ec2-instance-no-public-ip
          - uid: mondoo-aws-security-ec2-volume-inuse-check
          - uid: mondoo-aws-security-ebs-snapshot-public-restorable-check
      - title: AWS EFS Filesystem
        checks:
          - uid: mondoo-aws-security-efs-encrypted-check
      - title: AWS CloudWatch LogGroup
        checks:
          - uid: mondoo-aws-security-cloudwatch-log-group-encrypted
      - title: AWS ELB LoadBalancer
        checks:
          - uid: mondoo-aws-security-elb-deletion-protection-enabled
      - title: AWS ES Domain
        checks:
          - uid: mondoo-aws-security-elasticsearch-encrypted-at-rest
      - title: AWS KMS Key
        checks:
          - uid: mondoo-aws-security-rotation-customer-created-cmks-enabled
      - title: AWS SageMaker NotebookInstance
        checks:
          - uid: mondoo-aws-security-sagemaker-notebook-instance-kms-key-configured
      - title: AWS CloudTrail Trail
        checks:
          - uid: mondoo-aws-security-cloud-trail-encryption-enabled
    scoring_system: highest impact
queries:
  - uid: mondoo-aws-security-iam-root-access-key-check
    title: Ensure no root user account access key exists
    filters: asset.platform == "aws"
    impact: 85
    mql: |
      aws.iam.credentialReport.where(properties.user == "<root_account>").all(accessKey1Active == false)
      aws.iam.credentialReport.where(properties.user == "<root_account>").all(accessKey2Active == false)
    docs:
      desc: |
        This check ensures that the AWS root user account does not have an active access key. The AWS root user has full administrative privileges over the AWS account, making it the most powerful identity in AWS. AWS best practices recommend that root user credentials should never be used for everyday operations, and instead, AWS Identity and Access Management (IAM) users or roles should be utilized for all administrative tasks.

        **Rationale:**

        Root user access keys pose a high security risk if compromised, as they provide unrestricted access to all AWS resources. If an attacker gains access to a root user’s credentials, they can:

        - Delete or modify critical resources.
        - Access sensitive data across all AWS services.
        - Disable security controls, logging, and monitoring.

        By ensuring that no root user access keys exist, organizations can enforce security best practices and reduce the risk of unauthorized access.

        **Risk Mitigation:**

        - **Prevents root key compromise:** Eliminates a major attack vector for AWS account takeovers.
        - **Ensures compliance:** Aligns with security frameworks like CIS AWS Foundations Benchmark, SOC 2, PCI DSS, and ISO 27001.
        - **Encourages least privilege:** Promotes the use of IAM users and roles for secure access control.
      audit: |
        __cnspec shell__

        1. Open a terminal
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

          ```mql
          aws.iam.credentialReport.where( properties["user"] == "<root_account>") { accessKey1Active accessKey2Active }
          ```

          Example output:

          ```mql
          aws.iam.credentialReport.where: [
            0: {
              accessKey1Active: false
              accessKey2Active: false
            }
          ]
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Users → Root User.
            3.	Under Access Keys, verify that no access keys are listed.
            4.	If an access key exists, delete it immediately:
                - Select the Access Key ID.
                - Select Delete and confirm the action.
            5.	Ensure that MFA is enabled for the root user for additional security.
        - id: cli
          desc: |
            Using AWS CLI:

            Check if the root user has an access key:
            ```sh
            aws iam get-account-summary --query "SummaryMap.RootAccessKeysPresent"
            ```

            If the output is greater than 0, a root access key exists.

            Delete the root access key (replace ACCESS_KEY_ID with the actual key ID):
            ```sh
            aws iam delete-access-key --access-key-id ACCESS_KEY_ID --user-name root
            ```
        - id: terraform
          desc: |
            **Using Terraform**

            AWS does not allow direct management of the root user’s access key via Terraform. However, you should enforce IAM policies that prevent the creation of root access keys:
            ```hcl
            resource "aws_iam_account_password_policy" "secure_policy" {
              minimum_password_length        = 14
              require_uppercase_characters   = true
              require_lowercase_characters   = true
              require_numbers                = true
              require_symbols                = true
              allow_users_to_change_password = true
            }

            resource "aws_iam_policy" "deny_root_access_keys" {
              name        = "DenyRootAccessKeys"
              description = "Prevents creation of root user access keys"

              policy = jsonencode({
                Version = "2012-10-17"
                Statement = [
                  {
                    Effect   = "Deny"
                    Action   = "iam:CreateAccessKey"
                    Resource = "arn:aws:iam::123456789012:root"
                  }
                ]
              })
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html
        title: AWS Documentation - AWS account root user
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user_manage_delete-key.html
        title: AWS Documentation - Delete access keys for the root user
  - uid: mondoo-aws-security-root-account-mfa-enabled
    title: Ensure MFA is enabled for the "root user" account
    filters: asset.platform == "aws"
    impact: 95
    mql: aws.iam.credentialReport.where(properties.user == "<root_account>").all(mfaActive == true)
    docs:
      desc: |
        This check ensures that Multi-Factor Authentication (MFA) is enabled for the AWS root user account. The root user has full administrative privileges over an AWS account, making it the most critical identity. Enabling MFA significantly enhances security by requiring an additional authentication factor beyond just the password.

        **Rationale:**

        The root user account is the most powerful account in AWS, and if compromised, an attacker can gain full control over all AWS resources. Without MFA, an attacker who obtains the root password (via phishing, credential leaks, or brute-force attacks) can:

        -	Delete or modify critical resources.
        -	Disable security controls, logging, and monitoring.
        -	Access and exfiltrate sensitive data.

        By enabling Multi-Factor Authentication (MFA), organizations prevent unauthorized access, even if root credentials are compromised. AWS supports hardware MFA tokens and virtual MFA devices (such as Authy or Google Authenticator) for added security.

        **Risk Mitigation:**

        -	**Prevents account takeovers:** Even if the root password is stolen, MFA blocks unauthorized logins.
        -	**Ensures compliance:** Meets security standards such as CIS AWS Foundations Benchmark, SOC 2, ISO 27001, and PCI DSS.
        -	**Strengthens account security:** Adds an extra layer of authentication beyond the password.
      audit: |
        __cnspec shell__

        1. Open a terminal
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

           ```mql
           aws.iam.credentialReport.where( properties["user"] == "<root_account>") { mfaActive passwordLastChanged passwordLastUsed }
           ```

          Example output:

          ```mql
          aws.iam.credentialReport.where: [
            0: {
              mfaActive: true
            }
          ]
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Users → Root User.
            3.	Under Multi-Factor Authentication (MFA), check if an MFA device is assigned.
            4.	If no MFA is enabled, select Enable MFA and choose one of the following options:
              -	Virtual MFA device: Use a mobile authenticator app (e.g., Google Authenticator, Authy).
              -	Hardware MFA device: Use a FIDO2 security key or an AWS-supported MFA token.
            5.	Follow the on-screen steps to scan the QR code (for virtual MFA) or register a hardware MFA device.
            6.	Enter the two consecutive MFA codes generated by the device and select Enable MFA.

        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if MFA is enabled for the root user:
            ```sh
            aws iam get-account-summary --query "SummaryMap.AccountMFAEnabled"
            ```

            If the output is 0, MFA is not enabled.

            To enable MFA, first list existing MFA devices:
            ```sh
            aws iam list-mfa-devices --user-name root
            ```

            If no device is listed, assign a new MFA device (replace arn-of-mfa-device with the correct ARN):
            ```sh
            aws iam enable-mfa-device --user-name root --serial-number arn-of-mfa-device --authentication-code-1 123456 --authentication-code-2 654321
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            AWS does not allow managing root MFA directly through Terraform. However, you can enforce a security policy that blocks root logins unless MFA is enabled:
            ```hcl
            resource "aws_iam_policy" "require_root_mfa" {
              name        = "RequireRootMFA"
              description = "Blocks root logins without MFA"

              policy = jsonencode({
                Version = "2012-10-17"
                Statement = [
                  {
                    Effect   = "Deny"
                    Action   = "sts:AssumeRole"
                    Resource = "*"
                    Condition = {
                      BoolIfExists = {
                        "aws:MultiFactorAuthPresent": "false"
                      }
                    }
                  }
                ]
              })
            }
            ```

    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/enable-virt-mfa-for-root.html
        title: Enable a virtual MFA device for the root user (console)
  - uid: mondoo-aws-security-iam-password-policy
    title: Ensure strong account password policy requirements are used
    filters: asset.platform == "aws"
    impact: 60
    props:
      - uid: mondooAWSSecurityIamPasswordPolicyMaxPasswordAge
        title: Define the maximum number of days a password is allowed to exist before being rotated
        mql: "90"
      - uid: mondooAWSSecurityIamPasswordPolicyMinimumPasswordLength
        title: Minimum password length
        mql: "14"
      - uid: mondooAWSSecurityIamPasswordPolicyPasswordReusePrevention
        title: Number of passwords before allowing reuse
        mql: "24"
      - uid: mondooAWSSecurityIamPasswordPolicyRequireLowercaseCharacters
        title: Denotes whether lowercase characters are required for passwords
        mql: "true"
      - uid: mondooAWSSecurityIamPasswordPolicyRequireNumbers
        title: Denotes whether numbers are required for passwords
        mql: "true"
      - uid: mondooAWSSecurityIamPasswordPolicyRequireSymbols
        title: Denotes whether symbols are required for passwords
        mql: "true"
      - uid: mondooAWSSecurityIamPasswordPolicyRequireUppercaseCharacters
        title: Denotes whether uppercase characters are required for passwords
        mql: "true"
    mql: |
      // Ensure properties do exist.
      aws.iam.accountPasswordPolicy.RequireUppercaseCharacters != empty
      aws.iam.accountPasswordPolicy.RequireLowercaseCharacters != empty
      aws.iam.accountPasswordPolicy.RequireSymbols != empty
      aws.iam.accountPasswordPolicy.RequireNumbers != empty
      aws.iam.accountPasswordPolicy.MinimumPasswordLength != empty
      aws.iam.accountPasswordPolicy.PasswordReusePrevention != empty
      aws.iam.accountPasswordPolicy.MaxPasswordAge != empty
      // Validate each policy setting against props
      aws.iam.accountPasswordPolicy.where(RequireUppercaseCharacters != empty).all(RequireUppercaseCharacters == props.mondooAWSSecurityIamPasswordPolicyRequireUppercaseCharacters)
      aws.iam.accountPasswordPolicy.where(RequireLowercaseCharacters != empty).all(RequireLowercaseCharacters == props.mondooAWSSecurityIamPasswordPolicyRequireLowercaseCharacters)
      aws.iam.accountPasswordPolicy.where(RequireSymbols != empty).all(RequireSymbols == props.mondooAWSSecurityIamPasswordPolicyRequireSymbols)
      aws.iam.accountPasswordPolicy.where(RequireNumbers != empty).all(RequireNumbers == props.mondooAWSSecurityIamPasswordPolicyRequireNumbers)
      aws.iam.accountPasswordPolicy.where(MinimumPasswordLength != empty).all(MinimumPasswordLength >= props.mondooAWSSecurityIamPasswordPolicyMinimumPasswordLength)
      aws.iam.accountPasswordPolicy.where(PasswordReusePrevention != empty).all(PasswordReusePrevention >= props.mondooAWSSecurityIamPasswordPolicyPasswordReusePrevention)
      aws.iam.accountPasswordPolicy.where(MaxPasswordAge != empty).all(MaxPasswordAge <= props.mondooAWSSecurityIamPasswordPolicyMaxPasswordAge)
    docs:
      desc: |
        This check ensures that AWS accounts enforce a strong IAM password policy to enhance security by preventing weak or easily guessable passwords. A well-defined password policy helps protect against brute-force attacks, credential stuffing, and unauthorized access.

        **Rationale:**

        Without a strong password policy, IAM users may use weak passwords, increasing the risk of compromised credentials. Implementing a strict password policy ensures that AWS accounts comply with security best practices and regulatory standards such as CIS AWS Foundations Benchmark, NIST, ISO 27001, PCI DSS, and SOC 2.

        A secure IAM password policy should enforce the following requirements:

        -	At least one uppercase character
        -	At least one lowercase character
        -	At least one number
        -	At least one symbol (e.g., !@#$%^&*)
        -	A minimum password length of 14 characters
        -	Prevention of password reuse for at least the last 24 passwords
        -	Password expiration after 90 days

        **Risk Mitigation:**

        -	**Prevents brute-force attacks:** Ensures passwords are complex and difficult to guess.
        -	**Reduces credential stuffing risks:** Enforces frequent password changes and prevents password reuse.
        -	**Ensures compliance:** Meets security requirements for regulated industries.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

           ```mql
           aws.iam.accountPasswordPolicy
           ```

          Example output:

          ```mql
          aws.iam.accountPasswordPolicy: {
            AllowUsersToChangePassword: true
            ExpirePasswords: true
            HardExpiry: false
            MaxPasswordAge: "180"
            MinimumPasswordLength: "14"
            PasswordReusePrevention: "24"
            RequireLowercaseCharacters: true
            RequireNumbers: true
            RequireSymbols: true
            RequireUppercaseCharacters: true
          }
          ```
      remediation:
        - id: console
          desc: |
            Using AWS Console:

            1.	Navigate to the AWS IAM Console.
            2.	Select Account settings in the left panel.
            3.	In the Password Policy section, ensure the following settings are enabled:
              -	Require at least one uppercase letter
              -	Require at least one lowercase letter
              -	Require at least one number
              -	Require at least one symbol
              -	Require a minimum password length of 14 characters
              -	Prevent password reuse for the last 24 passwords
              -	Require password expiration every 90 days
            4.	Select Apply Password Policy to enforce these settings.

        - id: cli
          desc: |
            Using AWS CLI:

            Check the current password policy settings:
            ```sh
            aws iam get-account-password-policy
            ```

            If no password policy exists, apply a strong password policy:
            ```sh
            aws iam update-account-password-policy \
            --minimum-password-length 14 \
            --require-symbols true \
            --require-numbers true \
            --require-uppercase-characters true \
            --require-lowercase-characters true \
            --max-password-age 90 \
            --password-reuse-prevention 24 \
            --allow-users-to-change-password true
            ```
        - id: terraform
          desc: |
            Using Terraform:

            Define a strict password policy using Terraform:
            ```hcl
            resource "aws_iam_account_password_policy" "strong_policy" {
              minimum_password_length        = 14
              require_uppercase_characters   = true
              require_lowercase_characters   = true
              require_numbers                = true
              require_symbols                = true
              max_password_age               = 90
              password_reuse_prevention      = 24
              allow_users_to_change_password = true
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation**

            CloudFormation can enforce a strong IAM password policy by creating an AWS::IAM::AccountPasswordPolicy resource with the necessary complexity requirements.
            ```yaml
            Resources:
              StrongIAMPasswordPolicy:
                Type: "AWS::IAM::AccountPasswordPolicy"
                Properties:
                  MinimumPasswordLength: 14
                  RequireUppercaseCharacters: true
                  RequireLowercaseCharacters: true
                  RequireNumbers: true
                  RequireSymbols: true
                  MaxPasswordAge: 90
                  PasswordReusePrevention: 24
                  AllowUsersToChangePassword: true
            ```
    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords.html
        title: User passwords in AWS
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html
        title: Set an account password policy for IAM users
  - uid: mondoo-aws-security-access-keys-rotated
    title: Ensure IAM user access keys are rotated
    filters: asset.platform == "aws"
    impact: 70
    props:
      - uid: mondooAWSSecurityMaxAccessKeyAge
        title: Define the maximum number of days an IAM key is allowed to exist before rotation
        mql: "90"
    mql: |
      aws.iam.credentialReport.where(accessKey1Active == true && time.now - userCreationTime > props.mondooAWSSecurityMaxAccessKeyAge * time.day).all(time.now - accessKey1LastRotated < props.mondooAWSSecurityMaxAccessKeyAge * time.day)
      aws.iam.credentialReport.where(accessKey2Active == true && time.now - userCreationTime > props.mondooAWSSecurityMaxAccessKeyAge * time.day).all(time.now - accessKey2LastRotated < props.mondooAWSSecurityMaxAccessKeyAge * time.day)
    docs:
      desc: |
        This check ensures that AWS IAM user access keys are regularly rotated to reduce the risk of credential compromise. Access keys should be changed periodically to prevent long-term exposure and limit the impact of potential key leaks. AWS best practices recommend rotating IAM access keys every 90 days or less.

        **Rationale:**

        IAM access keys provide programmatic access to AWS services. If an access key is compromised, an attacker could gain unauthorized access to AWS resources. Regularly rotating access keys limits the window of opportunity for an attacker to use stolen credentials.

        A strong key rotation policy helps organizations comply with security frameworks such as CIS AWS Foundations Benchmark, PCI DSS, NIST, ISO 27001, and SOC 2.

        **Risk Mitigation:**

        - Reduces the risk of long-term key exposure
        - Ensures compliance with security and regulatory standards
        - Limits the impact of potential credential leaks
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.iam.credentialReport.where( accessKey1Active == true || accessKey2Active == true ) { properties['user'] accessKey1Active accessKey2Active accessKey1LastRotated accessKey2LastRotated }
        ```

        Example output:

        ```mql
        aws.iam.credentialReport.where: [
          0: {
            accessKey1LastRotated: 2024-09-01 01:32:29 +0000 +0000
            accessKey2LastRotated: Never
            accessKey1Active: true
            accessKey2Active: false
            properties[user]: "jimmy"
          }
          1: {
            accessKey1LastRotated: 2024-09-09 19:16:35 +0000 +0000
            accessKey2LastRotated: Never
            accessKey1Active: true
            accessKey2Active: false
            properties[user]: "robert"
          }
          2: {
            accessKey1LastRotated: 2024-06-15 07:18:34 +0000 +0000
            accessKey2LastRotated: Never
            accessKey1Active: true
            accessKey2Active: false
            properties[user]: "johnpaul"
          }
          3: {
            accessKey1LastRotated: 2024-09-29 21:53:04 +0000 +0000
            accessKey2LastRotated: Never
            accessKey1Active: true
            accessKey2Active: false
            properties[user]: "bonzo"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Users in the left panel.
            3.	Select an IAM user and go to the Security Credentials tab.
            4.	Under Access Keys, check the Last Used and Created Date columns.
            5.	If a key is older than 90 days, create a new access key:
              - Select Create access key
              - Update AWS applications and scripts to use the new key
              - Deactivate and delete the old access key
            6.	Repeat this process for all IAM users with access keys.
        - id: cli
          desc: |
            **Using AWS CLI:**

            List all IAM users with active access keys:
            ```sh
            aws iam list-users --query "Users[*].UserName"
            ```

            Check the age of access keys for each user:
            ```sh
            aws iam list-access-keys --user-name <username> --query "AccessKeyMetadata[*].{AccessKeyId:AccessKeyId, CreateDate:CreateDate}"
            ```

            If a key is older than 90 days, create a new key and delete the old one:
            ```sh
            aws iam create-access-key --user-name <username>
            aws iam delete-access-key --access-key-id <old-access-key-id> --user-name <username>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            AWS does not support automatic access key rotation in Terraform, but you can enforce IAM policies to require regular key rotation:
            ```hcl
            resource "aws_iam_policy" "enforce_key_rotation" {
              name        = "EnforceKeyRotation"
              description = "Requires IAM users to rotate access keys every 90 days"

              policy = jsonencode({
                Version = "2012-10-17",
                Statement = [
                  {
                    Effect   = "Deny",
                    Action   = "iam:UpdateAccessKey",
                    Resource = "*",
                    Condition = {
                      NumericGreaterThan = {
                        "aws:MultiFactorAuthAge": 7776000  # 90 days in seconds
                      }
                    }
                  }
                ]
              })
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
        title: AWS Documentation - Manage access keys for IAM users
  - uid: mondoo-aws-security-mfa-enabled-for-iam-console-access
    title: Ensure Multi-Factor Authentication is Enabled for All IAM users with console access
    filters: asset.platform == "aws"
    impact: 90
    mql: |
      aws.iam.credentialReport.where(passwordEnabled == true).all(mfaActive == true)
    docs:
      desc: |
        This check ensures that all AWS IAM users with console access have Multi-Factor Authentication (MFA) enabled. MFA provides an additional layer of security beyond just a username and password, reducing the risk of unauthorized account access in case credentials are compromised.

        **Rationale:**

        IAM users with console access rely on password-based authentication, which is vulnerable to phishing, credential leaks, and brute-force attacks. Without MFA, an attacker who obtains an IAM user’s password can gain unauthorized access to AWS resources. Enforcing MFA helps mitigate this risk by requiring a second factor, such as a one-time code from a mobile authenticator app or a hardware security key.

        AWS best practices and security frameworks such as CIS AWS Foundations Benchmark, PCI DSS, NIST, ISO 27001, and SOC 2 require MFA for all users with administrative or privileged access.

        **Risk Mitigation:**

        -	Prevents unauthorized access due to stolen or weak passwords
        -	Enhances security posture by enforcing multi-layer authentication
        -	Aligns with compliance and security best practices
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Run this command: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.iam.credentialReport.where(
          mfaActive != true
        ) {arn properties["user"]}
        ```
        Example output:

        ```mql
        aws.iam.credentialReport.where: [
          0: {
            properties[user]: "test-iam-user"
            arn: "arn:aws:iam::053121068929:user/users/test-iam-user"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Users in the left panel.
            3.	Identify users with Console Access enabled.
            4.	Select a user and go to the Security Credentials tab.
            5.	Under Multi-Factor Authentication (MFA), check if MFA is enabled.
            6.	If MFA is not enabled:
              - Select Assign MFA Device
              - Choose a method (Virtual MFA Device, FIDO Security Key, or Hardware MFA Token)
              - Follow the setup process and enter the required MFA codes
            7.	Repeat this process for all IAM users with console access.
        - id: cli
          desc: |
            **Using AWS CLI:**

            List IAM users with console access and check their MFA status:
            ```sh
            aws iam list-users --query "Users[*].UserName"
            ```

            For each user, check if MFA is enabled:
            ```sh
            aws iam list-mfa-devices --user-name <username>
            ```

            If no MFA device is listed, enforce MFA by setting up a virtual MFA device:
            ```sh
            aws iam enable-mfa-device --user-name <username> --serial-number "arn-of-mfa-device" --authentication-code-1 123456 --authentication-code-2 654321
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            AWS does not allow enforcing MFA directly via Terraform, but you can create an IAM policy that denies access unless MFA is enabled:
            ```hcl
            resource "aws_iam_policy" "enforce_mfa" {
              name        = "EnforceMFA"
              description = "Require MFA for all IAM users with console access"

              policy = jsonencode({
                Version = "2012-10-17",
                Statement = [
                  {
                    Effect   = "Deny",
                    Action   = [
                      "ec2:*",
                      "s3:*",
                      "iam:*",
                      "lambda:*"
                    ],
                    Resource  = "*",
                    Condition = {
                      Bool = {
                        "aws:MultiFactorAuthPresent": "false"
                      }
                    }
                  }
                ]
              })
            }
            ```

    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html
        title: AWS Multi-factor authentication in IAM
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_virtual.html
        title: Assign a virtual MFA device in the AWS Management Console
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs
        title: Terraform Documentation - AWS Provider
  - uid: mondoo-aws-security-iam-group-has-users-check
    title: Ensure IAM Groups are utilized by Assigning at Least One user
    impact: 30
    docs:
      desc: |
        This check ensures that IAM groups in AWS have at least one assigned user. IAM groups help manage permissions efficiently by allowing administrators to assign policies at the group level instead of managing permissions for individual IAM users. If IAM groups exist without any users, it indicates potential misconfigurations or unused access controls.

        **Rationale:**

        IAM groups simplify access management by allowing permissions to be applied collectively to users rather than individually. This reduces administrative overhead, enforces consistency, and minimizes the risk of misconfigured permissions. If IAM groups exist but have no users assigned, it may indicate:

        - Poor permission management practices.
        - Orphaned groups that should be removed or repurposed.
        - A failure to follow the principle of least privilege, leading to excessive permissions being granted to individual users.

        By ensuring that IAM groups are properly utilized, organizations can improve security, maintain structured access controls, and reduce human errors in permission assignments.

        **Risk Mitigation:**

        -	Reduces the risk of misconfigured permissions by enforcing structured access control.
        -	Simplifies permission management by grouping users with similar roles.
        -	Aligns with security best practices and compliance frameworks such as CIS AWS Foundations Benchmark, NIST, ISO 27001, and SOC 2.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.iam.groups.where( usernames.length == 0 ) {*}
        ```

        Example output:

        ```mql
        aws.iam.groups.where: [
          0: {
            name: "MyUserGroup"
            id: "AGPASSOFBMF7OMHVGHACB"
            createDate: 2022-01-11 18:19:26 +0000 UTC
            createAt: 2022-01-11 18:19:26 +0000 UTC
            usernames: []
            arn: "arn:aws:iam::12345678910:group/MyUserGroup"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Groups in the left panel.
            3.	Review IAM groups and check if they have at least one assigned user.
            4.	If a group has no users:
              -	Select the group name.
              -	Go to the Users tab and select Add Users to Group.
              -	Select the appropriate users and select Add Users.
            5.	If an IAM group is not needed, consider deleting it to reduce clutter.
        - id: cli
          desc: |
            **Using AWS CLI:**

            List all IAM groups:
            ```sh
            aws iam list-groups --query "Groups[*].GroupName"
            ```

            Check if an IAM group has users assigned:
            ```sh
            aws iam get-group --group-name <group-name> --query "Users[*].UserName"
            ```

            If a group has no users, add a user to the group:
            ```sh
            aws iam add-user-to-group --group-name <group-name> --user-name <user-name>
            ```

            If a group is unused and unnecessary, delete it:
            ```sh
            aws iam delete-group --group-name <group-name>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure IAM users are assigned to groups in Terraform:
            ```hcl
            resource "aws_iam_group" "admins" {
              name = "Admins"
            }

            resource "aws_iam_user" "user1" {
              name = "user1"
            }

            resource "aws_iam_group_membership" "admin_membership" {
              name  = "admin-membership"
              group = aws_iam_group.admins.name
              users = [aws_iam_user.user1.name]
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_manage.html
        title: AWS Documentation - IAM user groups
    variants:
      - uid: mondoo-aws-security-iam-group-has-users-check-account
      - uid: mondoo-aws-security-iam-group-has-users-check-single-group

  - uid: mondoo-aws-security-iam-group-has-users-check-account
    filters: asset.platform == "aws"
    mql: aws.iam.groups.all(usernames != empty)
  - uid: mondoo-aws-security-iam-group-has-users-check-single-group
    filters: asset.platform == "aws-iam-group"
    mql: aws.iam.group.usernames != empty

  - uid: mondoo-aws-security-iam-users-only-one-access-key
    title: Ensure there is only one active access key available for any single IAM user
    impact: 70
    filters: asset.platform == "aws"
    mql: |
      aws.iam.users.where(accessKeys.flat.where(Status == "Active")).all(accessKeys[0].length <= 1)
    docs:
      desc: |
        This check ensures that each AWS IAM user has no more than one active access key. IAM users can have up to two access keys, but maintaining multiple active keys increases the risk of credential leakage and unauthorized access if one of the keys is compromised. Best practices dictate that only one active key should exist per IAM user to reduce attack surfaces and ensure proper key management.

        **Rationale:**

        Access keys provide programmatic access to AWS services. If multiple active keys exist for a user:

        -	The risk of credential exposure increases.
        -	It becomes harder to track and rotate access keys securely.
        -	Unused keys may remain active longer than necessary, violating least privilege principles.

        AWS best practices and security frameworks such as CIS AWS Foundations Benchmark, PCI DSS, NIST, ISO 27001, and SOC 2 recommend that IAM users should have at most one active access key at any time.

        **Risk Mitigation:**

        -	Reduces exposure to key compromise by minimizing unused or unnecessary credentials.
        -	Simplifies access key rotation by ensuring only one key is active at any given time.
        -	Enhances security posture by enforcing least privilege access for IAM users.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.iam.users.where(accessKeys[0].length >= 1)
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Users in the left panel.
            3.	Select a user and go to the Security Credentials tab.
            4.	Under Access Keys, check how many active keys exist.
            5.	If more than one active access key is present:

              - Determine which key is in use.
              - Disable and delete the extra key(s).
              - If a key is needed for rotation, create a new key before deleting the old one.

        - id: cli
          desc: |
            **Using AWS CLI:**

            List all IAM users:
            ```sh
            aws iam list-users --query "Users[*].UserName"
            ```

            Check access keys for a specific user:
            ```sh
            aws iam list-access-keys --user-name <username> --query "AccessKeyMetadata[*].{AccessKeyId:AccessKeyId, Status:Status}"
            ```

            If more than one Active (Status: Active) key exists, deactivate and delete extra keys:
            ```sh
            aws iam update-access-key --user-name <username> --access-key-id <access-key-id> --status Inactive
            aws iam delete-access-key --user-name <username> --access-key-id <access-key-id>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            AWS does not support direct enforcement of single active access keys in Terraform, but you can enforce a security policy to limit IAM users to one active key:
            ```hcl
            resource "aws_iam_policy" "limit_access_keys" {
              name        = "LimitAccessKeys"
              description = "Ensure IAM users have at most one active access key"

              policy = jsonencode({
                Version = "2012-10-17",
                Statement = [
                  {
                    Effect   = "Deny",
                    Action   = "iam:CreateAccessKey",
                    Resource = "*",
                    Condition = {
                      NumericGreaterThan = {
                        "iam:AccessKeysCount": 1
                      }
                    }
                  }
                ]
              })
            }
            ```



  - uid: mondoo-aws-security-iam-user-no-inline-policies-check
    title: Ensure IAM users receive permissions only through groups
    impact: 70
    docs:
      desc: |
        This check ensures that AWS IAM users do not have directly attached policies and instead receive permissions exclusively through IAM groups. Assigning permissions via groups simplifies access management, enforces least privilege principles, and reduces the risk of misconfigured permissions at the individual user level.

        **Rationale:**

        IAM users should not have inline or managed policies attached directly to their accounts. Instead, permissions should be granted via IAM groups to:

        - Ensure consistent and scalable permission management
        - Reduce the risk of excessive privileges assigned to individual users
        - Improve auditability and compliance with security frameworks such as CIS AWS Foundations Benchmark, NIST, ISO 27001, and SOC 2

        By enforcing group-based access control, administrators can efficiently manage role-based permissions and prevent overly permissive access for individual IAM users.

        **Risk Mitigation:**

        - Reduces administrative overhead by managing permissions collectively
        - Prevents privilege creep by enforcing structured access control
        - Enhances security posture by ensuring users inherit only necessary permissions

      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.iam.users.where( policies.length != 0 || attachedPolicies.length != 0 ) { name arn policies attachedPolicies }
        ```

        Example output:

        ```mql
        aws.iam.users.where: [
          0: {
            arn: "arn:aws:iam::1234567890987:user/1234567890987-alice"
            name: "1234567890987-alice"
            attachedPolicies: []
            policies: [
              0: "excess_policy"
            ]
          }
          1: {
            arn: "arn:aws:iam::1234567890987:user/maria"
            name: "maria"
            attachedPolicies: [
              0: aws.iam.policy id = arn:aws:iam::1234567890987:policy/ec2-instance-connect-sendssh
            ]
            policies: []
          }
          2: {
            arn: "arn:aws:iam::1234567890987:user/bobby"
            name: "bobby"
            attachedPolicies: [
              0: aws.iam.policy id = arn:aws:iam::1234567890987:policy/terraform20210901011436036200000004
            ]
            policies: []
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS IAM Console.
            2.	Select Users in the left panel.
            3.	Select a user and go to the Permissions tab.
            4.	Identify users with directly attached policies (either managed or inline policies).
            5.	If direct policies exist:
              - Detach managed policies from the user.
              - Delete inline policies assigned to the user.
              - Assign the user to an IAM group that grants the necessary permissions.

        - id: cli
          desc: |
            **Using AWS CLI:**

            List all IAM users and check if they have directly attached policies:
            ```sh
            aws iam list-users --query "Users[*].UserName"
            ```

            Check managed policies attached to a user:
            ```sh
            aws iam list-attached-user-policies --user-name <username>
            ```

            Detach any managed policies:
            ```sh
            aws iam detach-user-policy --user-name <username> --policy-arn <policy-arn>
            ```

            Check inline policies assigned to a user:
            ```sh
            aws iam list-user-policies --user-name <username>
            ```

            Delete any inline policies:
            ```sh
            aws iam delete-user-policy --user-name <username> --policy-name <policy-name>
            ```

            Assign the user to an IAM group instead:
            ```sh
            aws iam add-user-to-group --group-name <group-name> --user-name <username>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Enforce group-based permissions and prevent direct policy assignments:
            ```hcl
            resource "aws_iam_group" "developers" {
              name = "Developers"
            }

            resource "aws_iam_user" "user1" {
              name = "user1"
            }

            resource "aws_iam_group_membership" "dev_group_membership" {
              name  = "dev-membership"
              group = aws_iam_group.developers.name
              users = [aws_iam_user.user1.name]
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html
        title: Managed policies and inline policies
    variants:
      - uid: mondoo-aws-security-iam-user-no-inline-policies-check-account
      - uid: mondoo-aws-security-iam-user-no-inline-policies-check-single-user
  - uid: mondoo-aws-security-iam-user-no-inline-policies-check-account
    filters: asset.platform == "aws"
    mql: |
      aws.iam.users.all(policies == empty)
      aws.iam.users.all(attachedPolicies == empty)
  - uid: mondoo-aws-security-iam-user-no-inline-policies-check-single-user
    filters: asset.platform == "aws-iam-user"
    mql: |
      aws.iam.user.policies == empty
      aws.iam.user.attachedPolicies == empty

  - uid: mondoo-aws-security-vpc-default-security-group-closed
    title: Ensure the default security group of every VPC restricts all traffic
    impact: 80
    docs:
      desc: |
        This check ensures that the default security group (SG) for every AWS Virtual Private Cloud (VPC) is configured to restrict all inbound and outbound traffic. By default, AWS creates a default security group for each VPC, which allows unrestricted communication between instances associated with the group. This can pose a significant security risk if not properly restricted.

        **Rationale:**

        The default security group in a VPC automatically allows all inbound and outbound traffic between instances using the same security group, which can:

        - Allow unintended access between instances, increasing the risk of lateral movement.
        -	Expose resources to potential unauthorized access if assigned inadvertently.
        -	Violate security best practices and compliance standards such as CIS AWS Foundations Benchmark, NIST, ISO 27001, and PCI DSS.

        To mitigate this risk, the default security group should be modified to restrict all traffic by removing all inbound and outbound rules.

        **Risk Mitigation:**

        -	Prevents unintended access by eliminating unrestricted communication between instances.
        -	Enforces least privilege by requiring explicit security group assignments.
        -	Improves network security posture by reducing exposure to internal threats.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.ec2.securityGroups.where(
          name == "default"
        ).where(
          ipPermissions.length != 0
          || ipPermissionsEgress.length != 0
        ){id name region ipPermissions{*} ipPermissionsEgress{*}}
        ```

        Example output:

        ```mql
        aws.ec2.securityGroups.where.where: [
          0: {
            ipPermissions: [
              0: {
                id: "sg-0bd4b1ef47132d3de-0"
                fromPort: 0
                toPort: 0
                ipProtocol: "-1"
                ipv6Ranges: []
                ipRanges: []
              }
            ]
            ipPermissionsEgress: []
            name: "default"
            region: "eu-north-1"
            id: "sg-0bd4b1ef47132d3de"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            Using AWS Console:

            1.	Navigate to the AWS VPC Console.
            2.	Select Security Groups from the left panel.
            3.	Filter by “default” security groups under the VPC ID column.
            4.	For each default security group:
              -	Remove all inbound rules by selecting Edit inbound rules and deleting all existing entries.
              -	Remove all outbound rules by selecting Edit outbound rules and deleting all existing entries.
            5.	Save the changes to ensure that the default security group does not allow any traffic.
        - id: cli
          desc: |
            Using AWS CLI:

            List all default security groups:
            ```sh
            aws ec2 describe-security-groups --filters Name=group-name,Values=default --query "SecurityGroups[*].GroupId"
            ```
            For each default security group, remove all inbound rules:
            ```sh
            aws ec2 revoke-security-group-ingress --group-id <sg-id> --protocol all --port -1 --cidr 0.0.0.0/0
            ```
            Remove all outbound rules:
            ```sh
            aws ec2 revoke-security-group-egress --group-id <sg-id> --protocol all --port -1 --cidr 0.0.0.0/0
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure the default security group restricts all traffic:
            ```hcl
            resource "aws_default_security_group" "default_sg" {
              vpc_id = aws_vpc.main.id

              ingress = []  # No inbound rules
              egress  = []  # No outbound rules

              tags = {
                Name = "Restricted Default SG"
              }
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-controls-reference.html
        title: AWS Documentation - Security Hub controls reference
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/index.html
        title: AWS Documentation - AWS CLI Reference - EC2
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs
        title: Terraform Documentation - AWS Provider
    variants:
      - uid: mondoo-aws-security-vpc-default-security-group-closed-account
      - uid: mondoo-aws-security-vpc-default-security-group-closed-secgroup
  - uid: mondoo-aws-security-vpc-default-security-group-closed-account
    title: Ensure the default security group of every VPC restricts all traffic
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.securityGroups.where(name == "default").all(
        ipPermissions == empty
      )
      aws.ec2.securityGroups.where(name == "default").all(
        ipPermissionsEgress == empty
      )
  - uid: mondoo-aws-security-vpc-default-security-group-closed-secgroup
    title: Ensure the default security group of every VPC restricts all traffic
    filters: asset.platform == "aws-security-group" && aws.ec2.securitygroup.name == "default"
    mql: |
      aws.ec2.securitygroup.ipPermissions == empty
      aws.ec2.securitygroup.ipPermissionsEgress == empty


  - uid: mondoo-aws-security-ec2-ebs-encryption-by-default
    title: Ensure EBS volume encryption is enabled by default
    filters: asset.platform == "aws"
    impact: 90
    mql: aws.ec2.ebsEncryptionByDefault.values.all(_ == true)
    docs:
      desc: |
        This check ensures that Amazon Elastic Block Store (EBS) volume encryption is enabled by default in an AWS account. Enabling default encryption ensures that all newly created EBS volumes are encrypted automatically using AWS Key Management Service (KMS) keys. This eliminates the risk of unencrypted volumes being created inadvertently.

        **Rationale:**

        By default, EBS volumes are not encrypted unless specified during creation. Without default encryption, users may create unencrypted EBS volumes, exposing sensitive data to unauthorized access if the volume is compromised. Enabling default EBS encryption ensures:

        -	Consistent enforcement of encryption across all new volumes.
        -	Improved security by protecting data at rest.
        -	Simplified compliance with regulations such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, and ISO 27001.

        **Risk Mitigation:**

        -	Prevents accidental creation of unencrypted volumes by enforcing encryption automatically.
        -	Ensures compliance with data protection standards that require encryption at rest.
        -	Reduces the risk of data breaches by protecting data stored on AWS-managed disks.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

          ```mql
          aws.ec2.ebsEncryptionByDefault
          ```

          Example output:

          ```mql
          aws.ec2.ebsEncryptionByDefault
            aws.ec2.ebsEncryptionByDefault: {
              ap-northeast-1: false
              ap-northeast-2: false
              ap-northeast-3: false
              ap-south-1: false
              ap-southeast-1: false
              ap-southeast-2: false
              ca-central-1: false
              eu-central-1: false
              eu-north-1: false
              eu-west-1: false
              eu-west-2: false
              eu-west-3: false
              sa-east-1: false
              us-east-1: true
              us-east-2: true
              us-west-1: false
              us-west-2: false
            }
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EC2 Console.
            2.	Select EC2 Dashboard > EBS > Settings.
            3.	Locate the Always encrypt new EBS volumes option.
            4.	If encryption is disabled, select Edit and check Enable encryption by default.
            5.	Select the KMS key to use (either the default AWS-managed key or a customer-managed key).
            6.	Select Save changes to enforce encryption by default.

        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if EBS encryption is enabled by default:
            ```sh
            aws ec2 get-ebs-encryption-by-default
            ```

            If the output is false, enable default encryption:
            ```sh
            aws ec2 enable-ebs-encryption-by-default
            ```

            To specify a custom KMS key for encryption:
            ```sh
            aws ec2 modify-ebs-default-kms-key-id --kms-key-id "arn:aws:kms:region:account-id:key/key-id"
            ```

        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure EBS encryption is enabled by default:
            ```hcl
            resource "aws_ebs_encryption_by_default" "default_encryption" {}
            ```

            To use a specific KMS key for encryption:
            ```hcl
            resource "aws_ebs_default_kms_key" "default" {
              key_arn = aws_kms_key.ebs_encryption.arn
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by-default
        title: AWS Documentation - Encryption by default

  - uid: mondoo-aws-security-s3-buckets-account-level-block-public-access
    title: Ensure public access to S3 buckets is blocked at the account level
    filters: asset.platform == "aws"
    impact: 95
    mql: |
      aws.s3control.accountPublicAccessBlock != empty
      aws.s3control.accountPublicAccessBlock.values.all(_ == true)
    docs:
      desc: |
        This check ensures that Amazon S3 Public Access Block settings are enabled at the account level to prevent accidental or intentional exposure of data stored in S3 buckets. Enforcing this setting at the account level ensures that no S3 bucket or object can be made public, even if bucket policies or ACLs attempt to allow public access.

        **Rationale:**

        By default, S3 allows granular control over access policies, but misconfigurations in bucket policies, access control lists (ACLs), or object permissions can lead to data leaks. Enforcing S3 Public Access Block settings at the account level ensures that:

        -	All buckets are protected from accidental public exposure.
        -	Individual users cannot override security policies by changing bucket-level settings.
        -	Compliance requirements for data protection and privacy regulations (such as CIS AWS Foundations Benchmark, GDPR, PCI DSS, and ISO 27001) are met.

        **Risk Mitigation:**

        -	Prevents accidental public data exposure by blocking public access to all S3 resources.
        -	Enhances security posture by enforcing organization-wide security policies.
        -	Ensures compliance with regulatory frameworks that require strict data access controls.
      audit: |
        __cnspec shell__

        1. Open a terminal
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.s3control.accountPublicAccessBlock
        ```

        Example output:

        ```mql
        aws.s3control.accountPublicAccessBlock: null
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS S3 Console.
            2.	Select Block Public Access Settings for this Account.
            3.	Ensure the following options are enabled:
              - Block public access to buckets and objects granted through new access control lists (ACLs)
              - Block public access to buckets and objects granted through any access control lists (ACLs)
              - Block public access to buckets and objects granted through new public bucket policies
              - Block public and cross-account access to buckets that have public policies
            4.	Select Save Changes to enforce these settings at the account level.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if public access is blocked at the account level:
            ```sh
            aws s3control get-public-access-block --account-id <account-id>
            ```

            If any of the settings are missing or set to false, enable full public access block:
            ```sh
            aws s3control put-public-access-block \
            --account-id <account-id> \
            --public-access-block-configuration '{
                "BlockPublicAcls": true,
                "IgnorePublicAcls": true,
                "BlockPublicPolicy": true,
                "RestrictPublicBuckets": true
            }'
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure account-level S3 public access block settings are enforced:
            ```hcl
            resource "aws_s3_account_public_access_block" "account_block" {
              block_public_acls       = true
              ignore_public_acls      = true
              block_public_policy     = true
              restrict_public_buckets = true
            }
            ```
    refs:
      - url: https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html
        title: Blocking public access to your Amazon S3 storage
      - url: https://docs.aws.amazon.com/AmazonS3/latest/userguide/configuring-block-public-access-account.html
        title: Configuring block public access settings for your account

  - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited
    title: Ensure Amazon S3 Buckets Have Public Access Restrictions Enforced
    impact: 100
    docs:
      desc: |
        This check ensures that each Amazon S3 bucket has public access restrictions enforced to prevent unauthorized exposure of sensitive data. While account-level public access block settings provide a broad safeguard, bucket-level controls ensure that no individual bucket is unintentionally exposed due to misconfigurations in bucket policies, ACLs, or object permissions.

        **Rationale:**

        Amazon S3 buckets can be made publicly accessible through:

        -	Bucket policies that allow public s3:GetObject or s3:ListBucket permissions.
        -	Access Control Lists (ACLs) that grant READ or WRITE permissions to AllUsers (0.0.0.0/0) or AuthenticatedUsers.
        -	Cross-account access where a bucket is unintentionally exposed via an overly permissive policy.

        Unrestricted public access to S3 buckets can result in:

        -	Data breaches exposing sensitive files to unauthorized users.
        -	Compliance violations under frameworks such as CIS AWS Foundations Benchmark, GDPR, PCI DSS, and ISO 27001.
        -	Malicious exploitation, including data theft, ransomware attacks, or unauthorized data modifications.

        By enforcing bucket-level public access restrictions, organizations can prevent unintended data exposure while maintaining granular control over access permissions.

        **Risk Mitigation:**

        -	Prevents accidental or intentional data leaks due to misconfigured bucket policies or ACLs.
        -	Strengthens security posture by enforcing explicit access control at the bucket level.
        -	Ensures regulatory compliance with industry data protection standards.

      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

        ```mql
          aws.s3.buckets.all(
            publicAccessBlock != empty && publicAccessBlock.values.all(_ == true)
          )
        ```

        Example output:

        ```mql
          [failed] [].all()
            actual:   [
              0: aws.s3.bucket id = arn:aws:s3:::mondoo-test2.policies.test-ui
            ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS S3 Console.
            2.	Select a bucket and go to the Permissions tab.
            3.	Under Block public access (bucket settings), ensure the following options are enabled:
              -	Block public access to buckets and objects granted through new access control lists (ACLs)
              -	Block public access to buckets and objects granted through any access control lists (ACLs)
              -	Block public access to buckets and objects granted through new public bucket policies
              -	Block public and cross-account access to buckets that have public policies
            4.	Under Bucket policy, review the policy and ensure it does not allow `Principal: "*"` or overly permissive access rules.
            5.	Under Access Control List (ACLs), verify that no objects or buckets have `READ` or `WRITE` permissions assigned to Everyone `(public)` or `Authenticated Users`.
            6.	Save changes and confirm that public access is fully restricted.

        - id: cli
          desc: |
            **Using AWS CLI:**

            Check the public access block settings for a specific bucket:
            ```sh
            aws s3api get-public-access-block --bucket <bucket-name>
            ```

            If public access is not fully blocked, apply the following settings:
            ```sh
            aws s3api put-public-access-block --bucket <bucket-name> --public-access-block-configuration '{
                "BlockPublicAcls": true,
                "IgnorePublicAcls": true,
                "BlockPublicPolicy": true,
                "RestrictPublicBuckets": true
            }'
            ```

            Check if the bucket policy allows public access:
            ```sh
            aws s3api get-bucket-policy --bucket <bucket-name> --query "Policy"
            ```

            If a policy allows public access (Principal: "*", "Effect": "Allow", and "Action": "s3:*"), remove or modify it:
            ```sh
            aws s3api delete-bucket-policy --bucket <bucket-name>
            ```

            Check if ACLs allow public or cross-account access:
            ```sh
            aws s3api get-bucket-acl --bucket <bucket-name>
            ```

            If ACLs allow public access ("Grantee": { "URI": "http://acs.amazonaws.com/groups/global/AllUsers" }), remove the ACL:
            ```sh
            aws s3api put-bucket-acl --bucket <bucket-name> --acl private
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure S3 public access block settings are enforced at the bucket level:
            ```hcl
            resource "aws_s3_bucket_public_access_block" "secure_bucket" {
              bucket = aws_s3_bucket.my_bucket.id

              block_public_acls       = true
              ignore_public_acls      = true
              block_public_policy     = true
              restrict_public_buckets = true
            }
            ```

            Ensure S3 bucket policies do not allow public access:
            ```
            resource "aws_s3_bucket_policy" "secure_policy" {
              bucket = aws_s3_bucket.my_bucket.id
              policy = jsonencode({
                Version = "2012-10-17",
                Statement = [
                  {
                    Effect    = "Deny",
                    Principal = "*",
                    Action    = "s3:*",
                    Resource  = [
                      "arn:aws:s3:::${aws_s3_bucket.my_bucket.id}",
                      "arn:aws:s3:::${aws_s3_bucket.my_bucket.id}/*"
                    ],
                    Condition = {
                      Bool = {
                        "aws:SecureTransport": "false"
                      }
                    }
                  }
                ]
              })
            }
            ```

    refs:
      - url: https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-controls-reference.html
        title: AWS Documentation - Security Hub controls reference
      - url: https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html
        title: AWS Documentation - Blocking public access to your Amazon S3 storage
      - url: https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html
        title: AWS CLI Command Reference - aws s3api put-public-access-block
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block
        title: Terraform Documentation - AWS Provider - aws_s3_bucket_public_access_block
    variants:
      - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-account
      - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-bucket
      - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-terraform-hcl
      - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-terraform-plan
      - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-terraform-state
  - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-account
    filters: asset.platform == "aws"
    mql: |
      aws.s3.buckets.all(publicAccessBlock != empty)
      aws.s3.buckets.all(publicAccessBlock.BlockPublicAcls == true)
      aws.s3.buckets.all(publicAccessBlock.BlockPublicPolicy == true)
      aws.s3.buckets.all(publicAccessBlock.IgnorePublicAcls == true)
      aws.s3.buckets.all(publicAccessBlock.RestrictPublicBuckets == true)
  - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-bucket
    filters: asset.platform == "aws-s3-bucket"
    mql: |
      aws.s3.bucket.publicAccessBlock != empty
      aws.s3.bucket.publicAccessBlock.BlockPublicAcls == true
      aws.s3.bucket.publicAccessBlock.BlockPublicPolicy == true
      aws.s3.bucket.publicAccessBlock.IgnorePublicAcls == true
      aws.s3.bucket.publicAccessBlock.RestrictPublicBuckets == true
  - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_s3_bucket" )
    mql: |
      terraform.resources.where( nameLabel == "aws_s3_bucket_public_access_block" ).all( arguments.block_public_acls == true )
      terraform.resources.where( nameLabel == "aws_s3_bucket_public_access_block" ).all( arguments.block_public_policy == true )
      terraform.resources.where( nameLabel == "aws_s3_bucket_public_access_block" ).all( arguments.ignore_public_acls == true )
      terraform.resources.where( nameLabel == "aws_s3_bucket_public_access_block" ).all( arguments.restrict_public_buckets == true )
  - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_s3_bucket" )
    mql: |
      terraform.plan.resourceChanges.where( type == "aws_s3_bucket_public_access_block" ).all( change.after.block_public_acls == true )
      terraform.plan.resourceChanges.where( type == "aws_s3_bucket_public_access_block" ).all( change.after.block_public_policy == true )
      terraform.plan.resourceChanges.where( type == "aws_s3_bucket_public_access_block" ).all( change.after.ignore_public_acls == true )
      terraform.plan.resourceChanges.where( type == "aws_s3_bucket_public_access_block" ).all( change.after.restrict_public_buckets == true )
  - uid: mondoo-aws-security-s3-bucket-level-public-access-prohibited-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_s3_bucket" )
    mql: |
      terraform.state.resources.where( type == "aws_s3_bucket_public_access_block" ).all( values.block_public_acls == true )
      terraform.state.resources.where( type == "aws_s3_bucket_public_access_block" ).all( values.block_public_policy == true )
      terraform.state.resources.where( type == "aws_s3_bucket_public_access_block" ).all( values.ignore_public_acls == true )
      terraform.state.resources.where( type == "aws_s3_bucket_public_access_block" ).all( values.restrict_public_buckets == true )


  - uid: mondoo-aws-security-ec2-instance-no-public-ip
    title: Ensure No Public IP associated with EC2 Instances
    impact: 80
    variants:
      - uid: mondoo-aws-security-ec2-instance-no-public-ip-all
      - uid: mondoo-aws-security-ec2-instance-no-public-ip-single
      - uid: mondoo-aws-security-ec2-instance-no-public-ip-terraform-hcl
      - uid: mondoo-aws-security-ec2-instance-no-public-ip-terraform-plan
      - uid: mondoo-aws-security-ec2-instance-no-public-ip-terraform-state
    docs:
      desc: |
        This check ensures that Amazon EC2 instances do not have public IP addresses to prevent unintended exposure of cloud resources to the internet. Instances with public IPs can be directly accessed from the internet, increasing the risk of security breaches, unauthorized access, and potential data exfiltration.

        **Rationale:**

        By default, EC2 instances launched in a public subnet may receive an auto-assigned public IP unless explicitly disabled. Additionally, Elastic IPs (EIPs) can be manually associated with instances. Direct public access to EC2 instances poses serious security risks, including:

        -	Increased attack surface for brute-force, DDoS, or credential stuffing attacks.
        -	Unauthorized access if security groups, ACLs, or instance configurations are misconfigured.
        -	Regulatory non-compliance with standards such as CIS AWS Foundations Benchmark, PCI DSS, and ISO 27001.

        To mitigate these risks, EC2 instances should be placed in private subnets and accessed securely using bastion hosts, AWS Systems Manager Session Manager, or VPNs instead of exposing them directly to the internet.

        **Risk Mitigation:**

        -	Reduces exposure to external threats by eliminating direct public access.
        -	Improves security posture by enforcing network segmentation best practices.
        -	Ensures compliance with cloud security frameworks and industry standards.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query to return a list of all running EC2 instances across all enabled regions that along with the `instanceId`, `region`, and the configured `publicIp`:

        ```mql
        aws.ec2.instances.where( state = "running" && publicIp != "" ) { instanceId region tags publicIp }
        ```

        Example output:

        ```mql
        aws.ec2.instances.where: [
          0: {
            instanceId: "i-0070af411a515f14a"
            tags: {
              Environment: "windows-development-vpc"
              Name: "win19-dev-workstation-106e1f1c"
              Terraform: "true"
            }
            publicIp: "54.55.222.9"
            region: "us-east-1"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EC2 Console.
            2.	Select Instances in the left panel.
            3.	Check the Public IPv4 Address column for any assigned public IPs.
            4.	If an instance has a public IP:
              -	Select the instance and select Networking > Manage IP Addresses.
              -	If using an Elastic IP (EIP), disassociate it from the instance.
              -	Modify the instance’s subnet settings to disable auto-assign public IPs.
            5.	If public access is necessary, use a bastion host, VPN, or AWS Systems Manager Session Manager instead of directly exposing the instance.
        - id: cli
          desc: |
            **Using AWS CLI:**

            List EC2 instances with public IP addresses:
            ```sh
            aws ec2 describe-instances --query "Reservations[*].Instances[*].[InstanceId, PublicIpAddress]" --output table
            ```

            If any instances have a Public IP, remove manually assigned Elastic IPs:
            ``sh
            aws ec2 disassociate-address --public-ip <public-ip-address>
            ```

            Modify an instance’s network interface to remove the public IP (requires instance stop/start):
            ``sh
            aws ec2 modify-instance-attribute --instance-id <instance-id> --no-associate-public-ip-address
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure that EC2 instances are launched without public IPs:
            ```hcl
            resource "aws_instance" "secure_instance" {
              ami             = "ami-12345678"
              instance_type   = "t3.micro"
              subnet_id       = aws_subnet.private_subnet.id
              associate_public_ip_address = false  # Ensure no public IP is assigned

              tags = {
                Name = "PrivateInstance"
              }
            }
            ```

            Ensure that default VPC subnets do not auto-assign public IPs:
            ```hcl
            resource "aws_subnet" "private_subnet" {
              vpc_id            = aws_vpc.main.id
              cidr_block        = "10.0.1.0/24"
              map_public_ip_on_launch = false  # Disable auto-assigned public IPs
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure that EC2 instances do not receive public IPs in CloudFormation templates:
            ```yaml
            Resources:
              PrivateInstance:
                Type: "AWS::EC2::Instance"
                Properties:
                  ImageId: "ami-12345678"
                  InstanceType: "t3.micro"
                  SubnetId: !Ref PrivateSubnet
                  NetworkInterfaces:
                    - AssociatePublicIpAddress: false
                      DeviceIndex: 0
              PrivateSubnet:
                Type: "AWS::EC2::Subnet"
                Properties:
                  VpcId: !Ref VPC
                  CidrBlock: "10.0.1.0/24"
                  MapPublicIpOnLaunch: false
            ```
    refs:
      - url: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html
        title: AWS Documentation - IP addressing for your VPCs and subnets
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance
        title: Terraform Registry - aws_instance

  - uid: mondoo-aws-security-ec2-instance-no-public-ip-all
    filters: asset.platform == "aws"
    mql: aws.ec2.instances.all(publicIp == empty)
  - uid: mondoo-aws-security-ec2-instance-no-public-ip-single
    filters: asset.platform == "aws-ec2-instance"
    mql: aws.ec2.instance.publicIp == empty
  - uid: mondoo-aws-security-ec2-instance-no-public-ip-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_instance" )
    mql: terraform.resources.where( nameLabel == "aws_instance" ).none( arguments.associate_public_ip_address == true )
  - uid: mondoo-aws-security-ec2-instance-no-public-ip-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_instance" )
    mql: terraform.plan.resourceChanges.where( type == "aws_instance" ).none( change.after.associate_public_ip_address == true )
  - uid: mondoo-aws-security-ec2-instance-no-public-ip-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_instance" )
    mql: terraform.state.resources.where( type == "aws_instance" ).none( values.associate_public_ip_address == true )


  - uid: mondoo-aws-security-ec2-imdsv2-check
    title: Ensure EC2 instances use IMDSv2
    impact: 90
    variants:
      - uid: mondoo-aws-security-ec2-imdsv2-check-all
      - uid: mondoo-aws-security-ec2-imdsv2-check-single
      - uid: mondoo-aws-security-ec2-imdsv2-check-terraform-hcl
      - uid: mondoo-aws-security-ec2-imdsv2-check-terraform-plan
      - uid: mondoo-aws-security-ec2-imdsv2-check-terraform-state
    docs:
      desc: |
        This check ensures that Amazon EC2 instances are configured to use Instance Metadata Service Version 2 (IMDSv2) instead of IMDSv1. IMDSv2 provides enhanced security by requiring session-based authentication using a token, reducing the risk of metadata theft and credential exposure.

        **Rationale:**


        IMDSv1 allows unauthenticated HTTP requests to retrieve instance metadata, making it vulnerable to server-side request forgery (SSRF) attacks, credential theft, and container breakout scenarios. IMDSv2 mitigates these risks by:

        -	Requiring session-based authentication with a limited-time token.
        -	Protecting against open WAF bypasses and SSRF exploitation.
        -	Enforcing request headers to prevent unauthorized access.

        AWS security best practices, as well as CIS AWS Foundations Benchmark, PCI DSS, ISO 27001, and NIST, recommend enforcing IMDSv2-only for all EC2 instances.

        **Risk Mitigation:**

        -	Prevents unauthorized metadata access by enforcing authenticated API requests.
        -	Mitigates SSRF vulnerabilities that could expose instance credentials.
        -	Aligns with compliance requirements for secure cloud environments.
      audit: |
        __cnspec shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

          ```bash
          aws.ec2.instances.where( httpTokens != "required" ) { arn instanceId region tags httpTokens }
          ```

          Example output:

          ```mql
          aws.ec2.instances.where: [
            0: {
              instanceId: "i-0070af411a515f14a"
              region: "us-east-1"
              arn: "arn:aws:ec2:us-east-1:1234375555:instance/i-0070af411a515f14a"
              httpTokens: "optional"
              tags: {
                Environment: "windows-development-vpc"
                Name: "win19-dev-workstation-106e1f1c"
                Terraform: "true"
              }
            }
          ]
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EC2 Console.
            2.	Select Instances in the left panel.
            3.	Select an instance and go to the Details tab.
            4.	Under Metadata version, check if it is set to V2 only.
            5.	If IMDSv1 is allowed, update the settings:

              -	Select Actions > Modify instance metadata options.
              -	Set Metadata version to IMDSv2 only.
              -	Set Hop limit to at least 1 (for containerized workloads).
              -	Select Save to apply the changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check the IMDS version for all EC2 instances:
            ```sh
            aws ec2 describe-instances --query "Reservations[*].Instances[*].{Instance:InstanceId, MetadataOptions:MetadataOptions}" --output table
            ```sh

            Update an instance to enforce IMDSv2 only:
            ```sh
            aws ec2 modify-instance-metadata-options --instance-id <instance-id> --http-tokens required --http-put-response-hop-limit 1
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure IMDSv2-only is enforced in Terraform:
            ```hcl
            resource "aws_instance" "secure_instance" {
              ami           = "ami-12345678"
              instance_type = "t3.micro"

              metadata_options {
                http_tokens                 = "required"  # Enforce IMDSv2
                http_put_response_hop_limit = 1
                http_endpoint               = "enabled"
              }

              tags = {
                Name = "SecureInstance"
              }
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Configure IMDSv2-only in a CloudFormation template:
            ```yaml
            Resources:
              SecureInstance:
                Type: "AWS::EC2::Instance"
                Properties:
                  ImageId: "ami-12345678"
                  InstanceType: "t3.micro"
                  MetadataOptions:
                    HttpTokens: "required"
                    HttpPutResponseHopLimit: 1
                    HttpEndpoint: "enabled"
            ```
  - uid: mondoo-aws-security-ec2-imdsv2-check-all
    filters: asset.platform == "aws"
    mql: aws.ec2.instances.where(state != /terminated|shutting-down/ && httpEndpoint == "enabled").all(httpTokens == "required")
  - uid: mondoo-aws-security-ec2-imdsv2-check-single
    filters: |
      asset.platform == "aws-ec2-instance"
      aws.ec2.instance.state != /terminated|shutting-down/
      aws.ec2.instance.httpEndpoint == "enabled"
    mql: aws.ec2.instance.httpTokens == "required"
  - uid: mondoo-aws-security-ec2-imdsv2-check-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_instance")
    mql: terraform.resources.where( nameLabel == "aws_instance" ).all( blocks.where( type == "metadata_options" ) { arguments.http_endpoint == "disabled" || arguments.http_tokens == "required" } )
  - uid: mondoo-aws-security-ec2-imdsv2-check-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_instance")
    mql: terraform.plan.resourceChanges.where( type == "aws_instance" ).all( change.after.metadata_options[0].http_endpoint == "disabled" || change.after.metadata_options[0].http_tokens == "required" )
  - uid: mondoo-aws-security-ec2-imdsv2-check-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_instance")
    mql: terraform.state.resources.where( type == "aws_instance" ).all( values.metadata_options[0].http_endpoint == "disabled" || values.metadata_options[0].http_tokens == "required" )


  - uid: mondoo-aws-security-vpc-flow-logs-enabled
    title: Ensure VPC flow logging is enabled in all VPCs
    impact: 70
    docs:
      desc: |
        This check ensures that Amazon Virtual Private Cloud (VPC) flow logs are enabled for all VPCs to capture network traffic metadata. VPC Flow Logs provide visibility into network traffic patterns, allowing security teams to monitor, detect anomalies, and investigate incidents such as unauthorized access, data exfiltration, and security breaches.

        **Rationale:**

        Without VPC Flow Logs, organizations lack visibility into ingress and egress network traffic within their AWS environment. This can lead to:

        - Difficulty in detecting security incidents, such as unauthorized connections or data leaks.
        -	Lack of compliance with industry standards (e.g., CIS AWS Foundations Benchmark, NIST, PCI DSS, ISO 27001).
        -	Inability to audit network activity, which is crucial for forensic investigations.

        Enabling VPC Flow Logs ensures that network traffic metadata is continuously collected, aiding in threat detection, security analytics, and compliance reporting.

        **Risk Mitigation:**

        -	Enhances network visibility by capturing VPC-level traffic metadata.
        -	Improves threat detection by monitoring suspicious or unauthorized traffic.
        -	Supports compliance requirements for security monitoring and audit logging.
      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect cnspec shell to your AWS environment: `cnspec shell aws`
        3. Run this query:

          ```mql
          aws.vpcs.where(
            flowLogs.length == 0
          ){id arn region state isDefault flowLogs tags}
          ```

          This example output shows only one VPC, but default VPCs in all regions fail this check.

          ```mql
          aws.vpcs.where: [
            0: {
              arn: "arn:aws:vpc:eu-north-1:053121068929:id/vpc-0c3955e3d04d2e09a"
              flowLogs: []
              id: "vpc-0c3955e3d04d2e09a"
              isDefault: true
              region: "eu-north-1"
              state: "available"
              tags: {}
            }
            ...
          ]
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS VPC Console.
            2.	Select Your VPCs from the left panel.
            3.	For each VPC:
              -	Select the VPC name.
              -	Go to the Flow Logs tab.
              -	Select Create Flow Log and configure:
              -	Filter: Choose All Traffic to capture both accepted and rejected packets.
              -	Destination: Select Amazon CloudWatch Logs or Amazon S3.
              -	IAM Role: Ensure an IAM role with appropriate permissions is used.
              -	Log Format: Use default or a custom log format if required.
              -	Select Create to enable flow logging.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if VPC Flow Logs are enabled for all VPCs:
            ```sh
            aws ec2 describe-flow-logs --query "FlowLogs[*].{VPC:ResourceId,LogGroup:LogGroupName}"
            ```

            If a VPC does not have flow logs enabled, create one:
            ```sh
            aws ec2 create-flow-logs \
            --resource-type VPC \
            --resource-ids <vpc-id> \
            --traffic-type ALL \
            --log-destination-type cloud-watch-logs \
            --log-group-name vpc-flow-logs \
            --deliver-logs-permission-arn arn:aws:iam::<account-id>:role/vpc-flow-logs-role
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure that VPC Flow Logs are enabled in Terraform:
            ```hcl
            resource "aws_flow_log" "vpc_flow_log" {
              vpc_id          = aws_vpc.main.id
              traffic_type    = "ALL"
              log_destination = aws_cloudwatch_log_group.vpc_logs.arn
            }

            resource "aws_cloudwatch_log_group" "vpc_logs" {
              name = "vpc-flow-logs"
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Enable VPC Flow Logs in CloudFormation:
            ```yaml
            Resources:
              VPCFlowLog:
                Type: "AWS::EC2::FlowLog"
                Properties:
                  ResourceType: "VPC"
                  ResourceId: !Ref MyVPC
                  TrafficType: "ALL"
                  LogDestinationType: "cloud-watch-logs"
                  LogGroupName: "/aws/vpc/flow-logs"
                  DeliverLogsPermissionArn: !Sub "arn:aws:iam::${AWS::AccountId}:role/vpc-flow-logs-role"
              MyVPC:
                Type: "AWS::EC2::VPC"
                Properties:
                  CidrBlock: "10.0.0.0/16"
            ```
    refs:
      - url: https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-controls-reference.html
        title: AWS Documentation - Security Hub controls reference
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/logs/index.html
        title: AWS Documentation - AWS CLI Command Reference - logs
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/index.html
        title: AWS Documentation - AWS CLI Command Reference - ec2
      - url: https://registry.terraform.io/providers/cloudposse/awsutils/latest/docs
        title: Terraform registry - Cloud Posse AWS Utils Provider
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs
        title: Terraform Documentation - AWS Provider
    variants:
      - uid: mondoo-aws-security-vpc-flow-logs-enabled-account
      - uid: mondoo-aws-security-vpc-flow-logs-enabled-single-vpc
  - uid: mondoo-aws-security-vpc-flow-logs-enabled-account
    filters: asset.platform == "aws"
    mql: |
      aws.vpcs.all(
        flowLogs.any(
          status == "ACTIVE" &&
          destination != empty &&
          destinationType == "cloud-watch-logs" &&
          deliverLogsStatus == "SUCCESS" &&
          trafficType == "REJECT" ||
          trafficType == "ALL"
          )
      )
  - uid: mondoo-aws-security-vpc-flow-logs-enabled-single-vpc
    filters: asset.platform == "aws-vpc"
    mql: |
      aws.vpc.flowLogs.any(
        status == "ACTIVE" &&
        destination != empty &&
        destinationType == "cloud-watch-logs" &&
        deliverLogsStatus == "SUCCESS" &&
        trafficType == "REJECT" ||
        trafficType == "ALL"
        )

  - uid: mondoo-aws-security-dynamodb-table-encrypted-kms
    title: Ensure DynamoDB tables are encrypted with AWS Key Management Service (KMS)
    impact: 30
    variants:
      - uid: mondoo-aws-security-dynamodb-table-encrypted-kms-all
      - uid: mondoo-aws-security-dynamodb-table-encrypted-kms-single
    docs:
      desc: |
        This check ensures that Amazon DynamoDB tables are encrypted using AWS Key Management Service (KMS) to protect sensitive data at rest. DynamoDB encryption at rest ensures that data is automatically encrypted before being written to disk and decrypted when read, preventing unauthorized access to stored information.

        **Rationale:**

        By default, AWS encrypts DynamoDB tables using an AWS-owned key, but customer-managed keys (CMKs) provide additional security benefits such as:

        -	Access control via AWS Identity and Access Management (IAM) policies.
        -	Key rotation for enhanced security and compliance.
        -	Auditability with CloudTrail logging of key usage.

        Encrypting DynamoDB tables using AWS KMS CMKs aligns with security best practices and compliance standards such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, ISO 27001, and NIST.

        **Risk Mitigation:**

        -	Prevents unauthorized access to sensitive data by enforcing encryption at rest.
        -	Enhances security by leveraging customer-managed KMS keys (CMKs) instead of default AWS-managed keys.
        -	Ensures compliance with industry security and regulatory requirements.
      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect to your AWS environment with cnspec shell: `cnspec shell aws`
        3. Run this query:

          ```mql
          aws.dynamodb.tables.where(
            sseDescription.length == 0
          ){*}
          ```

          Example output:

          ```mql
          aws.dynamodb.tables.where: [
            0: {
              tags: {}
              backups: []
              arn: "arn:aws:dynamodb:us-east-1:053121068929:table/GameScoresAutoscale"
              region: "us-east-1"
              continuousBackups: {
                ContinuousBackupsStatus: "ENABLED"
                PointInTimeRecoveryDescription: {
                  EarliestRestorableDateTime: "2022-08-02T18:54:51Z"
                  LatestRestorableDateTime: "2022-08-03T15:38:43.954Z"
                  PointInTimeRecoveryStatus: "ENABLED"
                }
              }
              sseDescription: {}
              name: "GameScoresAutoscale"
              provisionedThroughput: {
                LastDecreaseDateTime: null
                LastIncreaseDateTime: null
                NumberOfDecreasesToday: 0.000000
                ReadCapacityUnits: 1.000000
                WriteCapacityUnits: 1.000000
              }
            }
          ]
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS DynamoDB Console.
            2.	Select Tables from the left panel.
            3.	Select a table name and navigate to the Encryption section under Table Details.
            4.	Verify the KMS key type:
              -	If it is AWS-owned key, change it to a Customer-managed KMS key (CMK).
              -	Note that this requires creating a new table and migrating data, as encryption settings cannot be modified after table creation.
            5.	Create a new table with KMS encryption:
              -	Select Create Table.
              -	Under Encryption at rest, select AWS KMS and choose a Customer-Managed Key (CMK).
              -	Migrate data from the old table and delete the unencrypted table.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if a DynamoDB table is encrypted with AWS KMS CMKs:
            ```sh
            aws dynamodb describe-table --table-name <table-name> --query "Table.SSEDescription"
            ```

            If KMSMasterKeyArn is missing or the SSEType is not "KMS", you need to create a new encrypted table and migrate data:
            ```sh
            aws dynamodb create-table \
            --table-name <new-table-name> \
            --attribute-definitions AttributeName=Id,AttributeType=S \
            --key-schema AttributeName=Id,KeyType=HASH \
            --billing-mode PAY_PER_REQUEST \
            --sse-specification Enabled=true,SSEType=KMS,KMSMasterKeyId="arn:aws:kms:region:account-id:key/key-id"
            ```

            Migrate data from the old table and delete the unencrypted table:
            ```sh
            aws dynamodb delete-table --table-name <old-table-name>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure DynamoDB tables use KMS CMKs for encryption:
            ```hcl
            resource "aws_kms_key" "dynamodb_encryption" {
              description = "KMS key for DynamoDB encryption"
            }

            resource "aws_dynamodb_table" "secure_dynamodb" {
              name           = "secure-table"
              billing_mode   = "PAY_PER_REQUEST"
              hash_key       = "id"

              attribute {
                name = "id"
                type = "S"
              }

              server_side_encryption {
                enabled     = true
                kms_key_arn = aws_kms_key.dynamodb_encryption.arn
              }
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Enable KMS encryption for DynamoDB tables in CloudFormation:
            ```yaml
            Resources:
              SecureDynamoDBTable:
                Type: "AWS::DynamoDB::Table"
                Properties:
                  TableName: "secure-table"
                  BillingMode: "PAY_PER_REQUEST"
                  AttributeDefinitions:
                    - AttributeName: "Id"
                      AttributeType: "S"
                  KeySchema:
                    - AttributeName: "Id"
                      KeyType: "HASH"
                  SSESpecification:
                    SSEEnabled: true
                    SSEType: "KMS"
                    KMSMasterKeyId: !Ref DynamoDBKMSKey

              DynamoDBKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "KMS key for DynamoDB encryption"
                  EnableKeyRotation: true
                  KeyPolicy:
                    Version: "2012-10-17"
                    Statement:
                      - Effect: "Allow"
                        Principal:
                          AWS: "*"
                        Action: "kms:*"
                        Resource: "*"
            ```
    refs:
      - url: https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-controls-reference.html
        title: AWS Documentation - Security Hub controls reference
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dynamodb/index.html
        title: AWS Documentation - AWS CLI Command Reference - DynamoDB
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs
        title: Terraform Documentation - AWS Provider
      - url: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html
        title: AWS Documentation - DynamoDB encryption at rest
  - uid: mondoo-aws-security-dynamodb-table-encrypted-kms-all
    filters: asset.platform == "aws"
    mql: |
      aws.dynamodb.tables.all(sseDescription.SSEType == "KMS")
      aws.dynamodb.tables.all(sseDescription.Status == "ENABLED")
  - uid: mondoo-aws-security-dynamodb-table-encrypted-kms-single
    filters: |
      asset.platform == "aws-dynamodb-table"
    mql: |
      aws.dynamodb.table.sseDescription.SSEType == "KMS"
      aws.dynamodb.table.sseDescription.Status == "ENABLED"

  - uid: mondoo-aws-security-lambda-concurrency-check
    title: Ensure Lambda functions are configured with function-level concurrent execution limits
    impact: 60
    docs:
      desc: |
        This check ensures that AWS Lambda functions are configured with function-level concurrent execution limits to prevent excessive resource consumption and throttle protection at the account level. By default, Lambda functions share the account-level concurrency limit, which can lead to resource exhaustion, affecting other critical workloads if a single function consumes too many concurrent executions.

        **Rationale:**

        Without function-level concurrency limits:

        -	A single function with high invocation rates can consume all available concurrency for an AWS account, causing throttling for other functions.
        -	Unintentional denial-of-service (DoS) scenarios may occur due to unbounded parallel executions.
        -	Compliance frameworks such as CIS AWS Foundations Benchmark, PCI DSS, and ISO 27001 recommend enforcing concurrency controls to mitigate resource exhaustion risks.

        Enforcing function-level concurrency limits ensures fair resource distribution and prevents overuse of compute resources by a single Lambda function.

        **Risk Mitigation:**

        -	Prevents resource starvation by limiting the number of concurrent executions per function.
        -	Improves application stability by preventing accidental or intentional excessive execution.
        -	Enhances security and compliance by enforcing workload isolation.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS Lambda Console.
            2.	Select Functions from the left panel.
            3.	Select a Lambda function and go to the Configuration tab.
            4.	Under Concurrency, select Edit.
            5.	Enable Reserved Concurrency and set an appropriate concurrent execution limit.
            6.	Save the changes.

        - id: cli
          desc: |
            **Using AWS CLI:**

            Check the current concurrency setting for a Lambda function:
            ```sh
            aws lambda get-function-concurrency --function-name <function-name>
            ```

            If no reserved concurrency is set, configure a limit:
            ```sh
            aws lambda put-function-concurrency --function-name <function-name> --reserved-concurrent-executions <limit>
            ```

            To remove a concurrency limit if needed:
            ```sh
            aws lambda delete-function-concurrency --function-name <function-name>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure Lambda functions have a reserved concurrency limit:
            ```yaml
            resource "aws_lambda_function" "secure_lambda" {
              function_name    = "SecureLambda"
              role            = aws_iam_role.lambda_role.arn
              handler         = "index.handler"
              runtime         = "python3.8"
              filename        = "lambda.zip"

              reserved_concurrent_executions = 10  # Set function-level concurrency limit
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Define reserved concurrency settings for a Lambda function in CloudFormation:
            ```yaml
            Resources:
              SecureLambdaFunction:
                Type: "AWS::Lambda::Function"
                Properties:
                  FunctionName: "SecureLambda"
                  Handler: "index.handler"
                  Runtime: "python3.8"
                  Role: !GetAtt LambdaExecutionRole.Arn
                  Code:
                    S3Bucket: "my-lambda-bucket"
                    S3Key: "lambda.zip"

              SecureLambdaConcurrency:
                Type: "AWS::Lambda::Concurrency"
                Properties:
                  FunctionName: !Ref SecureLambdaFunction
                  ReservedConcurrentExecutions: 10
            ```
    variants:
      - uid: mondoo-aws-security-lambda-concurrency-check-single
      - uid: mondoo-aws-security-lambda-concurrency-check-all
  - uid: mondoo-aws-security-lambda-concurrency-check-single
    filters: |
      asset.platform == "aws-lambda-function"
    mql: |
      aws.lambda.function.concurrency > 0
      aws.lambda.function.concurrency <= 100
  - uid: mondoo-aws-security-lambda-concurrency-check-all
    filters: |
      asset.platform == "aws"
    mql: |
      aws.lambda.functions.all(concurrency > 0)
      aws.lambda.functions.all(concurrency <= 100)


  - uid: mondoo-aws-security-rds-instance-public-access-check
    title: Ensure all RDS instances are not publicly accessible
    impact: 100
    variants:
      - uid: mondoo-aws-security-rds-instance-public-access-check-all
      - uid: mondoo-aws-security-rds-instance-public-access-check-single
      - uid: mondoo-aws-security-rds-instance-public-access-check-terraform-hcl
      - uid: mondoo-aws-security-rds-instance-public-access-check-terraform-plan
      - uid: mondoo-aws-security-rds-instance-public-access-check-terraform-state
    docs:
      desc: |
        This check ensures that Amazon Relational Database Service (RDS) instances are not publicly accessible to prevent unauthorized access to sensitive databases. By default, RDS instances can be configured to allow public access, which exposes them to the internet, increasing the risk of security breaches, unauthorized data access, and potential data exfiltration.

        **Rationale:**

        Publicly accessible RDS instances pose significant security risks, including:

        - Unauthorized access if credentials or security configurations are weak.
        - Brute-force attacks on database authentication.
        - Non-compliance with security frameworks such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, and ISO 27001.

        To mitigate these risks, RDS instances should be deployed in private subnets and accessible only through bastion hosts, VPNs, or AWS PrivateLink instead of direct internet exposure.

        **Risk Mitigation:**

        - Prevents unauthorized access by restricting direct exposure to the internet.
        - Enhances database security by enforcing private networking best practices.
        - Ensures compliance with regulatory and cloud security standards.
      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect to your AWS environment with cnspec shell: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.rds.instances.where(publiclyAccessible == true) {arn name region dbInstanceIdentifier tags}
        ```

        Example output:

        ```mql
        aws.rds.instances.where: [
          0: {
            arn: "arn:aws:rds:us-moonbase-2:12345:db:rds-12345-mondoo-demo"
            tags: {
              Environment: "12345-mondoo-demo"
              Name: "12345-mondoo-demo-rds"
              git_file: "terraform/aws/db-app.tf"
              git_repo: "mondoo-demo-environment"
            }
            region: "us-moonbase-2"
            dbInstanceIdentifier: "rds-12345-mondoo-demo"
            name: "db1"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

              1.	Navigate to the AWS RDS Console.
              2.	Select Databases in the left panel.
              3.	Select an RDS instance and go to the Connectivity & security tab.
              4.	Check the Publicly accessible setting.
              5.	If the instance is publicly accessible, modify it:
                - Select Modify.
                - Under Connectivity, set Public access to No.
                - Select Continue, and then Apply immediately or schedule the change during the next maintenance window.
              6.	Ensure the instance is placed in a private subnet with security group rules allowing access only from authorized sources.

        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if an RDS instance is publicly accessible:
            ```sh
            aws rds describe-db-instances --query "DBInstances[*].{DBInstanceIdentifier:DBInstanceIdentifier, PubliclyAccessible:PubliclyAccessible}"
            ```

            If an instance is publicly accessible, modify it:
            ```sh
            aws rds modify-db-instance --db-instance-identifier <db-instance-id> --publicly-accessible false --apply-immediately
            ```

            Ensure that RDS is in a private subnet:
            ```sh
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=<vpc-id>" --query "Subnets[*].MapPublicIpOnLaunch"
            ```

            If `MapPublicIpOnLaunch` is `true`, move the RDS instance to a private subnet.
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure RDS instances are not publicly accessible:

            ```hcl
            resource "aws_db_instance" "secure_rds" {
              identifier              = "secure-rds-instance"
              engine                 = "mysql"
              instance_class         = "db.t3.micro"
              allocated_storage      = 20
              publicly_accessible    = false  # Ensure RDS is private
              vpc_security_group_ids = [aws_security_group.private_sg.id]
              db_subnet_group_name   = aws_db_subnet_group.private_db_subnet.name
            }

            resource "aws_db_subnet_group" "private_db_subnet" {
              name       = "private-db-subnet"
              subnet_ids = [aws_subnet.private1.id, aws_subnet.private2.id]
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure RDS instances are not publicly accessible:
            ```yaml
            Resources:
              SecureRDSInstance:
                Type: "AWS::RDS::DBInstance"
                Properties:
                  DBInstanceIdentifier: "secure-rds-instance"
                  Engine: "mysql"
                  DBInstanceClass: "db.t3.micro"
                  AllocatedStorage: 20
                  PubliclyAccessible: false
                  DBSubnetGroupName: !Ref PrivateDBSubnetGroup

              PrivateDBSubnetGroup:
                Type: "AWS::RDS::DBSubnetGroup"
                Properties:
                  DBSubnetGroupName: "private-db-subnet"
                  SubnetIds:
                    - !Ref PrivateSubnet1
                    - !Ref PrivateSubnet2
            ```
    refs:
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/rds/create-db-instance.html
        title: AWS CLI Command Reference - create-db-instance
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/rds/modify-db-instance.html
        title: AWS CLI Command Reference - modify-db-instance
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_instance#publicly_accessible-1
        title: Terraform Registry - aws_db_instance
      - url: https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-controls-reference.html
        title: AWS Documentation - Security Hub controls reference

  - uid: mondoo-aws-security-rds-instance-public-access-check-all
    filters: |
      asset.platform == "aws"
    mql: |
      aws.rds.instances.all(publiclyAccessible == false)
      aws.rds.instances
        .where(publiclyAccessible != false)
        .none(securityGroups.where(
          vpc.routeTables.where(
            routes.any(GatewayId == /igw-/ && DestinationCidrBlock == "0.0.0.0/0")
          )
        )
      )
  - uid: mondoo-aws-security-rds-instance-public-access-check-single
    filters: |
      asset.platform == "aws-rds-dbinstance"
    mql: |
      aws.rds.dbinstance.publiclyAccessible == false
      aws.rds.dbinstance.securityGroups.none(
        vpc.routeTables.where(
          routes.any(GatewayId == /igw-/ && DestinationCidrBlock == "0.0.0.0/0")
        )
      )
  - uid: mondoo-aws-security-rds-instance-public-access-check-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_db_instance" )
    mql: terraform.resources.where( nameLabel == "aws_db_instance" ).all( arguments.publicly_accessible != true )
  - uid: mondoo-aws-security-rds-instance-public-access-check-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_db_instance" )
    mql: terraform.plan.resourceChanges.where( type == "aws_db_instance" ).all( change.after.publicly_accessible != true )
  - uid: mondoo-aws-security-rds-instance-public-access-check-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_db_instance" )
    mql: terraform.state.resources.where( type == "aws_db_instance" ).all( values.publicly_accessible != true )

  - uid: mondoo-aws-security-redshift-cluster-public-access-check
    title: Ensure Redshift clusters are not publicly accessible
    impact: 95
    variants:
      - uid: mondoo-aws-security-redshift-cluster-public-access-check-all
      - uid: mondoo-aws-security-redshift-cluster-public-access-check-single
      - uid: mondoo-aws-security-redshift-cluster-public-access-check-terraform-hcl
      - uid: mondoo-aws-security-redshift-cluster-public-access-check-terraform-plan
      - uid: mondoo-aws-security-redshift-cluster-public-access-check-terraform-state
    docs:
      desc: |
        This check ensures that Amazon Redshift clusters are not publicly accessible to prevent unauthorized access and potential data breaches. A publicly accessible Redshift cluster can be reached from the internet, exposing it to security threats, including brute-force attacks and data exfiltration.

        **Rationale:**

        Amazon Redshift is used for data warehousing and analytics, often storing sensitive business intelligence and customer data. If a Redshift cluster is publicly accessible:

        -	Anyone on the internet could attempt to access it if misconfigured.
        -	Brute-force attacks could compromise database credentials.
        -	Compliance violations may occur under frameworks such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, ISO 27001, and SOC 2.

        To mitigate these risks, Redshift clusters should be placed in private subnets and accessed securely using VPNs, VPC peering, AWS PrivateLink, or AWS IAM authentication instead of direct internet exposure.

        **Risk Mitigation:**

        - Prevents unauthorized access by restricting internet exposure.
        -	Enhances security by ensuring database traffic remains private.
        -	Ensures compliance with industry security standards.
      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect to your AWS environment with cnspec shell: `cnspec shell aws`
        3. Run this query:

          ```mql
          aws.redshift.clusters.where(
            publiclyAccessible != false
          ){name arn region publiclyAccessible}
          ```

          Example output:

          ```mql
          aws.redshift.clusters.where: [
            0: {
              region: "us-east-1"
              publiclyAccessible: true
              name: "test-redshift-cluster"
              arn: "arn:aws:redshift:us-east-1:053121068929:cluster/test-redshift-cluster"
            }
          ]
          ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS Redshift Console.
            2.	Select Clusters in the left panel.
            3.	Select the cluster and go to the Properties tab.
            4.	Under Network and security, check Publicly accessible.
            5.	If the cluster is publicly accessible, modify the settings:
              - Select Edit publicly accessible setting.
              -	Set Publicly accessible to No.
              -	Select Save changes.
            6.	Ensure that the cluster is placed in a private subnet and that security groups restrict access to trusted sources only.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if a Redshift cluster is publicly accessible:
            ```sh
            aws redshift describe-clusters --query "Clusters[*].{Cluster:ClusterIdentifier, PubliclyAccessible:PubliclyAccessible}"
            ```

            If a cluster is publicly accessible, modify it:
            ```sh
            aws redshift modify-cluster --cluster-identifier <cluster-id> --publicly-accessible false
            ```

            Ensure the cluster is placed in a private subnet:
            ```sh
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=<vpc-id>" --query "Subnets[*].MapPublicIpOnLaunch"
            ```

            If `MapPublicIpOnLaunch` is `true`, move the Redshift cluster to a private subnet.
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure Redshift clusters are not publicly accessible:
            ```hcl
            resource "aws_redshift_cluster" "secure_redshift" {
              cluster_identifier  = "secure-redshift-cluster"
              node_type           = "dc2.large"
              master_username     = "admin"
              master_password     = "securepassword"
              publicly_accessible = false  # Ensure Redshift is private
              cluster_subnet_group_name = aws_redshift_subnet_group.private_redshift_subnet.name
            }

            resource "aws_redshift_subnet_group" "private_redshift_subnet" {
              name       = "private-redshift-subnet"
              subnet_ids = [aws_subnet.private1.id, aws_subnet.private2.id]
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure Redshift clusters are not publicly accessible:
            ```yaml
            Resources:
              SecureRedshiftCluster:
                Type: "AWS::Redshift::Cluster"
                Properties:
                  ClusterIdentifier: "secure-redshift-cluster"
                  NodeType: "dc2.large"
                  MasterUsername: "admin"
                  MasterUserPassword: "securepassword"
                  PubliclyAccessible: false
                  ClusterSubnetGroupName: !Ref PrivateRedshiftSubnetGroup

              PrivateRedshiftSubnetGroup:
                Type: "AWS::Redshift::ClusterSubnetGroup"
                Properties:
                  Description: "Private Redshift Subnet Group"
                  SubnetIds:
                    - !Ref PrivateSubnet1
                    - !Ref PrivateSubnet2
            ```

    refs:
      - url: https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-controls-reference.html
        title: AWS Documentation - Security Hub controls reference
      - url: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/redshift/index.html
        title: AWS Documentation - AWS CLI Command Reference - Redshift
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs
        title: Terraform Documentation - AWS Provider
  - uid: mondoo-aws-security-redshift-cluster-public-access-check-all
    filters: asset.platform == "aws"
    mql: aws.redshift.clusters.all(publiclyAccessible == false)
  - uid: mondoo-aws-security-redshift-cluster-public-access-check-single
    filters: asset.platform == "aws-redshift-cluster"
    mql: aws.redshift.cluster.publiclyAccessible == false
  - uid: mondoo-aws-security-redshift-cluster-public-access-check-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_redshift_cluster" )
    mql: terraform.resources.where( nameLabel == "aws_redshift_cluster" ).all( arguments.publicly_accessible != true )
  - uid: mondoo-aws-security-redshift-cluster-public-access-check-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_redshift_cluster" )
    mql: terraform.plan.resourceChanges.where( type == "aws_redshift_cluster" ).all( change.after.publicly_accessible != true )
  - uid: mondoo-aws-security-redshift-cluster-public-access-check-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_redshift_cluster" )
    mql: terraform.state.resources.where( type == "aws_redshift_cluster" ).all( values.publicly_accessible != true )

  - uid: mondoo-aws-security-ec2-volume-inuse-check
    title: Ensure EBS volumes attached to EC2 instances are configured for deletion on instance termination
    impact: 60
    props:
      - uid: mondooAWSSecurityEbsVolumeDeleteOnTermination
        title: Defines whether instances should be configured to delete volumes on termination
        mql: "true"
    variants:
      - uid: mondoo-aws-security-ec2-volume-inuse-check-all
      - uid: mondoo-aws-security-ec2-volume-inuse-check-single
    docs:
      desc: |
        This check ensures that Amazon Elastic Block Store (EBS) volumes attached to EC2 instances are configured to be automatically deleted when the instance is terminated. When an EBS volume is created and attached to an instance, the `DeleteOnTermination` attribute determines whether the volume persists after instance termination.

        **Rationale**

        By default, EBS volumes persist even after the associated EC2 instance is terminated, which can lead to orphaned volumes accumulating over time. These orphaned volumes not only increase storage costs but can also pose a security risk if they contain sensitive data and are left unmonitored.

        Setting the `DeleteOnTermination` flag ensures that unused EBS volumes do not persist beyond the lifecycle of the EC2 instance, reducing the risk of unauthorized access, data leaks, and unnecessary storage costs. This is particularly important in dynamic cloud environments where instances are frequently created and terminated.

        **Risk Mitigation**

        -	**Cost Optimization:** Prevents unnecessary storage costs by ensuring that unused volumes do not persist.
        -	**Security and Compliance:** Reduces the risk of orphaned volumes containing sensitive data.
        -	**Operational Efficiency:** Prevents clutter and improves resource management in AWS environments.

      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect to your AWS environment with cnspec shell: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.ec2.volumes.where( attachments.length == 0 ) {*}
        ```

        Example output:

        ```mql
        mondoo> aws.ec2.volumes.where( attachments.length == 0 ) {*}
        aws.ec2.volumes.where: [
          0: {
            volumeType: "gp2"
            attachments: []
            availabilityZone: "us-west-2a"
            encrypted: false
            id: "vol-0f5661d9f9db6dd3a"
            arn: "arn:aws:ec2:us-west-2:187043755555:volume/vol-0f5661d9f9db6dd3a"
            state: "available"
            tags: {
              Name: "Unattached Test"
            }
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EC2 Console.
            2.	Select Instances in the left panel.
            3.	Select an instance and go to the Storage tab.
            4.	Check if the Delete on Termination flag is enabled for all EBS volumes.
            5.	If not enabled:
              - Select Modify Instance (for root volumes) or Detach Volume and reattach with Delete on Termination enabled.
            6.	Save the changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if an EBS volume is set to delete on termination:
            ```sh
            aws ec2 describe-instances --query "Reservations[].Instances[].BlockDeviceMappings[?Ebs.DeleteOnTermination==`false`]" --output table
            ```

            Enable delete on termination for a specific volume:
            ```sh
            aws ec2 modify-instance-attribute --instance-id <instance-id> --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"DeleteOnTermination\":true}}]"
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure EBS volumes delete on termination:
            ```hcl
            resource "aws_instance" "secure_instance" {
              ami           = "ami-12345678"
              instance_type = "t3.micro"

              root_block_device {
                delete_on_termination = true
              }

              ebs_block_device {
                device_name           = "/dev/sdf"
                volume_size           = 20
                delete_on_termination = true
              }
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure EBS volumes are deleted on termination:
            ```yaml
            Resources:
              SecureInstance:
                Type: "AWS::EC2::Instance"
                Properties:
                  ImageId: "ami-12345678"
                  InstanceType: "t3.micro"
                  BlockDeviceMappings:
                    - DeviceName: "/dev/sda1"
                      Ebs:
                        VolumeSize: 20
                        DeleteOnTermination: true
                    - DeviceName: "/dev/sdf"
                      Ebs:
                        VolumeSize: 50
                        DeleteOnTermination: true
            ```
    refs:
      - url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-volume.html
        title: AWS Documentation - Delete an Amazon EBS volume
  - uid: mondoo-aws-security-ec2-volume-inuse-check-all
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.volumes.where(attachments != empty).all(attachments.any(DeleteOnTermination == props.mondooAWSSecurityEbsVolumeDeleteOnTermination))
  - uid: mondoo-aws-security-ec2-volume-inuse-check-single
    filters: asset.platform == "aws-ec2-volume" && aws.ec2.volume.attachments != empty
    mql: |
      aws.ec2.volume.attachments.any(DeleteOnTermination == true)

  - uid: mondoo-aws-security-ebs-snapshot-public-restorable-check
    title: Ensure EBS snapshots are not publicly restorable
    impact: 80
    docs:
      desc: |
        This check ensures that Amazon Elastic Block Store (EBS) snapshots are not configured to be publicly restorable. A public snapshot can be accessed by anyone, potentially exposing sensitive data. AWS provides fine-grained access controls for EBS snapshots, and by default, snapshots are private unless explicitly shared.

        **Rationale:**

        Publicly accessible EBS snapshots pose a significant security risk, as they can contain sensitive data such as personally identifiable information (PII), credentials, or proprietary application data. If a snapshot is mistakenly made public, it can be accessed by unauthorized parties, leading to data breaches, regulatory compliance violations, and reputational damage.

        Ensuring that snapshots remain private unless intentionally shared with specific AWS accounts mitigates the risk of data exposure while allowing controlled data sharing when necessary.

        **Risk Mitigation:**

        - **Data Security:** Prevents unauthorized access to potentially sensitive EBS snapshot data.
        - **Compliance:** Helps maintain compliance with security frameworks such as GDPR, HIPAA, and SOC 2.
        -	**Operational Control:** Ensures that data is shared only with intended AWS accounts or users.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EC2 Console.
            2.	Select Snapshots in the left panel.
            3.	Check the Permissions column for any snapshots listed as Public.
            4.	If a snapshot is public, update its permissions:
              -	Select the snapshot.
              -	Go to Modify Permissions under the Actions menu.
              -	Ensure that Public access is not selected.
              -	If needed, specify the AWS account IDs that require access.
              -	Select Save Changes.
        - id: cli
          desc: |
            Using AWS CLI:

            Check if any snapshots are publicly accessible:
            ```sh
            aws ec2 describe-snapshots --owner-id <account-id> --query "Snapshots[?Public].SnapshotId"
            ```

            Modify a snapshot to remove public access:
            ```sh
            aws ec2 modify-snapshot-attribute --snapshot-id <snapshot-id> --attribute createVolumePermission --operation-type remove --group-names all
            ```
        - id: terraform
          desc: |
            Using Terraform:

            Ensure EBS snapshots are private:
            ```hcl
            resource "aws_ebs_snapshot" "secure_snapshot" {
              volume_id = aws_ebs_volume.example.id

              tags = {
                Name = "secure-snapshot"
              }
            }

            resource "aws_ebs_snapshot_public_access_block" "secure_snapshot" {
              snapshot_id = aws_ebs_snapshot.secure_snapshot.id
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure that EBS snapshots are private in CloudFormation:
            ```yaml
            Resources:
              SecureEBSSnapshot:
                Type: "AWS::EC2::Snapshot"
                Properties:
                  VolumeId: !Ref SecureEBSVolume
                  Tags:
                    - Key: "Name"
                      Value: "Secure Snapshot"
              SecureEBSVolume:
                Type: "AWS::EC2::Volume"
                Properties:
                  Size: 20
                  AvailabilityZone: "us-east-1a"
            ```
    variants:
      - uid: mondoo-aws-security-ebs-snapshot-public-restorable-check-all
      - uid: mondoo-aws-security-ebs-snapshot-public-restorable-check-single
  - uid: mondoo-aws-security-ebs-snapshot-public-restorable-check-all
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.snapshots.all(createVolumePermission.none(Group == "all"))
  - uid: mondoo-aws-security-ebs-snapshot-public-restorable-check-single
    filters: asset.platform == "aws-ec2-snapshot" || asset.platform == "aws-ebs-snapshot"
    mql: |
      aws.ec2.snapshot.createVolumePermission.none(Group == "all")

  - uid: mondoo-aws-security-efs-encrypted-check
    title: Ensure EFS is configured to encrypt file data using KMS
    impact: 75
    variants:
      - uid: mondoo-aws-security-efs-encrypted-check-all
      - uid: mondoo-aws-security-efs-encrypted-check-single
    docs:
      desc: |
        This check ensures that Amazon Elastic File System (EFS) is configured to encrypt file data at rest using AWS Key Management Service (KMS). Encryption helps protect sensitive data from unauthorized access by ensuring that files are stored securely. AWS KMS provides centralized key management and integrates with EFS to automatically encrypt and decrypt data transparently.

        **Rationale:**

        By default, EFS does not enable encryption unless explicitly configured. Without encryption, data stored in EFS volumes remains in plaintext, making it vulnerable to unauthorized access if an attacker gains access to the storage system.
        AWS KMS encryption ensures that file data is protected at rest, preventing unauthorized users or malicious actors from accessing sensitive information. This is particularly important for compliance with security frameworks such as GDPR, HIPAA, PCI DSS, and SOC 2.

        **Risk Mitigation:**

        - **Data Security:** Protects data at rest from unauthorized access.
        -	**Regulatory Compliance:** Helps meet compliance requirements for data protection and privacy.
        -	**Centralized Key Management:** Uses AWS KMS for key lifecycle management, auditing, and fine-grained access control.
      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect to your AWS environment with cnspec shell: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.efs.filesystems.where( encrypted == false ) {*}
        ```

        Example output:

        ```mql
        aws.efs.filesystems.where: [
          0: {
            tags: {
              Name: "12344375555-mondoo-demo-efs"
              git_file: "terraform/aws/efs.tf"
              git_org: "mondoolabs"
              git_repo: "mondoo-demo-environment"
            }
            id: "fs-0a73947e541509f0e"
            region: "us-west-2"
            name: "12344375555-mondoo-demo-efs"
            kmsKey: null
            encrypted: false
            arn: "arn:aws:elasticfilesystem:us-west-2:12344375555:file-system/fs-0a73947e541509f0e"
          }
        ]
        ```
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EFS Console.
            2.	Select File Systems from the left panel.
            3.	Identify file systems that do not have encryption enabled.
            4.	If an EFS file system is not encrypted, encryption cannot be enabled on an existing file system. You must:
              -	Create a new EFS file system with encryption enabled.
              -	Migrate data from the old unencrypted file system to the new encrypted one.
            5.	While creating a new file system:
              -	Under General settings, check Enable encryption.
              -	Select AWS KMS as the encryption provider.
              -	Choose a customer-managed KMS key (CMK) if required.
            6.	Once data migration is complete, delete the old unencrypted file system.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if an EFS file system is encrypted:
            ```sh
            aws efs describe-file-systems --query "FileSystems[*].{FileSystemId:FileSystemId, Encrypted:Encrypted}"
            ```

            Create an encrypted EFS file system using a KMS key:
            ```sh
            aws efs create-file-system --creation-token "secure-efs" --encrypted --kms-key-id "arn:aws:kms:region:account-id:key/key-id"
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure EFS encryption is enabled with an AWS KMS key:
            ```hcl
            resource "aws_kms_key" "efs_encryption_key" {
              description             = "KMS key for EFS encryption"
              enable_key_rotation     = true
            }

            resource "aws_efs_file_system" "secure_efs" {
              encrypted  = true
              kms_key_id = aws_kms_key.efs_encryption_key.arn
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure EFS file systems are encrypted using AWS KMS:
            ```yaml
            Resources:
              SecureEFS:
                Type: "AWS::EFS::FileSystem"
                Properties:
                  Encrypted: true
                  KmsKeyId: !Ref EFSKMSKey

              EFSKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "KMS key for EFS encryption"
                  EnableKeyRotation: true
            ```
    refs:
      - url: https://docs.aws.amazon.com/efs/latest/ug/security-considerations.html
        title: AWS Documentation - Security in Amazon EFS
      - url: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/efs_file_system
        title: Terraform Registry - aws_efs_file_system resource
      - url: https://docs.aws.amazon.com/efs/latest/ug/creating-using-create-fs.html#creating-using-fs-part1-cli
        title: AWS Documentation - Creating a file system using the AWS CLI
  - uid: mondoo-aws-security-efs-encrypted-check-all
    filters: |
      asset.platform == "aws"
    mql: |
      aws.efs.filesystems.all(encrypted == true)
      aws.efs.filesystems.all(kmsKey != empty)
  - uid: mondoo-aws-security-efs-encrypted-check-single
    filters: |
      asset.platform == "aws-efs-filesystem"
    mql: |
      aws.efs.filesystem.encrypted == true
      aws.efs.filesystem.kmsKey != empty

  - uid: mondoo-aws-security-cloudwatch-log-group-encrypted
    title: Ensure CloudWatch logs are encrypted at rest using KMS CMKs
    filters: asset.platform == "aws"
    impact: 70
    docs:
      desc: |
        This check ensures that Amazon CloudWatch Log Groups are configured to encrypt log data at rest using AWS Key Management Service (KMS) Customer-Managed Keys (CMKs). CloudWatch Logs store critical operational and security data, and encrypting them using CMKs enhances security by providing better control over key management and access policies.

        **Rationale:**

        CloudWatch Logs often store critical system logs, security logs, and application logs, which may contain sensitive data. Without KMS CMK encryption:

        -	Logs are encrypted using default AWS-managed keys, which lack fine-grained access control.
        -	Compliance requirements under CIS AWS Foundations Benchmark, PCI DSS, HIPAA, and ISO 27001 may not be met.
        -	Security risks increase, as logs could be exposed to unauthorized access.

        Using customer-managed KMS CMKs allows organizations to control key permissions, enable key rotation, and track encryption events via AWS CloudTrail.

        **Risk Mitigation:**

        -	**Data Security:** Ensures sensitive log data is encrypted at rest.
        -	**Regulatory Compliance:** Helps meet security and compliance standards (e.g., GDPR, HIPAA, PCI DSS, SOC 2).
        -	**Access Control:** Provides better control over encryption keys, including key rotation and access permissions.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS CloudWatch Console.
            2.	Select Log Groups from the left panel.
            3.	Select a Log Group and go to the Log Group Settings.
            4.	Under Data Protection, check if KMS encryption is enabled.
            5.	If KMS is not configured:
              -	Select Edit and enable Use a KMS key to encrypt log data.
              -	Choose a Customer-Managed KMS Key (CMK) instead of the AWS-managed key.
              -	Select Save changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if a CloudWatch Log Group is encrypted with a KMS CMK:
            ```sh
            aws logs describe-log-groups --query "logGroups[*].{LogGroupName:logGroupName, KmsKeyId:kmsKeyId}"
            ```

            If a log group does not have a KMS key, assign one:
            ```sh
            aws logs associate-kms-key --log-group-name <log-group-name> --kms-key-id "arn:aws:kms:region:account-id:key/key-id"
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure CloudWatch Logs use KMS encryption:
            ```hcl
            resource "aws_kms_key" "cloudwatch_kms_key" {
              description         = "KMS key for CloudWatch Log encryption"
              enable_key_rotation = true
            }

            resource "aws_cloudwatch_log_group" "secure_log_group" {
              name       = "secure-log-group"
              kms_key_id = aws_kms_key.cloudwatch_kms_key.arn
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure CloudWatch Logs are encrypted using AWS KMS:
            ```yaml
            Resources:
              SecureCloudWatchLogs:
                Type: "AWS::Logs::LogGroup"
                Properties:
                  LogGroupName: "secure-log-group"
                  KmsKeyId: !Ref CloudWatchKMSKey

              CloudWatchKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "KMS key for CloudWatch Log encryption"
                  EnableKeyRotation: true
            ```
    mql: aws.cloudwatch.logGroups.all(kmsKey != empty)

  - uid: mondoo-aws-security-elb-deletion-protection-enabled
    title: Ensure Application Load Balancers are Configured with deletion protection enabled
    impact: 70
    filters: asset.platform == "aws"
    docs:
      desc: |
        This check ensures that AWS Application Load Balancers (ALBs) have deletion protection enabled to prevent accidental or unauthorized deletion. Deleting a load balancer without proper authorization or by mistake can result in application downtime, traffic disruption, and loss of critical configurations.

        **Rationale:**

        By default, deletion protection is disabled for Application Load Balancers, making them susceptible to accidental removal. If an ALB is deleted:

        -	All traffic routing is disrupted, causing application downtime.
        -	Reconfiguration is required, leading to potential misconfigurations or longer recovery times.
        -	Security policies and access control settings are lost, which may expose applications to threats.

        Enabling deletion protection ensures that ALBs cannot be deleted without explicitly disabling this setting first, providing an additional layer of protection against human errors and misconfigurations.

        **Risk Mitigation:**

        -	Prevents accidental deletions that could cause service outages.
        -	Ensures operational stability by enforcing change control processes.
        -	Reduces misconfigurations and security risks associated with unintended deletions.
      remediation:
        - id: console
          desc: |
            Using AWS Console:

            1. Navigate to the AWS EC2 Console.
            2. Select Load Balancers under the Load Balancing section.
            3. Select an Application Load Balancer (ALB).
            4. Select the Description tab.
            5. Under Deletion Protection, check if it is enabled.
            6. If it is disabled, modify the setting:
              -	Select Edit attributes.
              -	Enable Deletion protection.
              -	Select Save changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if an ALB has deletion protection enabled:
            ```sh
            aws elbv2 describe-load-balancers --query "LoadBalancers[*].{Name:LoadBalancerName, DeletionProtectionEnabled:Attributes[?Key=='deletion_protection.enabled'].Value | [0]}"
            ```

            Enable deletion protection for a specific ALB:
            ```sh
            aws elbv2 modify-load-balancer-attributes \
            --load-balancer-arn <load-balancer-arn> \
            --attributes Key=deletion_protection.enabled,Value=true
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure ALBs have deletion protection enabled:
            ```hcl
            resource "aws_lb" "secure_alb" {
              name               = "secure-application-lb"
              internal           = false
              load_balancer_type = "application"
              security_groups    = [aws_security_group.alb_sg.id]
              subnets            = [aws_subnet.public1.id, aws_subnet.public2.id]

              enable_deletion_protection = true  # Ensures deletion protection is enabled
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Enable deletion protection for ALBs:
            ```yaml
            Resources:
              SecureApplicationLoadBalancer:
                Type: "AWS::ElasticLoadBalancingV2::LoadBalancer"
                Properties:
                  Name: "secure-application-lb"
                  Type: "application"
                  Scheme: "internet-facing"
                  Subnets:
                    - !Ref PublicSubnet1
                    - !Ref PublicSubnet2
                  SecurityGroups:
                    - !Ref ALBSecurityGroup
                  LoadBalancerAttributes:
                    - Key: "deletion_protection.enabled"
                      Value: "true"
            ```
    mql: |
      aws.elb.loadBalancers.all(attributes.any(Key == "deletion_protection.enabled"))
      aws.elb.loadBalancers.all(attributes.where(Key == "deletion_protection.enabled").all(Value == true))
  - uid: mondoo-aws-security-elasticsearch-encrypted-at-rest
    title: Ensure Amazon OpenSearch Service Domains are Configured with Encryption-at-Rest
    impact: 70
    filters: asset.platform == "aws"
    docs:
      desc: |
        This check ensures that Amazon OpenSearch Service domains are configured to encrypt data at rest using AWS Key Management Service (KMS). Encryption at rest protects sensitive search and analytics data from unauthorized access, ensuring that stored data is automatically encrypted before being written to disk.

        **Rationale:**

        By default, OpenSearch Service does not encrypt data at rest unless explicitly enabled. Without encryption:

        -	Sensitive data remains in plaintext, making it vulnerable to unauthorized access.
        -	Security and compliance risks increase under regulations such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, and ISO 27001.
        -	Data exposure risks exist if an OpenSearch domain is compromised.

        To mitigate these risks, OpenSearch domains should be encrypted using AWS KMS, which provides centralized key management, access control, and audit logging.

        **Risk Mitigation:**

        -	Prevents unauthorized access by ensuring all stored data is encrypted.
        -	Enhances compliance with industry security frameworks requiring encryption at rest.
        -	Improves security posture by integrating AWS KMS for key management.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS OpenSearch Service Console.
            2.	Select Domains in the left panel.
            3.	Select an OpenSearch domain and go to the Security tab.
            4.	Under Encryption at rest, check if encryption is enabled.
            5.	If encryption is not enabled, create a new OpenSearch domain with encryption:
              - Select Create domain.
              - Under Encryption at rest, enable encrypt data at rest.
              -	Choose a customer-managed KMS key (CMK) if required.
              -	Complete the domain creation process.
            6.	Migrate data from the unencrypted domain to the new encrypted domain, then delete the unencrypted domain.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if an OpenSearch domain is encrypted:
            ```sh
            aws opensearch describe-domain --domain-name <domain-name> --query "DomainStatus.EncryptionAtRestOptions"
            ```

            If encryption is not enabled, create a new encrypted OpenSearch domain:
            ```sh
            aws opensearch create-domain \
            --domain-name "secure-opensearch-domain" \
            --encryption-at-rest-options Enabled=true,KmsKeyId="arn:aws:kms:region:account-id:key/key-id"
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure OpenSearch Service encryption is enabled using AWS KMS:
            ```hcl
            resource "aws_kms_key" "opensearch_kms_key" {
              description         = "KMS key for OpenSearch encryption"
              enable_key_rotation = true
            }

            resource "aws_opensearch_domain" "secure_opensearch" {
              domain_name = "secure-opensearch-domain"

              encryption_at_rest {
                enabled    = true
                kms_key_id = aws_kms_key.opensearch_kms_key.arn
              }
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Enable encryption at rest for OpenSearch domains:
            ```yaml
            Resources:
              SecureOpenSearchDomain:
                Type: "AWS::OpenSearchService::Domain"
                Properties:
                  DomainName: "secure-opensearch-domain"
                  EncryptionAtRestOptions:
                    Enabled: true
                    KmsKeyId: !Ref OpenSearchKMSKey

              OpenSearchKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "KMS key for OpenSearch encryption"
                  EnableKeyRotation: true
            ```
    mql: aws.es.domains.all(encryptionAtRestEnabled == true)
  - uid: mondoo-aws-security-rotation-customer-created-cmks-enabled
    title: Ensure rotation for customer-managed keys (CMKs) is enabled
    impact: 80
    filters: asset.platform == "aws"
    docs:
      desc: |
        This check ensures that AWS Key Management Service (KMS) customer-managed keys (CMKs) have automatic key rotation enabled. Enabling key rotation ensures that cryptographic keys are periodically refreshed, reducing the risk of long-term key compromise and enhancing overall security.

        **Rationale:**

        By default, AWS does not enable automatic rotation for CMKs, leaving encryption keys unchanged unless manually rotated. Without key rotation:

        -	Keys remain in use indefinitely, increasing the impact of potential key compromise.
        -	Compliance issues may arise under security frameworks such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, ISO 27001, and NIST.
        -	Security best practices require periodic key changes to limit exposure.

        AWS allows automatic key rotation for symmetric CMKs every 365 days, ensuring that keys remain secure over time.

        **Risk Mitigation:**

        -	Reduces key compromise risk by ensuring keys are periodically refreshed.
        -	Ensures compliance with security frameworks that mandate key rotation.
        -	Improves cryptographic security by limiting key lifespan.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS KMS Console.
            2.	Select Customer managed keys in the left panel.
            3.	Select a CMK to review its settings.
            4.	Under Key rotation, check if Automatic rotation is enabled.
            5.	If rotation is disabled, select Edit key settings:
              -	Enable Automatic key rotation.
              -	Select Save changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if key rotation is enabled for all customer-managed CMKs:
            ```sh
            aws kms list-keys --query "Keys[*].KeyId" | while read -r key_id; do
                aws kms get-key-rotation-status --key-id "$key_id" --query "KeyRotationEnabled"
            done
            ```

            If rotation is not enabled, activate it:
            ```sh
            aws kms enable-key-rotation --key-id <key-id>
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure KMS key rotation is enabled:
            ```hcl
            resource "aws_kms_key" "secure_kms_key" {
              description         = "Customer-managed KMS key with automatic rotation"
              enable_key_rotation = true
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Enable KMS key rotation for CMKs:
            ```yaml
            Resources:
              SecureKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "Customer-managed KMS key with automatic rotation"
                  EnableKeyRotation: true
            ```
    mql: |
      aws.kms.keys
        .where(metadata.KeyState == "Enabled")
        .where(metadata.KeySpec == "SYMMETRIC_DEFAULT")
        .all(keyRotationEnabled == true)
  - uid: mondoo-aws-security-sagemaker-notebook-instance-kms-key-configured
    title: Ensure SageMaker notebook instances are configured to use KMS
    impact: 50
    filters: asset.platform == "aws"
    mql: |
      aws.sagemaker.notebookInstances.all(details.kmsKey != empty)
    docs:
      desc: |
        This check ensures that Amazon SageMaker notebook instances are configured to encrypt data at rest using AWS Key Management Service (KMS). Enabling KMS encryption enhances security by protecting sensitive machine learning (ML) data stored in SageMaker notebook instances from unauthorized access.

        **Rationale:**

        By default, SageMaker notebook instances may store training datasets, model artifacts, and proprietary code in Amazon EBS volumes. Without KMS encryption:

        -	Data is stored in plaintext, making it vulnerable to unauthorized access.
        -	Security and compliance risks increase, especially for organizations under PCI DSS, HIPAA, ISO 27001, and CIS AWS Foundations Benchmark.
        -	No centralized control over encryption keys, preventing fine-grained access control and auditing.

        To mitigate these risks, SageMaker notebook instances should be encrypted using AWS KMS, allowing secure key management, key rotation, and logging of key usage.

        **Risk Mitigation:**

        -	Prevents unauthorized access to sensitive machine learning data.
        -	Ensures compliance with security frameworks that mandate encryption.
        -	Improves security posture by leveraging AWS KMS for centralized key management.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS SageMaker Console.
            2.	Select Notebook instances in the left panel.
            3.	Select a notebook instance and go to the Storage volume settings.
            4.	Check if the Encryption key is set to an AWS KMS key.
            5.	If no KMS key is assigned, update the instance by creating a new SageMaker notebook instance with encryption enabled:
              -	Select Create Notebook Instance.
              -	Under Encryption key, select a customer-managed KMS key (CMK).
              -	Launch the new instance and migrate existing data.
            6.	Delete the unencrypted notebook instance once migration is complete.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check if SageMaker notebook instances are encrypted with KMS:
            ```sh
            aws sagemaker list-notebook-instances --query "NotebookInstances[*].{Name:NotebookInstanceName, KmsKeyId:KmsKeyId}"
            ```

            If an instance does not have a KMS key, create a new encrypted notebook instance:
            ```sh
            aws sagemaker create-notebook-instance \
            --notebook-instance-name "secure-sagemaker-instance" \
            --instance-type "ml.t3.medium" \
            --role-arn "arn:aws:iam::<account-id>:role/SageMakerRole" \
            --kms-key-id "arn:aws:kms:region:account-id:key/key-id"
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure SageMaker notebook instances are encrypted using a KMS key:
            ```hcl
            resource "aws_kms_key" "sagemaker_kms_key" {
              description         = "KMS key for SageMaker notebook encryption"
              enable_key_rotation = true
            }

            resource "aws_sagemaker_notebook_instance" "secure_notebook" {
              name               = "secure-sagemaker-instance"
              instance_type      = "ml.t3.medium"
              role_arn          = aws_iam_role.sagemaker_role.arn
              kms_key_id        = aws_kms_key.sagemaker_kms_key.arn
            }
            ```
        - id: cloudformation
          desc: |
            ```yaml
            Resources:
              SecureSageMakerNotebook:
                Type: "AWS::SageMaker::NotebookInstance"
                Properties:
                  NotebookInstanceName: "secure-sagemaker-instance"
                  InstanceType: "ml.t3.medium"
                  RoleArn: !GetAtt SageMakerRole.Arn
                  KmsKeyId: !Ref SageMakerKMSKey

              SageMakerKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "KMS key for SageMaker encryption"
                  EnableKeyRotation: true
            ```
  - uid: mondoo-aws-security-cloud-trail-encryption-enabled
    title: Ensure CloudTrail trails are configured to use the server-side encryption KMS
    impact: 70
    filters: asset.platform == "aws"
    mql: |
      aws.cloudtrail.trails.all(kmsKey != empty)
    docs:
      desc: |
        This check ensures that AWS CloudTrail trails are configured to use AWS Key Management Service (KMS) for server-side encryption (SSE). Using KMS encryption protects log data from unauthorized access and enhances the security of audit logs.

        **Rationale:**

        By default, CloudTrail logs are not encrypted using a customer-managed KMS key (CMK) unless explicitly configured. Without KMS encryption:

        -	CloudTrail logs may be accessed or modified if security policies are misconfigured.
        -	Sensitive log data is stored in plaintext, increasing security risks.
        -	Compliance requirements under CIS AWS Foundations Benchmark, PCI DSS, HIPAA, ISO 27001, and NIST may not be met.

        Using KMS CMKs provides fine-grained access control, automatic key rotation, and detailed audit logging via AWS CloudTrail.

        **Risk Mitigation:**

        -	Prevents unauthorized access to audit logs by enforcing encryption.
        -	Enhances security posture by integrating AWS KMS for key management.
        -	Ensures compliance with industry security frameworks requiring encryption at rest.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS CloudTrail Console.
            2.	Select Trails in the left panel.
            3.	Select a CloudTrail trail and go to the Storage Location section.
            4.	Under Log file SSE-KMS encryption, check if encryption is enabled.
            5.	If KMS encryption is not enabled, modify the trail:
              -	Select Edit.
              -	Under Log file encryption, select Use a custom AWS KMS key.
              -	Choose an existing Customer-Managed KMS Key (CMK) or create a new one.
              -	Select Save changes.
        - id: cli
          desc: |
            Using AWS CLI:

            Check if a CloudTrail trail is using KMS encryption:
            ```sh
            aws cloudtrail describe-trails --query "trailList[*].{TrailName:Name, KmsKeyId:KmsKeyId}"
            ```

            If KMS encryption is not configured, update the trail:
            ```sh
            aws cloudtrail update-trail --name <trail-name> --kms-key-id "arn:aws:kms:region:account-id:key/key-id"
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure CloudTrail is encrypted with KMS:
            ```hcl
            resource "aws_kms_key" "cloudtrail_kms_key" {
              description         = "KMS key for CloudTrail encryption"
              enable_key_rotation = true
            }

            resource "aws_cloudtrail" "secure_trail" {
              name                          = "secure-cloudtrail"
              s3_bucket_name                = aws_s3_bucket.cloudtrail_logs.id
              is_multi_region_trail         = true
              kms_key_id                    = aws_kms_key.cloudtrail_kms_key.arn
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Enable CloudTrail encryption using KMS:
            ```yaml
            Resources:
              SecureCloudTrail:
                Type: "AWS::CloudTrail::Trail"
                Properties:
                  TrailName: "secure-cloudtrail"
                  S3BucketName: !Ref CloudTrailLogsBucket
                  IsMultiRegionTrail: true
                  KMSKeyId: !Ref CloudTrailKMSKey

              CloudTrailKMSKey:
                Type: "AWS::KMS::Key"
                Properties:
                  Description: "KMS key for CloudTrail encryption"
                  EnableKeyRotation: true
            ```
      audit: |
        __cnspec Shell__

        1. Open a terminal.
        2. Connect to your AWS environment with cnspec shell: `cnspec shell aws`
        3. Run this query:

        ```mql
        aws.cloudtrail.trails.all(
          kmsKey != empty
        )
        ```

        Example output:

        ```mql
        [failed] [].all()
          actual:   [
            0: aws.cloudtrail.trail id = arn:aws:cloudtrail:us-east-1:053121068929:trail/s3-data-events
          ]
        ```
  - uid: mondoo-aws-security-secgroup-restricted-ssh
    title: Ensure security groups restrict incoming SSH traffic
    impact: 90
    docs:
      desc: |
        This check ensures that AWS security groups are configured to restrict incoming SSH `(port 22)` traffic. Allowing unrestricted SSH access `(0.0.0.0/0 or ::/0)` poses a significant security risk by exposing instances to unauthorized access attempts, brute-force attacks, and potential exploitation by malicious actors.

        **Rationale:**

        By default, AWS allows full control over inbound and outbound traffic through security groups. However, allowing unrestricted SSH access increases the risk of unauthorized logins, credential theft, and automated attacks. Instead, SSH access should be restricted to specific IP addresses, such as known administrative networks, bastion hosts, or VPN subnets.

        **Restricting SSH access helps mitigate:**

        - **Brute-force attacks:** Reduces exposure to automated SSH login attempts.
        - **Unauthorized access:** Limits access to only trusted networks.
        - **Compliance risks:** Aligns with best practices in security frameworks like PCI DSS, NIST, and CIS benchmarks.

        **Risk Mitigation:**

        -	**Minimize attack surface:** Reduces the number of exposed SSH endpoints.
        -	**Ensure least privilege access:** Restricts access to only authorized IP ranges.
        -	**Improve network security:** Protects EC2 instances from unauthorized access.
    remediation:
      - id: console
        desc: |
          **Using AWS Console:**

          1.	Navigate to the Amazon EC2 Console.
          2.	Select Security Groups from the left navigation panel.
          3.	Identify security groups that have inbound rules allowing port 22 (SSH) from 0.0.0.0/0 (IPv4) or ::/0 (IPv6).
          4.	Edit the security group’s inbound rules and replace 0.0.0.0/0 with a trusted IP range (e.g., your office network or VPN).
          5.	Save the changes.
      - id: cli
        desc: |
          **Using AWS CLI:**

          Check security groups with unrestricted SSH access:
          ```sh
          aws ec2 describe-security-groups --query "SecurityGroups[?IpPermissions[?FromPort==`22` && (IpRanges[].CidrIp=='0.0.0.0/0' || Ipv6Ranges[].CidrIpv6=='::/0')]].GroupId"
          ```

          Revoke unrestricted SSH access:
          ```sh
          aws ec2 revoke-security-group-ingress --group-id <security-group-id> --protocol tcp --port 22 --cidr 0.0.0.0/0
          ```

          Add a more restrictive SSH rule (replace x.x.x.x/x with a trusted IP range):
          ```sh
          aws ec2 authorize-security-group-ingress --group-id <security-group-id> --protocol tcp --port 22 --cidr x.x.x.x/x
          ```

      - id: terraform
        desc: |
          **Using Terraform**

          Ensure SSH access is restricted to a specific IP range:
          ```hcl
          resource "aws_security_group" "secure_sg" {
            name        = "secure-sg"
            description = "Restricts SSH access"

            ingress {
              from_port   = 22
              to_port     = 22
              protocol    = "tcp"
              cidr_blocks = ["203.0.113.0/24"] # Replace with trusted IP range
            }
            egress {
              from_port   = 0
              to_port     = 0
              protocol    = "-1"
              cidr_blocks = ["0.0.0.0/0"]  # Allow outbound traffic
            }
          }
          ```
      - id: cloudformation
        desc: |
          **Using CloudFormation**

          This template ensures that SSH access (port 22) is only allowed from a specific IP range (e.g., a corporate VPN or a bastion host).
          ```yaml
          Resources:
            SecureSecurityGroup:
              Type: "AWS::EC2::SecurityGroup"
              Properties:
                GroupDescription: "Security group with restricted SSH access"
                VpcId: !Ref VPC
                SecurityGroupIngress:
                  - IpProtocol: "tcp"
                    FromPort: 22
                    ToPort: 22
                    CidrIp: "192.168.1.0/24"  # Replace with your trusted IP range
                SecurityGroupEgress:
                  - IpProtocol: "-1"
                    FromPort: -1
                    ToPort: -1
                    CidrIp: "0.0.0.0/0"  # Allow outbound traffic
                Tags:
                  - Key: "Name"
                    Value: "RestrictedSSHSecurityGroup"

            VPC:
              Type: "AWS::EC2::VPC"
              Properties:
                CidrBlock: "10.0.0.0/16"
                EnableDnsSupport: true
                EnableDnsHostnames: true
                Tags:
                  - Key: "Name"
                    Value: "SecureVPC"
          ```
    variants:
      - uid: mondoo-aws-security-secgroup-restricted-ssh-single
      - uid: mondoo-aws-security-secgroup-restricted-ssh-all
      - uid: mondoo-aws-security-secgroup-restricted-ssh-terraform-hcl
      - uid: mondoo-aws-security-secgroup-restricted-ssh-terraform-plan
      - uid: mondoo-aws-security-secgroup-restricted-ssh-terraform-state
  - uid: mondoo-aws-security-secgroup-restricted-ssh-all
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.securityGroups.where(ipPermissions.any(
        ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0"))).all(
          ipPermissions.none(fromPort <= 22 && toPort >= 22 && toPort != 0)
      )
  - uid: mondoo-aws-security-secgroup-restricted-ssh-single
    filters: asset.platform == "aws-security-group"
    mql: |
      aws.ec2.securitygroup.ipPermissions.where(
        fromPort <= 22 && toPort >= 22 && toPort != 0).none(
          ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0")
          )
  - uid: mondoo-aws-security-secgroup-restricted-ssh-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_security_group")
    mql: terraform.resources.where( nameLabel == "aws_security_group" && blocks.where( type == "ingress").contains( arguments.to_port == 22 )).none( blocks.where( type == "ingress" ) { arguments.cidr_blocks.contains("0.0.0.0/0") || arguments.cidr_blocks.contains("::/0") })
  - uid: mondoo-aws-security-secgroup-restricted-ssh-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_security_group")
    mql: terraform.plan.resourceChanges.where( type == "aws_security_group" && change.after.ingress.contains( to_port == 22 )).none( change.after.ingress[0].cidr_blocks.contains("0.0.0.0/0") || change.after.ingress[0].cidr_blocks.contains("::/0"))
  - uid: mondoo-aws-security-secgroup-restricted-ssh-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_security_group")
    mql: terraform.state.resources.where( type == "aws_security_group" && values.ingress.contains( to_port == 22 ) ).none( values.ingress[0].cidr_blocks.contains("0.0.0.0/0") || values.ingress[0].cidr_blocks.contains("::/0"))

  - uid: mondoo-aws-security-secgroup-restricted-vnc
    title: Ensure security groups restrict incoming VNC traffic
    impact: 90
    docs:
      desc: |
        This check ensures that AWS security groups do not allow unrestricted incoming Virtual Network Computing (VNC) traffic, which operates on ports 5900-5903. Allowing unrestricted VNC access (0.0.0.0/0 or ::/0) exposes instances to unauthorized access attempts, brute-force attacks, and remote exploitation.

        **Rationale:**

        VNC is a widely used remote desktop protocol that, if exposed to the internet, can be exploited by attackers for unauthorized access, credential theft, or lateral movement within the network. Attackers frequently scan for open VNC ports, and leaving them unrestricted significantly increases the risk of compromise.

        To mitigate risks, VNC access should be restricted to trusted IP addresses, such as an internal corporate network, a VPN, or a bastion host.

        **Risks of unrestricted VNC access:**

        -	**Brute-force attacks:** Attackers can repeatedly attempt to guess VNC passwords.
        -	**Unencrypted connections:** Many VNC implementations do not encrypt traffic by default.
        -	**Unauthorized remote access:** Unrestricted access allows attackers to control the remote system.

        **Risk Mitigation:**

        -	**Limit exposure:** Restrict VNC access to known IP addresses (e.g., office VPN or bastion host).
        -	**Use encrypted alternatives:** Consider replacing VNC with more secure alternatives like SSH tunneling or AWS Session Manager.
        -	**Enhance authentication:** Use strong passwords, multi-factor authentication (MFA), and network segmentation.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the Amazon EC2 Console.
            2.	Select Security Groups from the left navigation panel.
            3.	Identify security groups that have inbound rules allowing traffic on ports 5900-5903 from 0.0.0.0/0 (IPv4) or ::/0 (IPv6).
            4.	Edit the security group’s inbound rules and remove any rule that allows unrestricted VNC access.
            5.	If necessary, replace 0.0.0.0/0 with a trusted IP range (e.g., your office VPN or bastion host).
            6.	Save the changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check security groups with unrestricted VNC access:
            ```sh
            aws ec2 describe-security-groups --query "SecurityGroups[?IpPermissions[?((FromPort==`5900` || FromPort==`5901` || FromPort==`5902` || FromPort==`5903`) && (IpRanges[].CidrIp=='0.0.0.0/0' || Ipv6Ranges[].CidrIpv6=='::/0'))]].GroupId"
            ```

            Revoke unrestricted VNC access:
            ```sh
            aws ec2 revoke-security-group-ingress --group-id <security-group-id> --protocol tcp --port 5900-5903 --cidr 0.0.0.0/0
            ```

            Allow VNC access only from a trusted IP range (replace x.x.x.x/x with your specific IP range):
            ```sh
            aws ec2 authorize-security-group-ingress --group-id <security-group-id> --protocol tcp --port 5900-5903 --cidr x.x.x.x/x
            ```

        - id: terraform
          desc: |
            **Ensure security groups restrict VNC access to a specific IP range:**

            ```hcl
            resource "aws_security_group" "restricted_vnc" {
              name        = "restricted-vnc"
              description = "Restricts VNC access"

              ingress {
                from_port   = 5900
                to_port     = 5903
                protocol    = "tcp"
                cidr_blocks = ["203.0.113.0/24"] # Replace with trusted IP range
              }

                egress {
                  from_port   = 0
                  to_port     = 0
                  protocol    = "-1"
                  cidr_blocks = ["0.0.0.0/0"]  # Allow outbound traffic
                }
            }
            ```
    variants:
      - uid: mondoo-aws-security-secgroup-restricted-vnc-single
      - uid: mondoo-aws-security-secgroup-restricted-vnc-all
      - uid: mondoo-aws-security-secgroup-restricted-vnc-terraform-hcl
      - uid: mondoo-aws-security-secgroup-restricted-vnc-terraform-plan
      - uid: mondoo-aws-security-secgroup-restricted-vnc-terraform-state
  - uid: mondoo-aws-security-secgroup-restricted-vnc-all
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.securityGroups.where(ipPermissions.any(
        ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0"))).all(
          ipPermissions.none(fromPort <= 5900 && toPort >= 5900 && toPort != 0)
       )
  - uid: mondoo-aws-security-secgroup-restricted-vnc-single
    filters: asset.platform == "aws-security-group"
    mql: |
      aws.ec2.securitygroup.ipPermissions.where(
        fromPort <= 5900 && toPort >= 5900 && toPort != 0).none(
          ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0")
          )
  - uid: mondoo-aws-security-secgroup-restricted-vnc-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_security_group")
    mql: terraform.resources.where( nameLabel == "aws_security_group" && blocks.where( type == "ingress").contains( arguments.to_port == 5900 )).none( blocks.where( type == "ingress" ) { arguments.cidr_blocks.contains("0.0.0.0/0") || arguments.cidr_blocks.contains("::/0") })
  - uid: mondoo-aws-security-secgroup-restricted-vnc-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_security_group")
    mql: terraform.plan.resourceChanges.where( type == "aws_security_group" && change.after.ingress.contains( to_port == 5900 )).none( change.after.ingress[0].cidr_blocks.contains("0.0.0.0/0") || change.after.ingress[0].cidr_blocks.contains("::/0"))
  - uid: mondoo-aws-security-secgroup-restricted-vnc-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_security_group")
    mql: terraform.state.resources.where( type == "aws_security_group" && values.ingress.contains( to_port == 5900 ) ).none( values.ingress[0].cidr_blocks.contains("0.0.0.0/0") || values.ingress[0].cidr_blocks.contains("::/0"))

  - uid: mondoo-aws-security-secgroup-restricted-rdp
    title: Ensure security groups restrict incoming RDP traffic
    impact: 90
    docs:
      desc: |
        This check ensures that AWS security groups are not configured to allow unrestricted inbound Remote Desktop Protocol (RDP) traffic, which operates on port 3389. Allowing unrestricted RDP access (0.0.0.0/0 or ::/0) significantly increases the risk of brute-force attacks, unauthorized remote access, and exploitation by malicious actors.

        **Rationale:**

        RDP is a widely used protocol for remote administration of Windows-based instances in AWS. However, exposing port 3389 to the internet without restrictions creates a high-security risk. Attackers continuously scan for open RDP ports and attempt brute-force login attacks using credential stuffing or password spraying techniques.

        Instead of allowing open RDP access, organizations should:

        -	Restrict RDP access to trusted IP addresses (e.g., corporate networks, VPNs, or bastion hosts).
        - Use AWS Systems Manager Session Manager as a secure alternative to direct RDP access.
        - Enable multi-factor authentication (MFA) and strong authentication mechanisms.

        **Risk Mitigation:**

        -	**Prevent brute-force attacks:** Limits exposure to automated credential-guessing attacks.
        -	**Ensure least privilege access:** Only trusted IPs can establish RDP sessions.
        -	**Improve security posture:** Protects Windows servers from unauthorized access and exploits.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the Amazon EC2 Console.
            2.	Select Security Groups from the left navigation panel.
            3.	Identify security groups that allow inbound rules on port 3389 (RDP) from 0.0.0.0/0 (IPv4) or ::/0 (IPv6).
            4.	Select Edit inbound rules and remove any rule that allows unrestricted RDP access.
            5.	If necessary, replace 0.0.0.0/0 with a trusted IP range (e.g., a VPN or bastion host).
            6.	Save the changes.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check for security groups with unrestricted RDP access:
            ```sh
            aws ec2 describe-security-groups --query "SecurityGroups[?IpPermissions[?FromPort==`3389` && (IpRanges[].CidrIp=='0.0.0.0/0' || Ipv6Ranges[].CidrIpv6=='::/0')]].GroupId"
            ```

            Revoke unrestricted RDP access:
            ```sh
            aws ec2 revoke-security-group-ingress --group-id <security-group-id> --protocol tcp --port 3389 --cidr 0.0.0.0/0
            ```

            Allow RDP access only from a trusted IP range (replace x.x.x.x/x with your corporate/VPN subnet):
            ```sh
            aws ec2 authorize-security-group-ingress --group-id <security-group-id> --protocol tcp --port 3389 --cidr x.x.x.x/x
            ```

        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure security groups restrict RDP access to a specific IP range:
            ```hcl
            resource "aws_security_group" "restricted_rdp" {
              name        = "restricted-rdp"
              description = "Restricts RDP access"

              ingress {
                from_port   = 3389
                to_port     = 3389
                protocol    = "tcp"
                cidr_blocks = ["203.0.113.0/24"] # Replace with a trusted IP range
              }

                egress {
                  from_port   = 0
                  to_port     = 0
                  protocol    = "-1"
                  cidr_blocks = ["0.0.0.0/0"]  # Allow outbound traffic
                }
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure security groups do not allow unrestricted RDP access:
            ```yaml
            Resources:
              SecureSecurityGroup:
                Type: "AWS::EC2::SecurityGroup"
                Properties:
                  GroupDescription: "Security group with restricted RDP access"
                  VpcId: !Ref VPC
                  SecurityGroupIngress:
                    - IpProtocol: "tcp"
                      FromPort: 3389
                      ToPort: 3389
                      CidrIp: "192.168.1.0/24"  # Restrict to a trusted IP range
                  SecurityGroupEgress:
                    - IpProtocol: "-1"
                      FromPort: -1
                      ToPort: -1
                      CidrIp: "0.0.0.0/0"  # Allow outbound traffic
                  Tags:
                    - Key: "Name"
                      Value: "RestrictedRDPSecurityGroup"

              VPC:
                Type: "AWS::EC2::VPC"
                Properties:
                  CidrBlock: "10.0.0.0/16"
                  EnableDnsSupport: true
                  EnableDnsHostnames: true
                  Tags:
                    - Key: "Name"
                      Value: "SecureVPC"
            ```
    variants:
      - uid: mondoo-aws-security-secgroup-restricted-rdp-single
      - uid: mondoo-aws-security-secgroup-restricted-rdp-all
      - uid: mondoo-aws-security-secgroup-restricted-rdp-terraform-hcl
      - uid: mondoo-aws-security-secgroup-restricted-rdp-terraform-plan
      - uid: mondoo-aws-security-secgroup-restricted-rdp-terraform-state
  - uid: mondoo-aws-security-secgroup-restricted-rdp-all
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.securityGroups.where(ipPermissions.any(
       ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0"))).all(
          ipPermissions.none(fromPort <= 3389 && toPort >= 3389 && toPort != 0)
       )
  - uid: mondoo-aws-security-secgroup-restricted-rdp-single
    filters: asset.platform == "aws-security-group"
    mql: |
      aws.ec2.securitygroup.ipPermissions.where(
        fromPort <= 3389 && toPort >= 3389 && toPort != 0).none(
          ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0")
          )
  - uid: mondoo-aws-security-secgroup-restricted-rdp-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_security_group")
    mql: terraform.resources.where( nameLabel == "aws_security_group" && blocks.where( type == "ingress").contains( arguments.to_port == 3389 )).none( blocks.where( type == "ingress" ) { arguments.cidr_blocks.contains("0.0.0.0/0") || arguments.cidr_blocks.contains("::/0") })
  - uid: mondoo-aws-security-secgroup-restricted-rdp-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_security_group")
    mql: terraform.plan.resourceChanges.where( type == "aws_security_group" && change.after.ingress.contains( to_port == 3389 )).none( change.after.ingress[0].cidr_blocks.contains("0.0.0.0/0") || change.after.ingress[0].cidr_blocks.contains("::/0"))
  - uid: mondoo-aws-security-secgroup-restricted-rdp-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_security_group")
    mql: terraform.state.resources.where( type == "aws_security_group" && values.ingress.contains( to_port == 3389 ) ).none( values.ingress[0].cidr_blocks.contains("0.0.0.0/0") || values.ingress[0].cidr_blocks.contains("::/0"))

  - uid: mondoo-aws-security-secgroup-restrict-traffic
    title: Ensure Security Groups Restrict Access to Specific IPs and Ports
    impact: 90
    docs:
      desc: |
        This check ensures that AWS security groups do not allow unrestricted access to all IPs (0.0.0.0/0 or ::/0) on any port. Allowing open access to security groups exposes AWS resources to unauthorized access, increasing the risk of security breaches, brute-force attacks, and data exfiltration.

        **Rationale:**

        AWS security groups act as virtual firewalls that control inbound and outbound traffic. When a security group allows access from all IPs (0.0.0.0/0 for IPv4 or ::/0 for IPv6) on all ports, it poses significant risks:

        -	Unrestricted access to sensitive services (e.g., SSH on port 22, RDP on port 3389, or databases like MySQL on port 3306).
        -	Increased exposure to brute-force attacks, credential stuffing, and other cyber threats.
        -	Compliance violations under security frameworks such as CIS AWS Foundations Benchmark, PCI DSS, HIPAA, ISO 27001, and NIST.

        To mitigate these risks, security groups should be configured with the principle of least privilege, allowing access only to specific IP ranges and necessary ports.

        Risk Mitigation:

        -	Prevents unauthorized access by restricting inbound and outbound traffic.
        -	Reduces exposure to external attacks (e.g., brute force, malware injections).
        -	Ensures compliance with industry best practices and cloud security standards.
      remediation:
        - id: console
          desc: |
            **Using AWS Console:**

            1.	Navigate to the AWS EC2 Console.
            2.	Select Security Groups in the left panel.
            3.	Identify security groups with inbound rules allowing 0.0.0.0/0 or ::/0.
            4.	If a rule allows unrestricted access, modify it:
              -	Select Edit inbound rules.
              -	Restrict the source IP range to trusted networks (e.g., corporate VPN, private subnets, specific IPs).
              -	Select Save changes.
            5.	Repeat for outbound rules, ensuring traffic is limited to necessary destinations.
        - id: cli
          desc: |
            **Using AWS CLI:**

            Check security groups for unrestricted inbound rules:
            ```sh
            aws ec2 describe-security-groups --query "SecurityGroups[*].{ID:GroupId, Name:GroupName, Ingress:IpPermissions[*]}"
            ```

            Revoke a rule that allows open access:
            ```sh
            aws ec2 revoke-security-group-ingress \
            --group-id <security-group-id> \
            --protocol tcp \
            --port 22 \
            --cidr 0.0.0.0/0
            ```

            Add a restricted rule (e.g., allow SSH only from a trusted IP range):
            ```sh
            aws ec2 authorize-security-group-ingress \
            --group-id <security-group-id> \
            --protocol tcp \
            --port 22 \
            --cidr 192.168.1.0/24
            ```
        - id: terraform
          desc: |
            **Using Terraform:**

            Ensure security groups do not allow open access:
            ```hcl
            resource "aws_security_group" "restricted_sg" {
              name        = "restricted-security-group"
              description = "Security group with restricted access"
              vpc_id      = aws_vpc.main.id

              ingress {
                description = "Allow SSH from a trusted IP range"
                from_port   = 22
                to_port     = 22
                protocol    = "tcp"
                cidr_blocks = ["192.168.1.0/24"]  # Restrict to a known IP range
              }

              egress {
                from_port   = 0
                to_port     = 0
                protocol    = "-1"
                cidr_blocks = ["0.0.0.0/0"]  # Allow all outbound traffic (if necessary)
              }
            }
            ```
        - id: cloudformation
          desc: |
            **Using AWS CloudFormation:**

            Ensure security groups do not allow unrestricted access:
            ```yaml
            Resources:
              SecureSecurityGroup:
                Type: "AWS::EC2::SecurityGroup"
                Properties:
                  GroupDescription: "Security group with restricted access"
                  VpcId: !Ref VPC
                  SecurityGroupIngress:
                    - IpProtocol: "tcp"
                      FromPort: 22
                      ToPort: 22
                      CidrIp: "192.168.1.0/24"  # Restrict to a trusted IP range
                  SecurityGroupEgress:
                    - IpProtocol: "-1"
                      FromPort: -1
                      ToPort: -1
                      CidrIp: "0.0.0.0/0"  # Allow outbound traffic
                  Tags:
                    - Key: "Name"
                      Value: "RestrictedSecurityGroup"

              VPC:
                Type: "AWS::EC2::VPC"
                Properties:
                  CidrBlock: "10.0.0.0/16"
                  EnableDnsSupport: true
                  EnableDnsHostnames: true
                  Tags:
                    - Key: "Name"
                      Value: "SecureVPC"
            ```
    variants:
      - uid: mondoo-aws-security-secgroup-restrict-traffic-single
      - uid: mondoo-aws-security-secgroup-restrict-traffic-all
      - uid: mondoo-aws-security-secgroup-restrict-traffic-terraform-hcl
      - uid: mondoo-aws-security-secgroup-restrict-traffic-terraform-plan
      - uid: mondoo-aws-security-secgroup-restrict-traffic-terraform-state
  - uid: mondoo-aws-security-secgroup-restrict-traffic-single
    filters: asset.platform == "aws-security-group"
    mql: |
      aws.ec2.securitygroup.ipPermissions.where(
        fromPort == -1 && toPort == -1).none(
          ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0")
          )
  - uid: mondoo-aws-security-secgroup-restrict-traffic-all
    filters: asset.platform == "aws"
    mql: |
      aws.ec2.securityGroups.where(ipPermissions.any(
        ipRanges.contains("0.0.0.0/0") || ipRanges.contains("::/0"))).all(
          ipPermissions.none(fromPort == -1 && toPort == -1)
       )
  - uid: mondoo-aws-security-secgroup-restrict-traffic-terraform-hcl
    filters: asset.platform == "terraform-hcl" && terraform.resources.contains( nameLabel == "aws_security_group")
    mql: terraform.resources.where( nameLabel == "aws_security_group" && blocks.where( type == "ingress").contains( arguments.to_port == -1 )).none( blocks.where( type == "ingress" ) { arguments.cidr_blocks.contains("0.0.0.0/0") || arguments.cidr_blocks.contains("::/0") })
  - uid: mondoo-aws-security-secgroup-restrict-traffic-terraform-plan
    filters: asset.platform == "terraform-plan" && terraform.plan.resourceChanges.contains( type == "aws_security_group")
    mql: terraform.plan.resourceChanges.where( type == "aws_security_group" && change.after.ingress.contains( to_port == -1 )).none( change.after.ingress[0].cidr_blocks.contains("0.0.0.0/0") || change.after.ingress[0].cidr_blocks.contains("::/0"))
  - uid: mondoo-aws-security-secgroup-restrict-traffic-terraform-state
    filters: asset.platform == "terraform-state" && terraform.state.resources.contains( type == "aws_security_group")
    mql: terraform.state.resources.where( type == "aws_security_group" && values.ingress.contains( to_port == -1 ) ).none( values.ingress[0].cidr_blocks.contains("0.0.0.0/0") || values.ingress[0].cidr_blocks.contains("::/0"))
