# Copyright (c) Mondoo, Inc.
# SPDX-License-Identifier: BUSL-1.1

policies:
  - uid: mondoo-kubernetes-security
    name: Kubernetes Cluster and Workload Security
    version: 1.1.0
    license: BUSL-1.1
    tags:
      mondoo.com/category: security
      mondoo.com/platform: linux,kubernetes,k8s
    authors:
      - name: Mondoo, Inc
        email: hello@mondoo.com
    docs:
      desc: |-
        # Overview

        The Kubernetes Cluster and Workload Security by Mondoo provides guidance for establishing secure Kubernetes cluster configurations and workload deployments.

        If you have questions, comments, or have identified ways to improve this policy, please write us at hello@mondoo.com, or reach out in [GitHub Discussions](https://github.com/orgs/mondoohq/discussions).

        ## Remote scan

        Remote scans use native transports in cnspec to provide on demand scan results without the need to install any agents, or integration.

        For a complete list of native transports run:

        ```bash
        cnspec scan --help
        ```

        ### Prerequisites

        Remote scans of Kubernetes clusters requires a `KUBECONFIG` with access to the cluster you want to scan.

        ### Scan a Kubernetes cluster

        Open a terminal and configure an environment variable with the path to your `KUBECONFIG`:

        ```bash
        export KUBECONFIG=/path/to/kubeconfig
        ```

        Run a scan of the Kubernetes cluster:

        ```bash
        cnspec scan k8s
        ```

        ## Join the community!

        Our goal is to build policies that are simple to deploy, accurate, and actionable.

        If you have any suggestions for how to improve this policy, or if you need support, [join the community](https://github.com/orgs/mondoohq/discussions) in GitHub Discussions.
    groups:
      - title: Kubernetes API Server
        filters: |
          asset.family.contains(_ == 'linux')
          processes.where( executable == /kube-apiserver/ ).list != []
        checks:
          - uid: mondoo-kubernetes-security-api-server-no-anonymous-auth
          - uid: mondoo-kubernetes-security-https-api-server
          - uid: mondoo-kubernetes-security-secure-admin-conf
          - uid: mondoo-kubernetes-security-secure-controller-manager_conf
          - uid: mondoo-kubernetes-security-secure-etcd-data-dir
          - uid: mondoo-kubernetes-security-secure-kube-apiserver-yml
          - uid: mondoo-kubernetes-security-secure-pki-directory
          - uid: mondoo-kubernetes-security-secure-scheduler_conf
      - title: Kubernetes kubelet
        filters: |
          asset.family.contains(_ == 'linux')
          processes.where( executable == /kubelet/ ).list != []
        checks:
          - uid: mondoo-kubernetes-security-kubelet-anonymous-authentication
          - uid: mondoo-kubernetes-security-kubelet-authorization-mode
          - uid: mondoo-kubernetes-security-kubelet-event-record-qps
          - uid: mondoo-kubernetes-security-kubelet-iptables-util-chains
          - uid: mondoo-kubernetes-security-kubelet-protect-kernel-defaults
          - uid: mondoo-kubernetes-security-kubelet-read-only-port
          - uid: mondoo-kubernetes-security-kubelet-rotate-certificates
          - uid: mondoo-kubernetes-security-kubelet-strong-ciphers
          - uid: mondoo-kubernetes-security-kubelet-tls-certificate
          - uid: mondoo-kubernetes-security-secure-kubelet-cert-authorities
          - uid: mondoo-kubernetes-security-secure-kubelet-config
      - title: Kubernetes CronJobs Security
        filters: asset.platform == "k8s-cronjob"
        checks:
          - uid: mondoo-kubernetes-security-cronjob-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-cronjob-capability-net-raw
          - uid: mondoo-kubernetes-security-cronjob-capability-sys-admin
          - uid: mondoo-kubernetes-security-cronjob-containerd-socket
          - uid: mondoo-kubernetes-security-cronjob-crio-socket
          - uid: mondoo-kubernetes-security-cronjob-docker-socket
          - uid: mondoo-kubernetes-security-cronjob-hostipc
          - uid: mondoo-kubernetes-security-cronjob-hostnetwork
          - uid: mondoo-kubernetes-security-cronjob-hostpath-readonly
          - uid: mondoo-kubernetes-security-cronjob-hostpid
          - uid: mondoo-kubernetes-security-cronjob-imagepull
          - uid: mondoo-kubernetes-security-cronjob-limitcpu
          - uid: mondoo-kubernetes-security-cronjob-limitmemory
          - uid: mondoo-kubernetes-security-cronjob-ports-hostport
          - uid: mondoo-kubernetes-security-cronjob-privilegedcontainer
          - uid: mondoo-kubernetes-security-cronjob-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-cronjob-runasnonroot
          - uid: mondoo-kubernetes-security-cronjob-serviceaccount
      - title: Kubernetes StatefulSets Security
        filters: asset.platform == "k8s-statefulset"
        checks:
          - uid: mondoo-kubernetes-security-statefulset-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-statefulset-capability-net-raw
          - uid: mondoo-kubernetes-security-statefulset-capability-sys-admin
          - uid: mondoo-kubernetes-security-statefulset-containerd-socket
          - uid: mondoo-kubernetes-security-statefulset-crio-socket
          - uid: mondoo-kubernetes-security-statefulset-docker-socket
          - uid: mondoo-kubernetes-security-statefulset-hostipc
          - uid: mondoo-kubernetes-security-statefulset-hostnetwork
          - uid: mondoo-kubernetes-security-statefulset-hostpath-readonly
          - uid: mondoo-kubernetes-security-statefulset-hostpid
          - uid: mondoo-kubernetes-security-statefulset-imagepull
          - uid: mondoo-kubernetes-security-statefulset-limitcpu
          - uid: mondoo-kubernetes-security-statefulset-limitmemory
          - uid: mondoo-kubernetes-security-statefulset-ports-hostport
          - uid: mondoo-kubernetes-security-statefulset-privilegedcontainer
          - uid: mondoo-kubernetes-security-statefulset-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-statefulset-runasnonroot
          - uid: mondoo-kubernetes-security-statefulset-serviceaccount
      - title: Kubernetes Deployments Security
        filters: asset.platform == "k8s-deployment"
        checks:
          - uid: mondoo-kubernetes-security-deployment-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-deployment-capability-net-raw
          - uid: mondoo-kubernetes-security-deployment-capability-sys-admin
          - uid: mondoo-kubernetes-security-deployment-containerd-socket
          - uid: mondoo-kubernetes-security-deployment-crio-socket
          - uid: mondoo-kubernetes-security-deployment-docker-socket
          - uid: mondoo-kubernetes-security-deployment-hostipc
          - uid: mondoo-kubernetes-security-deployment-hostnetwork
          - uid: mondoo-kubernetes-security-deployment-hostpath-readonly
          - uid: mondoo-kubernetes-security-deployment-hostpid
          - uid: mondoo-kubernetes-security-deployment-imagepull
          - uid: mondoo-kubernetes-security-deployment-k8s-dashboard
          - uid: mondoo-kubernetes-security-deployment-limitcpu
          - uid: mondoo-kubernetes-security-deployment-limitmemory
          - uid: mondoo-kubernetes-security-deployment-ports-hostport
          - uid: mondoo-kubernetes-security-deployment-privilegedcontainer
          - uid: mondoo-kubernetes-security-deployment-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-deployment-runasnonroot
          - uid: mondoo-kubernetes-security-deployment-serviceaccount
          - uid: mondoo-kubernetes-security-deployment-tiller
      - title: Kubernetes Jobs Security
        filters: asset.platform == "k8s-job"
        checks:
          - uid: mondoo-kubernetes-security-job-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-job-capability-net-raw
          - uid: mondoo-kubernetes-security-job-capability-sys-admin
          - uid: mondoo-kubernetes-security-job-containerd-socket
          - uid: mondoo-kubernetes-security-job-crio-socket
          - uid: mondoo-kubernetes-security-job-docker-socket
          - uid: mondoo-kubernetes-security-job-hostipc
          - uid: mondoo-kubernetes-security-job-hostnetwork
          - uid: mondoo-kubernetes-security-job-hostpath-readonly
          - uid: mondoo-kubernetes-security-job-hostpid
          - uid: mondoo-kubernetes-security-job-imagepull
          - uid: mondoo-kubernetes-security-job-limitcpu
          - uid: mondoo-kubernetes-security-job-limitmemory
          - uid: mondoo-kubernetes-security-job-ports-hostport
          - uid: mondoo-kubernetes-security-job-privilegedcontainer
          - uid: mondoo-kubernetes-security-job-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-job-runasnonroot
          - uid: mondoo-kubernetes-security-job-serviceaccount
      - title: Kubernetes ReplicaSets Security
        filters: asset.platform == "k8s-replicaset"
        checks:
          - uid: mondoo-kubernetes-security-replicaset-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-replicaset-capability-net-raw
          - uid: mondoo-kubernetes-security-replicaset-capability-sys-admin
          - uid: mondoo-kubernetes-security-replicaset-containerd-socket
          - uid: mondoo-kubernetes-security-replicaset-crio-socket
          - uid: mondoo-kubernetes-security-replicaset-docker-socket
          - uid: mondoo-kubernetes-security-replicaset-hostipc
          - uid: mondoo-kubernetes-security-replicaset-hostnetwork
          - uid: mondoo-kubernetes-security-replicaset-hostpath-readonly
          - uid: mondoo-kubernetes-security-replicaset-hostpid
          - uid: mondoo-kubernetes-security-replicaset-imagepull
          - uid: mondoo-kubernetes-security-replicaset-limitcpu
          - uid: mondoo-kubernetes-security-replicaset-limitmemory
          - uid: mondoo-kubernetes-security-replicaset-ports-hostport
          - uid: mondoo-kubernetes-security-replicaset-privilegedcontainer
          - uid: mondoo-kubernetes-security-replicaset-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-replicaset-runasnonroot
          - uid: mondoo-kubernetes-security-replicaset-serviceaccount
      - title: Kubernetes DaemonSets Security
        filters: asset.platform == "k8s-daemonset"
        checks:
          - uid: mondoo-kubernetes-security-daemonset-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-daemonset-capability-net-raw
          - uid: mondoo-kubernetes-security-daemonset-capability-sys-admin
          - uid: mondoo-kubernetes-security-daemonset-containerd-socket
          - uid: mondoo-kubernetes-security-daemonset-crio-socket
          - uid: mondoo-kubernetes-security-daemonset-docker-socket
          - uid: mondoo-kubernetes-security-daemonset-hostipc
          - uid: mondoo-kubernetes-security-daemonset-hostnetwork
          - uid: mondoo-kubernetes-security-daemonset-hostpath-readonly
          - uid: mondoo-kubernetes-security-daemonset-hostpid
          - uid: mondoo-kubernetes-security-daemonset-imagepull
          - uid: mondoo-kubernetes-security-daemonset-limitcpu
          - uid: mondoo-kubernetes-security-daemonset-limitmemory
          - uid: mondoo-kubernetes-security-daemonset-ports-hostport
          - uid: mondoo-kubernetes-security-daemonset-privilegedcontainer
          - uid: mondoo-kubernetes-security-daemonset-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-daemonset-runasnonroot
          - uid: mondoo-kubernetes-security-daemonset-serviceaccount
      - title: Kubernetes Pods Security
        filters: asset.platform == "k8s-pod"
        checks:
          - uid: mondoo-kubernetes-security-pod-allowprivilegeescalation
          - uid: mondoo-kubernetes-security-pod-capability-net-raw
          - uid: mondoo-kubernetes-security-pod-capability-sys-admin
          - uid: mondoo-kubernetes-security-pod-containerd-socket
          - uid: mondoo-kubernetes-security-pod-crio-socket
          - uid: mondoo-kubernetes-security-pod-docker-socket
          - uid: mondoo-kubernetes-security-pod-hostipc
          - uid: mondoo-kubernetes-security-pod-hostnetwork
          - uid: mondoo-kubernetes-security-pod-hostpath-readonly
          - uid: mondoo-kubernetes-security-pod-hostpid
          - uid: mondoo-kubernetes-security-pod-imagepull
          - uid: mondoo-kubernetes-security-pod-k8s-dashboard
          - uid: mondoo-kubernetes-security-pod-limitcpu
          - uid: mondoo-kubernetes-security-pod-limitmemory
          - uid: mondoo-kubernetes-security-pod-ports-hostport
          - uid: mondoo-kubernetes-security-pod-privilegedcontainer
          - uid: mondoo-kubernetes-security-pod-readonlyrootfilesystem
          - uid: mondoo-kubernetes-security-pod-runasnonroot
          - uid: mondoo-kubernetes-security-pod-serviceaccount
          - uid: mondoo-kubernetes-security-pod-tiller
    scoring_system: 2
props:
  - uid: allowedCiphers
    title: Define the hardened SSL/ TLS ciphers
    mql: |
      return ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256",
        "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA",
        "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
        "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA",
        "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
        "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
        "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
        "TLS_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256",
        "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"]
queries:
  - uid: mondoo-kubernetes-security-kubelet-anonymous-authentication
    title: Disable anonymous authentication for kubelet
    impact: 100
    mql: |
      k8s.kubelet.configuration['authentication']['anonymous']['enabled'] == false
    docs:
      desc: |
        Ensure that the kubelet is configured to disable anonymous requests to the kubelet server.
        Otherwise the kubelet will allow unauthenticated access to its HTTPS endpoint. Request will have the privileges of the role `system:public-info-viewer`. This might expose data to an attacker.
      audit: |
        If running the kubelet with the CLI parameter '--anonymous-auth', or running with 'authentication.anonymous.enabled' defined in the kubelet configuration file, ensure that the value is set to 'false'.
      remediation: |
        Set the '--anonymous-auth' CLI parameter and/or the 'authentication.anonymous.enabled' field in the kubelet configuration file to 'false'.
    refs:
      - url: https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication
        title: Kubelet authentication
  - uid: mondoo-kubernetes-security-kubelet-event-record-qps
    title: Configure kubelet to capture all event creation
    impact: 30
    mql: |
      k8s.kubelet.configuration['eventRecordQPS'] == 0
    docs:
      desc: |
        Ensure that the kubelet is configured to capture all event creation so as to avoid potentially not logging important events.
        Be aware that this might expose your Cluster to a DoS risk.
      audit: |
        If running the kubelet with the CLI parameter '--event-qps', or running with 'eventRecordQPS' defined in the kubelet configuration file, ensure that the value is set to '0'.
      remediation: |
        Set the '--event-qps' CLI parameter and/or the 'eventRecordQPS' field in the kubelet configuration file to '0'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-iptables-util-chains
    title: Configure kubelet to ensure IPTables rules are set on host
    impact: 30
    mql: |
      k8s.kubelet.configuration['makeIPTablesUtilChains'] == true
    docs:
      desc: |
        Ensure that the kubelet is set up to create IPTable utility rules for various kubernetes components.
      audit: |
        If running the kubelet with the CLI parameter '--make-iptables-util-chains', or running with 'makeIPTablesUtilChains' defined in the kubelet configuration file, ensure that the value is set to 'true'.
      remediation: |
        Set the '--make-iptables-util-chains' CLI parameter and/or the 'makeIPTablesUtilChains' field in the kubelet configuration file to 'true'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-protect-kernel-defaults
    title: Configure kubelet to protect kernel defaults
    impact: 60
    mql: |
      k8s.kubelet.configuration["protectKernelDefaults"] == "true"
    docs:
      desc: |
        Ensure that the kubelet is set up to error if the underlying kernel tunables are different than the kubelet defaults. By default the kubelet will attempt to modify the kernel as the kubelet starts up.
      audit: |
        If running the kubelet with the CLI parameter '--protect-kernel-defaults', or running with 'protectKernelDefaults' defined in the kubelet configuration file, ensure that the value is set to 'true'.
      remediation: |
        Set the '--protect-kernel-defaults' CLI parameter and/or the 'protectKernelDefaults' field in the kubelet configuration file to 'true'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-read-only-port
    title: Do not allow unauthenticated read-only port on kubelet
    impact: 60
    mql: |
      k8s.kubelet.configuration['readOnlyPort'] == 0 || k8s.kubelet.configuration['readOnlyPort'] == null
    docs:
      desc: |
        Ensure the kubelet is not configured to serve up unauthenticated read-only access.
        This would expose data to unauthenticated users.
      audit: |
        If running the kubelet with the CLI parameter '--read-only-port', or running with 'readOnlyPort' defined in the kubelet configuration file, ensure that the value is either '0' or simply not set ('0' is the default).
      remediation: |
        Set the '--read-only-port' CLI parameter or the 'readOnlyPort' field in the kubelet configuration file to '0'.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-authorization-mode
    title: Ensure the kubelet is not configured with the AlwaysAllow authorization mode
    impact: 100
    mql: |
      k8s.kubelet.configuration['authorization']['mode'] != "AlwaysAllow"
    docs:
      desc: |
        Ensure the kubelet is not configured with the AlwaysAllow authorization mode.
        It would allow all requests.
      audit: |
        If running the kubelet with the CLI parameter '--authorization-mode', or running with 'authorization.mode' defined in the kubelet configuration file, ensure that the value is not set to 'AlwaysAllow'.
      remediation: |
        If the kubelet is configured with the CLI parameter '--authorization-mode', set it to something that isn't 'AlwaysAllow' (eg 'Webhook').

        If the kubelet is configured via the kubelet config file with the 'authorization.mode' parameter, set it to something that isn't 'AlwaysAllow' (eg. 'Webhook').
    refs:
      - url: https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization
        title: Kubelet authorization
  - uid: mondoo-kubernetes-security-kubelet-strong-ciphers
    title: Configure kubelet to use only strong cryptography
    impact: 100
    props:
      - uid: allowedCiphers
        title: Define the hardened SSL/ TLS ciphers
        mql: |
          return ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256",
            "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA",
            "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
            "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA",
            "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
            "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
            "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
            "TLS_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256",
            "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"]
    mql: |
      k8s.kubelet.configuration['tlsCipherSuites'] != null
      if (k8s.kubelet.configuration['tlsCipherSuites'] != null) {
        k8s.kubelet.configuration['tlsCipherSuites'].map( _.trim ).containsOnly(props.allowedCiphers)
      }
    docs:
      desc: |
        Ensure the kubelet runs with only strong cryptography support. Weak or old ciphers might expose your data.
      audit: |
        If running the kubelet with the CLI parameter '--tls-cipher-suites', or running with 'tlsCipherSuites' defined in the kubelet configuration file, ensure that the list of allowed ciphers is not empty and that all included ciphers are included in the following list:

        "TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
        "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
        "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA",
        "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
        "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_RSA_WITH_3DES_EDE_CBC_SHA",
        "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256", "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"
      remediation: |
        Define the list of allowed TLS ciphers to include only items from the strong list of ciphers.

        If the kubelet is configured with the CLI parameter '--tls-cipher-suites', update the list (or define the parameter) to only include strong ciphers.

        If the kubelet is configured via the kubelet config file with the 'tlsCipherSuites' parameter, update the list (or create an entry for 'tlsCipherSuites') to only include string ciphers.
    refs:
      - url: https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
        title: Kubelet configuration
  - uid: mondoo-kubernetes-security-kubelet-tls-certificate
    title: Run kubelet with a user-provided certificate/key
    impact: 100
    mql: |
      k8s.kubelet.configuration["tlsCertFile"] != null
      k8s.kubelet.configuration["tlsPrivateKeyFile"] != null
    docs:
      desc: |
        Ensure that the kubelet is not running with self-signed certificates generated by the kubelet itself.
      audit: |
        The kubelet CLI parameters override values in the kubelet configuration file.

        Check the kubelet CLI parameters to see whether '--tls-cert-file' and '--tls-private-key' are set to a non-empty path/string.

        Check the kubelet configuration file to see whether 'tlsCertFile' and 'tlsPrivateKeyFile' are set to a non-empty path/string.
      remediation: |
        Configure the kubelet to use a user-provided certificate/key pair for serving up HTTPS.

        After acquiring the TLS certificate/key pair, update the kubelet configuration file

        Or if using the deprecated kubelet CLI parameters, update the '--tls-cert-file' and '--tls-private-key-file' parameters to use the new certificate/key.
  - uid: mondoo-kubernetes-security-kubelet-rotate-certificates
    title: Run kubelet with automatic certificate rotation
    impact: 80
    mql: |
      k8s.kubelet.configuration["rotateCertificates"] != "false"
    docs:
      desc: |
        Ensure the kubelet is running with automatic certificate rotation so that the kubelet will automatically renew certificates with the API server as certificates near expiration.
        Otherwise the communication between the kubelet and the API server will be interrupted.
      audit: |
        Check the kubelet CLI parameters to ensure '--rotate-certificates' is not set to false, and that the kubelet config file has not set 'rotateCertificates' to false.
      remediation: |
        Depending on where the configuration behavior is defined (CLI parameters override config file values), update the kubelet CLI parameters to set '--rotate-certificates' to true, and/or update the kubelet configuration to set 'rotateCertificates' to true.
    refs:
      - url: https://kubernetes.io/docs/tasks/tls/certificate-rotation/
        title: Configure Certificate Rotation for the Kubelet
  - uid: mondoo-kubernetes-security-secure-kubelet-config
    title: Ownership and permissions of kubelet configuration should be restricted
    impact: 80
    mql: |
      if (k8s.kubelet.configFile != null) {
        if (k8s.kubelet.configFile.exists) {
          k8s.kubelet.configFile {
            user.name == "root"
            group.name == "root"
          }
          k8s.kubelet.configFile.permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |
        Ensure proper file ownership and read-write-execute permissions for kubelet configuration file.
        Otherwise unprivileged users might get access to sensitive information.
      audit: |
        View the kubelet configuration file details:

        ```
        $ ls -l /etc/kubernetes/kubelet.conf
        -rw-r--r-- 1 root root 1155 Sep 21 15:03 /etc/kubernetes/kubelet.conf
        ```
      remediation: |
        Update the ownership and permissions:

        ```
        chown root:root /etc/kubernetes/kubelet.conf
        chmod 600 /etc/kubernetes/kubelet.conf
        ```
  - uid: mondoo-kubernetes-security-secure-kubelet-cert-authorities
    title: Specify a kubelet certificate authorities file and ensure proper ownership and permissions
    impact: 100
    mql: |
      k8s.kubelet.configuration['authentication']['x509']['clientCAFile'] != null
      if (k8s.kubelet.configuration['authentication']['x509']['clientCAFile'] != null) {
        cafile = k8s.kubelet.configuration["authentication"]["x509"]["clientCAFile"]
        file(cafile) {
          user.name == "root"
          group.name == "root"
        }
        file(cafile).permissions {
          user_readable == true
          user_executable == false
          group_readable == false
          group_writeable == false
          group_executable == false
          other_readable == false
          other_writeable == false
          other_executable == false
        }
      }
    docs:
      desc: |
        Ensure appropriate ownership and permissions for the kubelet's certificate authorities configuration file.
      audit: |
        View the ownership and permissions:

        ```
        $ ls -l /etc/srv/kubernetes/pki/ca-certificates.crt
        -rw------- 1 root root 1159 Sep 13 04:14 /etc/srv/kubernetes/pki/ca-certificates.crt
        ```
      remediation: |
        Update the ownership and permissions:

        ```
        chown root:root /etc/srv/kubernetes/pki/ca-certificates.crt
        chmod 600 /etc/srv/kubernetes/pki/ca-certificates.crt
        ```
  - uid: mondoo-kubernetes-security-secure-kube-apiserver-yml
    title: Set secure file permissions on the API server pod specification file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/manifests/kube-apiserver.yaml").exists) {
        file("/etc/kubernetes/manifests/kube-apiserver.yaml") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        Ensure that the API server pod specification file has permissions of `600` and is owned by `root:root`.
        Otherwise unprivileged users might change it.
      remediation: |-
        Run this command on the Control Plane node:

        ```
        chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml
        chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml
        ```
  - uid: mondoo-kubernetes-security-secure-etcd-data-dir
    title: |
      Set secure directory permissions on the etcd data directory.
      Otherwise unprivileged users might get access to sensitive data stored in etcd, i.e., Kubernetes Secrets.
    impact: 60
    mql: |
      if (file("/var/lib/etcd").exists) {
        file("/var/lib/etcd") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == true
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "etcd"
          group.name == "etcd"
        }
      } else {
        dir = processes.where( executable == /etcd/ ).list[0].flags["data-dir"]
        file(dir) {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == true
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "etcd"
          group.name == "etcd"
        }
      }
    docs:
      desc: Ensure that the etcd data directory has permissions of `700` and is owned by `etcd:etcd`.
      remediation: |-
        On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

        ```
        ps -ef | grep etcd
        ```

        Run the below command:

        ```
        chmod 700 /var/lib/etcd
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/secret/
        title: Kubernetes Secrets
  - uid: mondoo-kubernetes-security-secure-admin-conf
    title: Set secure file permissions on the admin.conf file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/admin.conf").exists) {
        file("/etc/kubernetes/admin.conf") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        Ensure that the `admin.conf` file has permissions of `600` and is owned by root:root.
        Otherwise unprivileged users might get admin access to the Kubernetes API server.
      remediation: |-
        Run this command on the Control Plane node:

        ```
        chmod 600 /etc/kubernetes/admin.conf
        chown root:root /etc/kubernetes/admin.conf
        ```
    refs:
      - url: https://kubernetes.io/docs/setup/
        title: Kubernetes Setup
  - uid: mondoo-kubernetes-security-secure-scheduler_conf
    title: Set secure file permissions on the scheduler.conf file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/scheduler.conf").exists) {
        file("/etc/kubernetes/scheduler.conf") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: Ensure that the `scheduler.conf` file has permissions of `600` and is owned by `root:root`.
      remediation: |-
        Run this command on the Control Plane node:

        ```
        chmod 600 /etc/kubernetes/scheduler.conf
        chown root:root /etc/kubernetes/scheduler.conf
        ```
  - uid: mondoo-kubernetes-security-secure-controller-manager_conf
    title: Set secure file permissions on the controller-manager.conf file
    impact: 60
    mql: |
      if (file("/etc/kubernetes/controller-manager.conf").exists) {
        file("/etc/kubernetes/controller-manager.conf") {
          permissions.user_writeable == true
          permissions.group_writeable == false
          permissions.other_writeable == false
          permissions.user_readable == true
          permissions.group_readable == false
          permissions.other_readable == false
          permissions.user_executable == false
          permissions.group_executable == false
          permissions.other_executable == false
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: Ensure that the `controller-manager.conf` file has permissions of `600` and is owned by `root:root`.
      remediation: |-
        Run this command on the Control Plane node:

        ```
        chmod 600 /etc/kubernetes/controller-manager.conf
        chown root:root /etc/kubernetes/controller-manager.conf
        ```
  - uid: mondoo-kubernetes-security-secure-pki-directory
    title: Ensure that the Kubernetes PKI/SSL directory is owned by root:root
    impact: 65
    mql: |
      if (processes.where(executable == /kube-apiserver/).list[0].flags["etcd-certfile"] != null) {
        clientCAFile = processes.where(executable == /kube-apiserver/).list[0].flags["etcd-certfile"]
        ssldir = file(clientCAFile).dirname
        file(ssldir) {
          user.name == "root"
          group.name == "root"
        }
      } else {
        file("/etc/kubernetes/pki") {
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |
        Ensure that the Kubernetes PKI/SSL directory is owned by `root:root`.
        Otherwise unprivileged users could change the PKI/SSL certificates the whole encryption of the cluster relies on.
      remediation: |-
        Run one of these commands on the Control Plane node depending on the location of your PKI/SSL directory:

        ```
        chown -R root:root /etc/kubernetes/pki/
        ```

        or

        ```
        chown -R root:root /etc/kubernetes/ssl/
        ````
    refs:
      - url: https://kubernetes.io/docs/setup/best-practices/certificates/
        title: PKI certificates and requirements
  - uid: mondoo-kubernetes-security-https-api-server
    title: Ensure the kube-apiserver is not listening on an insecure HTTP port
    impact: 70
    mql: |
      processes.where(executable == /kube-apiserver/).list {
        flags["insecure-port"] == 0
      }
    docs:
      desc: |
        Ensure the kube-apiserver is not listening on an insecure HTTP port.
        Otherwise unencrypted traffic could be intercepted and sensitive data could be leaked.
      remediation: |-
        Find the kube-apiserver process and check the `insecure-port` argument. If the argument is set to `0`, then the kube-apiserver is not listening on an insecure HTTP port:
        ```
        ps aux | grep kube-apiserver
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/controlling-access/#transport-security
        title: Controlling Access to the Kubernetes API - Transport security
  - uid: mondoo-kubernetes-security-api-server-no-anonymous-auth
    title: |
      Ensure the kube-apiserver does not allow anonymous authentication.
      When allowed, request will have the privileges of the role `system:public-info-viewer`. This might expose data to an attacker.
    impact: 100
    mql: |
      processes.where(executable == /kube-apiserver/).list {
        flags["anonymous-auth"] == "false"
      }
    docs:
      desc: Ensure the kube-apiserver does not allow anonymous authentication.
      remediation: |-
        Find the kube-apiserver process and check the `--anonymous-auth` argument. If the argument is set to `false`, then the kube-apiserver does not allow anonymous authentication:
        ```
        ps aux | grep kube-apiserver
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
        title: Anonymous requests
  - uid: mondoo-kubernetes-security-pod-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: |
      k8s.pod {
        podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
      }
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-cronjob-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: |
      k8s.cronjob {
        podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
      }
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-statefulset-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.statefulset.podSpec['volumes'] == null || k8s.statefulset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-deployment-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.deployment.podSpec['volumes'] == null || k8s.deployment.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-job-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.job.podSpec['volumes'] == null || k8s.job.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-replicaset-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.replicaset.podSpec['volumes'] == null || k8s.replicaset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-daemonset-docker-socket
    title: Container should not mount the Docker socket
    impact: 100
    mql: k8s.daemonset.podSpec['volumes'] == null || k8s.daemonset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/docker.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/docker.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/docker.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-pod-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.pod.podSpec['volumes'] == null || k8s.pod.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-cronjob-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.cronjob.podSpec['volumes'] == null || k8s.cronjob.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-statefulset-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.statefulset.podSpec['volumes'] == null || k8s.statefulset.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-deployment-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.deployment.podSpec['volumes'] == null || k8s.deployment.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-job-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.job.podSpec['volumes'] == null || k8s.job.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-replicaset-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.replicaset.podSpec['volumes'] == null || k8s.replicaset.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-daemonset-containerd-socket
    title: Container should not mount the containerd socket
    impact: 100
    mql: k8s.daemonset.podSpec['volumes'] == null || k8s.daemonset.podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /run/containerd/containerd.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /run/containerd/containerd.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-pod-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.pod.podSpec['volumes'] == null || k8s.pod.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-cronjob-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.cronjob.podSpec['volumes'] == null || k8s.cronjob.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-statefulset-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.statefulset.podSpec['volumes'] == null || k8s.statefulset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-deployment-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.deployment.podSpec['volumes'] == null || k8s.deployment.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-job-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.job.podSpec['volumes'] == null || k8s.job.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-replicaset-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.replicaset.podSpec['volumes'] == null || k8s.replicaset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-daemonset-crio-socket
    title: Container should not mount the CRI-O socket
    impact: 100
    mql: k8s.daemonset.podSpec['volumes'] == null || k8s.daemonset.podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    docs:
      desc: |
        Do not mount the container runtime socket into any container.
        This would allow direct access to the container runtime without any authentication.
        This would allow to create privileged containers and to access the host file system.
        Or create containers which would not show up in the Kubernetes API.
      audit: |
        Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              volumeMounts:
                - mountPath: /var/run/crio/crio.sock
                  name: vol
        ```
      remediation: |
        Ensure workloads do not have `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          volumes:
            - name: vol
              hostPath:
                - path: /var/run/crio/crio.sock  # <--- this shouldn't be there
        ```
    refs:
      - url: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-1-do-not-expose-the-docker-daemon-socket-even-to-the-containers
        title: Docker security
  - uid: mondoo-kubernetes-security-pod-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.pod.ephemeralContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.pod.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.pod.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        Do not allow privilege escalation in containers.
        Even, when the container is not running as root, it could still escalate privileges.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.cronjob.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.cronjob.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        Do not allow privilege escalation in containers.
        Even, when the container is not running as root, it could still escalate privileges.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.statefulset.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.statefulset.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        Do not allow privilege escalation in containers.
        Even, when the container is not running as root, it could still escalate privileges.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.deployment.containers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.deployment.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN Note that this field cannot be set when spec.os.name is windows.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.job.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.job.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        Do not allow privilege escalation in containers.
        Even, when the container is not running as root, it could still escalate privileges.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.replicaset.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.replicaset.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        Do not allow privilege escalation in containers.
        Even, when the container is not running as root, it could still escalate privileges.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-allowprivilegeescalation
    title: Container should not allow privilege escalation
    impact: 100
    mql: |
      k8s.daemonset.initContainers.all( securityContext['allowPrivilegeEscalation'] != true )
      k8s.daemonset.containers.all( securityContext['allowPrivilegeEscalation'] != true )
    docs:
      desc: |
        Do not allow privilege escalation in containers.
        Even, when the container is not running as root, it could still escalate privileges.
      audit: |
        Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: true
        ```
      remediation: |
        Ensure `allowPrivilegeEscalation` is set to `false` or not present in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              securityContext:
                allowPrivilegeEscalation: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.pod.ephemeralContainers.all( securityContext['privileged'] != true )
      k8s.pod.initContainers.all( securityContext['privileged'] != true )
      k8s.pod.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.cronjob.initContainers.all( securityContext['privileged'] != true )
      k8s.cronjob.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.statefulset.initContainers.all( securityContext['privileged'] != true )
      k8s.statefulset.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.deployment.containers.all( securityContext['privileged'] != true )
      k8s.deployment.initContainers.all( securityContext['privileged'] != true )     
    docs:
      desc: |
        Running a privileged container means that the container has the host's capabilities including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.job.initContainers.all( securityContext['privileged'] != true )
      k8s.job.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.replicaset.initContainers.all( securityContext['privileged'] != true )
      k8s.replicaset.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-privilegedcontainer
    title: Container should not run as a privileged container
    impact: 100
    mql: |
      k8s.daemonset.initContainers.all( securityContext['privileged'] != true )
      k8s.daemonset.containers.all( securityContext['privileged'] != true )
    docs:
      desc: |
        Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
      audit: |
        Check for the existence of `privileged: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: true
        ```
      remediation: |
        Remove the `privileged` setting from the container spec:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```

        Or explicitly set `privileged` to `false`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                privileged: false
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.pod.ephemeralContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.pod.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.pod.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.cronjob.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.cronjob.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.statefulset.containers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.statefulset.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.deployment.containers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.deployment.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )   
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.job.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.job.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.replicaset.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.replicaset.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-readonlyrootfilesystem
    title: Container should use an immutable root filesystem
    impact: 80
    mql: |
      k8s.daemonset.initContainers.all( securityContext['readOnlyRootFilesystem'] == true )
      k8s.daemonset.containers.all( securityContext['readOnlyRootFilesystem'] == true )
    docs:
      desc: |
        Running a container with an immutable (read-only) file system prevents the modification of running containers.
      audit: |
        Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
      remediation: |
        Ensure `readOnlyRootFilesystem` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                readOnlyRootFilesystem: true
        ```
    refs:
      - url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      if (k8s.pod.annotations['policies.k8s.mondoo.com/mondoo-kubernetes-security-pod-runasnonroot'] != 'ignore') {
        k8s.pod {
          podSecurityContext=podSpec['securityContext']
          ephemeralContainers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
          initContainers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
          containers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
        }
      }
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-cronjob-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      if (k8s.cronjob.annotations['policies.k8s.mondoo.com/mondoo-kubernetes-security-cronjob-runasnonroot'] != 'ignore') {
        k8s.cronjob {
          podSecurityContext=podSpec['securityContext']
          initContainers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
          containers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
        }
      }
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-statefulset-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.statefulset {
        podSecurityContext=podSpec['securityContext']
        initContainers {
          a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
          res = securityContext['runAsNonRoot'] == true || a
          res == true
        }
        containers {
          a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
          res = securityContext['runAsNonRoot'] == true || a
          res == true
        }
      }
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-deployment-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.deployment.containers.all( securityContext['runAsNonRoot'] == true )
      k8s.deployment.initContainers.all( securityContext['runAsNonRoot'] == true )   
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-job-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      if (k8s.job.annotations['policies.k8s.mondoo.com/mondoo-kubernetes-security-job-runasnonroot'] != 'ignore') {
        k8s.job {
          podSecurityContext=podSpec['securityContext']
          initContainers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
          containers {
            a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
            res = securityContext['runAsNonRoot'] == true || a
            res == true
          }
        }
      }
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-replicaset-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.replicaset {
        podSecurityContext=podSpec['securityContext']
        initContainers {
          a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
          res = securityContext['runAsNonRoot'] == true || a
          res == true
        }
        containers {
          a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
          res = securityContext['runAsNonRoot'] == true || a
          res == true
        }
      }
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-daemonset-runasnonroot
    title: Container should not run as root
    impact: 100
    mql: |
      k8s.daemonset {
        podSecurityContext=podSpec['securityContext']
        initContainers {
          a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
          res = securityContext['runAsNonRoot'] == true || a
          res == true
        }
        containers {
          a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
          res = securityContext['runAsNonRoot'] == true || a
          res == true
        }
      }
    docs:
      desc: |
        Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
        When containers run as the `root` user, they have the same privileges as `root` on the host system.
      audit: |
        Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `runAsNonRoot` is set to `true` in the `securityContext`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
              securityContext:
                runAsNonRoot: true
        ```

        It is also possible to set it for all containers at the Pod level:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          securityContext:
            runAsNonRoot: true
          containers:
            - name: container-name
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#securitycontext-v1-core
        title: Configure a Security Context for a Pod or Container
  - uid: mondoo-kubernetes-security-pod-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.pod.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network including loopback devices. This capability can be used to intercept network traffic including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-cronjob-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.cronjob.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-statefulset-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.statefulset.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-deployment-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.deployment.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-job-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: k8s.job.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-replicaset-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: |
      k8s.replicaset.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-daemonset-hostnetwork
    title: Pod should not run with hostNetwork
    impact: 80
    mql: |
      k8s.daemonset.podSpec['hostNetwork'] != true
    docs:
      desc: Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
      audit: |
        Check for the existence of `hostNetwork: true` setting in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostNetwork` is set to `false` or not present in `spec`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          hostNetwork: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-pod-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.pod.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-cronjob-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.cronjob.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-statefulset-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.statefulset.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-deployment-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.deployment.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-job-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.job.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-replicaset-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: k8s.replicaset.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-daemonset-hostpid
    title: Pod should not run with hostPID
    impact: 80
    mql: |
      k8s.daemonset.podSpec['hostPID'] != true
    docs:
      desc: Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
      audit: |
        Check for the existence of `hostPID: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostPID` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostPID: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-pod-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.pod.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-cronjob-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.cronjob.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-statefulset-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.statefulset.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-deployment-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: k8s.deployment.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-job-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.job.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-replicaset-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.replicaset.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-daemonset-hostipc
    title: Pod should not run with hostIPC
    impact: 80
    mql: |
      k8s.daemonset.podSpec['hostIPC'] != true
    docs:
      desc: |
        Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
      audit: |
        Check for the existence of `hostIPC: true` setting in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: true
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
      remediation: |
        Ensure `hostIPC` is set to `false` or not present in `spec`:

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          hostIPC: false
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: Host namespaces
  - uid: mondoo-kubernetes-security-pod-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.pod.podSpec['serviceAccount'] == null || k8s.pod.podSpec['serviceAccount'] == k8s.pod.podSpec['serviceAccountName']
      k8s.pod.podSpec['serviceAccountName'] != '' || k8s.pod.podSpec['automountServiceAccountToken'] == false
      k8s.pod.podSpec['serviceAccountName'] != 'default' || k8s.pod.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-cronjob-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.cronjob.podSpec['serviceAccount'] == null || k8s.cronjob.podSpec['serviceAccount'] == k8s.cronjob.podSpec['serviceAccountName']
      k8s.cronjob.podSpec['serviceAccountName'] != '' || k8s.cronjob.podSpec['automountServiceAccountToken'] == false
      k8s.cronjob.podSpec['serviceAccountName'] != 'default' || k8s.cronjob.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-statefulset-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.statefulset.podSpec['serviceAccount'] == null || k8s.statefulset.podSpec['serviceAccount'] == k8s.statefulset.podSpec['serviceAccountName']
      k8s.statefulset.podSpec['serviceAccountName'] != '' || k8s.statefulset.podSpec['automountServiceAccountToken'] == false
      k8s.statefulset.podSpec['serviceAccountName'] != 'default' || k8s.statefulset.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-deployment-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.deployment.podSpec['serviceAccount'] == null || k8s.deployment.podSpec['serviceAccount'] == k8s.deployment.podSpec['serviceAccountName']
      k8s.deployment.podSpec['serviceAccountName'] != '' || k8s.deployment.podSpec['automountServiceAccountToken'] == false
      k8s.deployment.podSpec['serviceAccountName'] != 'default' || k8s.deployment.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-job-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.job.podSpec['serviceAccount'] == null || k8s.job.podSpec['serviceAccount'] == k8s.job.podSpec['serviceAccountName']
      k8s.job.podSpec['serviceAccountName'] != '' || k8s.job.podSpec['automountServiceAccountToken'] == false
      k8s.job.podSpec['serviceAccountName'] != 'default' || k8s.job.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-replicaset-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.replicaset.podSpec['serviceAccount'] == null || k8s.replicaset.podSpec['serviceAccount'] == k8s.replicaset.podSpec['serviceAccountName']
      k8s.replicaset.podSpec['serviceAccountName'] != '' || k8s.replicaset.podSpec['automountServiceAccountToken'] == false
      k8s.replicaset.podSpec['serviceAccountName'] != 'default' || k8s.replicaset.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-daemonset-serviceaccount
    title: Pod should not run with the default service account
    impact: 30
    mql: |
      k8s.daemonset.podSpec['serviceAccount'] == null || k8s.daemonset.podSpec['serviceAccount'] == k8s.daemonset.podSpec['serviceAccountName']
      k8s.daemonset.podSpec['serviceAccountName'] != '' || k8s.daemonset.podSpec['automountServiceAccountToken'] == false
      k8s.daemonset.podSpec['serviceAccountName'] != 'default' || k8s.daemonset.podSpec['automountServiceAccountToken'] == false
    docs:
      desc: |
        Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts.
        These ServiceAccounts should only have the permissions necessary.
        The Pods should not use the default ServiceAccount (named 'default') that is included in every Namespace.
        The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'.
        In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
        When every Pods uses the default ServiceAccount and the ServiceAccount's privileges get extended, all Pods get these permissions.
        When a Pod is compromised, the attacker has access to the API using the default ServiceAccount.
      audit: |
        Check that Pods do not set the legacy '.spec.serviceAccount':

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccount: some-account
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

        ```yaml
        apiVersion: v1
        kind: Pod
        spec:
          serviceAccountName: ""
          containers:
            - name: example-app
              image: index.docker.io/yournamespace/repository
        ```

        Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
        Because of that, we also need to check for the field.
      remediation: |
        Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

        Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
  - uid: mondoo-kubernetes-security-pod-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.pod.ephemeralContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.pod.initContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.pod.containers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-cronjob-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.cronjob.initContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.cronjob.containers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-statefulset-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.statefulset.initContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.statefulset.containers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-deployment-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.deployment.initContainers.all( imagePullPolicy == 'Always' && correctImage = image != /:latest/ && image.contains(':') == true )
      k8s.deployment.containers.all( imagePullPolicy == 'Always' && correctImage = image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-job-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.job.initContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.job.containers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-replicaset-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.replicaset.containers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.replicaset.initContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-daemonset-imagepull
    title: Container image pull should be consistent
    impact: 60
    mql: |
      k8s.daemonset.containers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
      k8s.daemonset.initContainers.all( imagePullPolicy == 'Always' && image != /:latest/ && image.contains(':') == true )
    docs:
      desc: |
        It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA).
        Avoid using rolling tags like `latest` or `master` as they can change over time.
      audit: |
        Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
      remediation: |
        Ensure `imagePullPolicy` is set to `Always` and ensure `image` uses either a tag or a digest (SHA):

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              imagePullPolicy: Always
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
        title: Image pull policy
  - uid: mondoo-kubernetes-security-pod-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.pod.initContainers.all( resources['limits']['cpu'] != null )
      k8s.pod.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes Pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-cronjob-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.cronjob.initContainers.all( resources['limits']['cpu'] != null )
      k8s.cronjob.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-statefulset-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.statefulset.initContainers.all( resources['limits']['cpu'] != null )
      k8s.statefulset.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-deployment-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.deployment.initContainers.all( resources['limits']['cpu'] != null )
      k8s.deployment.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-job-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.job.initContainers.all( resources['limits']['cpu'] != null )
      k8s.job.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-replicaset-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.replicaset.initContainers.all( resources['limits']['cpu'] != null )
      k8s.replicaset.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-daemonset-limitcpu
    title: Container should have a CPU limit
    impact: 20
    mql: |
      k8s.daemonset.initContainers.all( resources['limits']['cpu'] != null )
      k8s.daemonset.containers.all( resources['limits']['cpu'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of CPU resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
      remediation: |
        Define the required resources for CPU `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  cpu: "500m"
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        title: Resource Management for Pods and Containers
  - uid: mondoo-kubernetes-security-pod-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.pod.initContainers.all( resources['limits']['memory'] != null )
      k8s.pod.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-cronjob-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.cronjob.initContainers.all( resources['limits']['memory'] != null )
      k8s.cronjob.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-statefulset-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.statefulset.initContainers.all( resources['limits']['memory'] != null )
      k8s.statefulset.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-deployment-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.deployment.initContainers.all( resources['limits']['memory'] != null )
      k8s.deployment.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-job-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.job.initContainers.all( resources['limits']['memory'] != null )
      k8s.job.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-replicaset-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.replicaset.initContainers.all( resources['limits']['memory'] != null )
      k8s.replicaset.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-daemonset-limitmemory
    title: Container should have a memory limit
    impact: 20
    mql: |
      k8s.daemonset.initContainers.all( resources['limits']['memory'] != null )
      k8s.daemonset.containers.all( resources['limits']['memory'] != null )
    docs:
      desc: |
        Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
      audit: |
        Check for the existence of memory resources in `limits`:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
      remediation: |
        Define the required resources for memory `limits` in the manifest:

        ```yaml
        ---
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: app
              image: images.my-company.example/app:v1.2.3
              resources:
                limits:
                  memory: "1Gi"
        ```
  - uid: mondoo-kubernetes-security-pod-capability-net-raw
    title: Pods should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.pod.podSpec['ephemeralContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['ephemeralContainers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['initContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['initContainers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.pod.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        Pods should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no Pods have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a Pod that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these Pods with:

        ```kubectl get pods -A -o json | jq -r '.items[] | select( .spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Pods that explicitly add the NET_RAW or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: examplePod
          namespace: example-namespace
        spec:
          containers:
            - securityContext:
                capabilities:
                  add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any Pods that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - securityContext:
                capabilities:
                  drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-daemonset-capability-net-raw
    title: DaemonSets should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.daemonset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.daemonset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        DaemonSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no DaemonSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a DaemonSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```kubectl get daemonsets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any DaemonSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any DaemonSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-replicaset-capability-net-raw
    title: ReplicaSets should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.replicaset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.replicaset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        ReplicaSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no ReplicaSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a ReplicaSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```kubectl get replicasets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any ReplicaSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any ReplicaSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-job-capability-net-raw
    title: Jobs should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        Jobs should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no Jobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a Job that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```kubectl get jobs -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Jobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any Jobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-deployment-capability-net-raw
    title: Deployments should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'].none( _['add'].contains("NET_RAW") ))
      k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'].none( _['add'].contains("ALL") ))
    docs:
      desc: |
        Deployments should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no Deployments have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a Deployment that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```kubectl get deployments -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Deployments that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any Deployments that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-statefulset-capability-net-raw
    title: StatefulSets should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.statefulset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.statefulset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        StatefulSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no StatefulSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a StatefulSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```kubectl get statefulsets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any StatefulSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any StatefulSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-cronjob-capability-net-raw
    title: CronJobs should not run with NET_RAW capability
    impact: 80
    mql: |
      k8s.cronjob.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^NET_RAW$|^ALL$/ ) } )
      k8s.cronjob.podSpec['containers'].all( _['securityContext']['capabilities'] { _['drop'].any( _.upcase == /^NET_RAW$|^ALL$/ ) } )
    docs:
      desc: |
        CronJobs should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
      audit: |
        Check to ensure no CronJobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

        ```kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


        Additionally, a CronJob that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

        ```kubectl get cronjobs -A -o json | jq -r '.items[] | select( .spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any CronJobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - securityContext:
                        capabilities:
                          add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
        ```

        For any CronJobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - securityContext:
                        capabilities:
                          drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        title: 'Kubernetes Security Standards: Capabilities'
  - uid: mondoo-kubernetes-security-pod-capability-sys-admin
    title: Pods should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.pod.podSpec['initContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
      k8s.pod.podSpec['ephemeralContainers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
      k8s.pod.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        Pods should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no Pods have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Pods that explicitly add the SYS_ADMIN or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: examplePod
          namespace: example-namespace
        spec:
          containers:
            - securityContext:
                capabilities:
                  add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-daemonset-capability-sys-admin
    title: DaemonSets should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.daemonset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        DaemonSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no DaemonSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any DaemonSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-replicaset-capability-sys-admin
    title: ReplicaSets should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.replicaset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        ReplicaSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no ReplicaSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any ReplicaSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-job-capability-sys-admin
    title: Jobs should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.job.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        Jobs should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no Jobs have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Jobs that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-deployment-capability-sys-admin
    title: Deployments should not run with SYS_ADMIN capability
    impact: 80
    mql: k8s.deployment.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        Deployments should not run wIt even allows containers not running as root to run certain tasks as if the user was root with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no Deployments have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Deployments that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-statefulset-capability-sys-admin
    title: StatefulSets should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.statefulset.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        StatefulSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no StatefulSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any StatefulSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - securityContext:
                    capabilities:
                      add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-cronjob-capability-sys-admin
    title: CronJobs should not run with SYS_ADMIN capability
    impact: 80
    mql: |
      k8s.cronjob.podSpec['containers'].all( _['securityContext']['capabilities'] { _['add'] == null || _['add'].none( _.upcase == /^SYS_ADMIN$|^ALL$/ ) } )
    docs:
      desc: |
        CronJobs should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
        It even allows containers not running as root to run certain tasks as if the user was root.
      audit: |
        Check to ensure no CronJobs have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

        ```kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any CronJobs that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - securityContext:
                        capabilities:
                          add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Capabilities'
      - url: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
        title: Docker default capabilities
  - uid: mondoo-kubernetes-security-pod-ports-hostport
    title: Pods should not bind to a host port
    impact: 80
    mql: |
      k8s.pod.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
      k8s.pod.podSpec['initContainers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        Pods should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no Pods are binding any of their containers to a host port:

        ```kubectl get pods -A -o json | jq -r '.items[] | select( (.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any Pods that bind to a host port, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not bind to a host port:

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - ports:
              - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                name: http
                protocol: TCP
              - containerPort: 443
                name: https
                protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-daemonset-ports-hostport
    title: DaemonSets should not bind to a host port
    impact: 80
    mql: |
      k8s.daemonset.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        DaemonSets should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no DaemonSets are binding any of their containers to a host port:

        ```kubectl get daemonsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any DaemonSets that bind to a host port, update the DaemonSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-replicaset-ports-hostport
    title: ReplicaSets should not bind to a host port
    impact: 80
    mql: |
      k8s.replicaset.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        ReplicaSets should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no ReplicaSets are binding any of their containers to a host port:

        ```kubectl get replicasets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any ReplicaSets that bind to a host port, update the ReplicaSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-job-ports-hostport
    title: Jobs should not bind to a host port
    impact: 80
    mql: |
      k8s.job.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        Jobs should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no Jobs are binding any of their containers to a host port:

        ```kubectl get jobs -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any ReplicaSets that bind to a host port, update the Jobs to ensure they do not bind to a host port:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-deployment-ports-hostport
    title: Deployments should not bind to a host port
    impact: 80
    mql: | 
      k8s.deployment.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        Deployments should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no Deployments are binding any of their containers to a host port:

        ```kubectl get deployments -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any Deployments that bind to a host port, update the Deployments to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-statefulset-ports-hostport
    title: StatefulSets should not bind to a host port
    impact: 80
    mql: |
      k8s.statefulset.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        StatefulSets should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no StatefulSets are binding any of their containers to a host port:

        ```kubectl get statefulsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any StatefulSets that bind to a host port, update the StatefulSets to ensure they do not bind to a host port:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - ports:
                  - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                    name: http
                    protocol: TCP
                  - containerPort: 443
                    name: https
                    protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-cronjob-ports-hostport
    title: CronJobs should not bind to a host port
    impact: 80
    mql: |
      k8s.cronjob.podSpec['containers'].all( _['ports'] == null || _['ports'].none( _['hostPort']))
    docs:
      desc: |
        CronJobs should not bind to the underlying host port. This allows bypassing certain network access control systems.
        Host ports also expose the Container outside the Kubernetes cluster, which might not be intended.
      audit: |
        Check to ensure no CronJobs are binding any of their containers to a host port:

        ```kubectl get cronjobs -A -o json | jq -r '.items[] | select( (.spec.jobTemplate.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
      remediation: |
        For any CronJobs that bind to a host port, update the CronJobs to ensure they do not bind to a host port:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          jobTemplate:
            spec:
              template:
                spec:
                  containers:
                    - ports:
                      - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                        name: http
                        protocol: TCP
                      - containerPort: 443
                        name: https
                        protocol: TCP
        ```
    refs:
      - url: https://kubernetes.io/docs/concepts/configuration/overview/#services
        title: 'Kubernetes Configuration Best Practices: hostPort'
      - url: https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
        title: 'Kubernetes Security Standards: Host Ports'
  - uid: mondoo-kubernetes-security-pod-hostpath-readonly
    title: Pods should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.pod.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
      k8s.pod.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['initContainers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
      k8s.pod.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['ephemeralContainers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        Pods should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a Pod are mounting hostPath volumes as read-write:

        ```kubectl get pods -A -o json | jq -r '.items[] | [.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Pod containers that mount a hostPath volume as read-write, update them (or the Deployment/StatefulSet/etc that created the Pod):

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: example
          namespace: example-namespace
        spec:
          containers:
            - volumeMounts:
              - mountPath: /host
                name: hostpath-volume
                readOnly: true # <-- ensure readOnly is set to true
          volumes:
            - hostPath:
                path: /etc
              name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-daemonset-hostpath-readonly
    title: DaemonSets should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.daemonset.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        DaemonSets should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a DaemonSet are mounting hostPath volumes as read-write:

        ```kubectl get daemonsets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any DaemonSet containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: DaemonSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-replicaset-hostpath-readonly
    title: ReplicaSets should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.replicaset.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        ReplicaSets should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a ReplicaSet are mounting hostPath volumes as read-write:

        ```kubectl get replicasets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any ReplicaSet containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-job-hostpath-readonly
    title: Jobs should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.job.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        Jobs should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a Job are mounting hostPath volumes as read-write:

        ```kubectl get jobs -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Job containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-deployment-hostpath-readonly
    title: Deployments should mount any host path volumes as read-only
    impact: 80
    mql: | 
      k8s.deployment.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        Deployments should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a Deployment are mounting hostPath volumes as read-write:

        ```kubectl get deployments -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any Deployment containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-statefulset-hostpath-readonly
    title: StatefulSets should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.statefulset.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        StatefulSets should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a StatefulSet are mounting hostPath volumes as read-write:

        ```kubectl get statefulsets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any StatefulSet containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: apps/v1
        kind: StatefulSet
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-cronjob-hostpath-readonly
    title: CronJobs should mount any host path volumes as read-only
    impact: 80
    mql: |
      k8s.cronjob.podSpec {
        hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
        _['containers'] {
          _['name']
          if( _['volumeMounts'] != null ) {
            _['volumeMounts'] {
              n = _['name']
              if( hostPathVolumes.contains(n) ) {
                _['readOnly'] == true
              } else {
                true
              }
            }
          } else {
            true
          }
        }
      }
    docs:
      desc: |
        CronJobs should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
        This can even lead to container escapes.
      audit: |
        Check to ensure no containers in a CronJob are mounting hostPath volumes as read-write:

        ```kubectl get cronjobs -A -o json | jq -r '.items[] | [.spec.jobTemplate.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.jobTemplate.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
      remediation: |
        For any CronJob containers that mount a hostPath volume as read-write, update them:

        ```yaml
        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: example
          namespace: example-namespace
        spec:
          template:
            spec:
              containers:
                - volumeMounts:
                  - mountPath: /host
                    name: hostpath-volume
                    readOnly: true # <-- ensure readOnly is set to true
              volumes:
                - hostPath:
                    path: /etc
                  name: hostpath-volume
        ```
  - uid: mondoo-kubernetes-security-deployment-tiller
    title: Deployments should not run Tiller (Helm v2)
    impact: 40
    mql: |
      k8s.deployment.podSpec["containers"].none( _["image"].contains("tiller") )
    docs:
      desc: |
        Tiller is the in-cluster component for the Helm v2 package manager. It is communicating directly to the Kubernetes API and therefore it has broad RBAC permissions. An attacker can use that to get cluster-wide access.
      audit: |
        Verify there are no deployments running Tiller:
        ```kubectl get deployments -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
      remediation: |
        Delete any deployments that are running Tiller.
  - uid: mondoo-kubernetes-security-pod-tiller
    title: Pods should not run Tiller (Helm v2)
    impact: 40
    mql: |
      k8s.pod.podSpec["containers"].none( _["image"].contains("tiller") )
      k8s.pod.podSpec["initContainers"].none( _["image"].contains("tiller") )
      k8s.pod.podSpec["ephemeralContainers"].none( _["image"].contains("tiller") )
    docs:
      desc: |
        Tiller is the in-cluster component for the Helm v2 package manager. It is communicating directly to the Kubernetes API and therefore it has broad RBAC permissions. An attacker can use that to get cluster-wide access.
      audit: |
        Verify there are no pods running Tiller:
        ```kubectl get pods -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
      remediation: |
        Delete any pods that are running Tiller.
  - uid: mondoo-kubernetes-security-deployment-k8s-dashboard
    title: Pods should not run Kubernetes dashboard
    impact: 40
    mql: |
      k8s.deployment.podSpec["containers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.deployment.labels["app"] == null || k8s.deployment.labels["app"] != "kubernetes-dashboard"
      k8s.deployment.labels["k8s-app"] == null || k8s.deployment.labels["k8s-app"] != "kubernetes-dashboard"
    docs:
      desc: |
        The Kubernetes dashboard allows browsing through cluster resources such as workloads, configmaps and secrets. In 2019 Tesla was hacked because their Kubernetes dashboard was publicly exposed. This allowed the attackers to extract credentials and deploy Bitcoin miners on the cluster.
      audit: |
        Verify there are no deployments running Kubernetes dashboard:
        ```kubectl get deployments -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
      remediation: |
        Delete any deployments that are running Kubernetes dashboard.
  - uid: mondoo-kubernetes-security-pod-k8s-dashboard
    title: Pods should not run Kubernetes dashboard
    impact: 40
    mql: |
      k8s.pod.podSpec["containers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.pod.podSpec["initContainers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.pod.podSpec["ephemeralContainers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
      k8s.pod.labels["app"] == null || k8s.pod.labels["app"] != "kubernetes-dashboard"
      k8s.pod.labels["k8s-app"] == null || k8s.pod.labels["k8s-app"] != "kubernetes-dashboard"
    docs:
      desc: |
        The Kubernetes dashboard allows browsing through cluster resources such as workloads, configmaps and secrets. In 2019 Tesla was hacked because their Kubernetes dashboard was publicly exposed. This allowed the attackers to extract credentials and deploy Bitcoin miners on the cluster.
      audit: |
        Verify there are no pods running Kubernetes dashboard:
        ```kubectl get pods -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
      remediation: |
        Delete any pods that are running Kubernetes dashboard.
